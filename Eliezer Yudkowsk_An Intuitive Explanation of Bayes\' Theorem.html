<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        An Intuitive Explanation of Bayes\' Theorem      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Eliezer Yudkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><strong>Followup to</strong>: Anthropic Decision Theory</p><br><p>When I originally proposed the decision theory known as "Anthropic Decision Theory"\xe2\x80\x94which is the subject of the sequence <em>Anthropic Reasoning and Decision Theory\xe2\x80\x94A Brief Introduction</em>\xe2\x80\x94the idea was that we should have a decision theory which could take into account the observation that intelligent life tends to evolve around planets that have liquid water available.</p><br><p>I am starting to suspect that this approach may be going in the wrong direction.</p><br><p>Why? Because of something known as the "simplest explanation":  The simplest explanation for liquid water is that life needs water. To explain water, you need to explain biology, biology needs to explain cells, cells need to explain DNA, DNA needs to explain information. The entire chain\xe2\x80\x94DNA to RNA to protein to metabolism\xe2\x80\x94has to come together, and if you remove an intermediate, such as DNA, you're probably going to lose vital information\xe2\x80\x94the DNA sequence you're building in your head right now.  ("If you think the laws of physics are going to let you pull some miracle out of your hat, you are sorely mistaken.")  If you start pulling bits and pieces out of the chain\xe2\x80\x94DNA, RNA, mitochondria, the hydrolase that uses ATP to pump ions out of the cell\xe2\x80\x94you will probably be losing vital information, because it's all entangled and interdependent, in a way a layman cannot fathom.</p><br><p>So the idea behind "Anthropic Rationality" is not to make a decision between different ways to account for observer selection effects. After all, every way to account for an observer selection effect will do. Nor is it merely a question of choosing a theory that has many particles in it. Suppose for simplicity the multiverse is governed by Maxwell's Equations of motion instead of Special Relativity\xe2\x80\x94every way of phrasing the equations in a wave equation does the same physics, the same probabilities, the same math. If we are going to start thinking of the universe as made up of parts, like a computer program rather than an infinitesimally fine amplitude distribution, the math is the same regardless. You have to start thinking of observer selection effects, rather than being indifferent between different ways of phrasing the math.</p><br><p>If, faced with two theories that predict the same data, our brains are more likely to start thinking "Oh, one of them must be right", then the same argument carries over to Solomonoff Induction\xe2\x80\x94which, just like any other induction, is an uncomputable prior...  The idea behind Solomonoff Induction is not to choose the priors that have the most "stuff" in them, but rather to use the simplicity of the prior as a heuristic for choosing theories which make more accurate predictions.  (That is, the prior probabilities that we attach to different theories in Solomonoff Induction are proportional to K(d(t)) with d(t) the Kolmogorov complexity of the theory t, which is the length of the shortest computer program on some universal Turing machine that computes the same sequences which the theory t claims to predict.)</p><br><p>In my previous post I described a thought experiment in which we can compute an exact Bayesian update (given certain initial priors) on how probable we should think the Great Filter hypothesis is, given various pieces of data <em>we</em> have seen.</p><br><p>But as this illustrates, we don't actually know the exact initial data\xe2\x80\x94all we know is that life evolved on Earth. Now, I have said very little about what our initial priors should be\xe2\x80\x94that's the big problem! My own priors were probably pretty bad, I suspect.</p><br><p>What's wrong with anthropic decision theory? Nothing! The question "How reasonable is this thought experiment?" is not a decision problem; it is not a binary between "This is a clever way to help" or "This is an obvious hack" or "I can do better by considering other hypotheses."  The question is "How probable am I going to think this thought experiment is a way to help?"  And that will depend on both the data we've seen, and the simplicity of the data.</p><br><p>But how you feel about the concept of "Simple Explanation" will depend on <em>that same question in the Solomonoff sense</em>.  (Which is to say, the probability assigned to the simplest explanation of the data you've seen is just another update to the probability you previously assigned to the Great Filter.)</p><br><p>The "Intuitive Explanation" of Bayes's Theorem is written for humans without formal mathematics under our belts. But still, let's say you already see the math. Maybe I'm wrong, and it looks like Solomonoff Induction isn't the way to go, but there <em>is</em> some nice intuitive way to think about it.</p><br><p>Suppose there's a magical button that can <em>automatically</em> show you the next thing in the Great Filter\xe2\x80\x94in such a way that it seems like a reasonable guess, with high likelihood. You press the button, and it shows you "Lifespan on Earth predicted", and you see that the Great Filter is off-Earth. But the magical button has these caveats attached:</p><br><blockquote>
<p>By "Lifespa on Earth" is meant a perfectly specific region, the Earth's crust and mantle (including a human-level intelligence to press the button).  And "Off-Earth" is defined as all parts of the Solar System not within 50 megaparsecs of Earth. This is in line with astronomical evidence that no aliens would be able to travel even close to the speed of light, while still being able to visit Earth. It should make the results of running the anthropic update look pretty sensible.</p>
</blockquote><br><p>If you like, you can also use a more general category of "alien" rather than "aliens", but then you'll need to update on having made the discovery. It <em>may</em> be that humans, and hominids in the past, were the <em>only</em> species to make this discovery, but it's probably also in the range of possibilities that we're the only species who'll make this sort of update.</p><br><p>But the button has a limitation to it:  The magic button doesn't know anything about <em>explanations</em> in biology, and so doesn't know that if the Great Filter is at the Earth, we also have to explain what Earth says at this point. Maybe if you give it enough data, it'll figure out what Earth says at different points.</p><br><p>Okay, but suppose that the button says "Lifespans on Earth predicted" <em>without</em> the limitation.  (Lemma 5.1 from <em>An Anthropic Principle Primer</em>:  If we're talking about a <em>complete</em> model, not a <em>sparse</em> model, it must have <em>a priori</em> probability at least 10\xe2\x88\x92100 that a given set of observations could come from that model.)  What would that update rule be?</p><br><p>Well, you could use Bayes's rule itself. The probability distribution P(data|explain) is how much you update on seeing a particular explanation, where "explain" in this case is the Great Filter hypothesis.</p><br><p>Supposing I take the Great Filter at face value, then if I just got this single piece of data "Life on Earth predicted", then my _a priori _prior probabilities for several different explanations, P(data|not UFAI), are just different numbers like 10\xe2\x88\x9220, 10\xe2\x88\x9215, 10\xe2\x88\x9213, and 10\xe2\x88\x9211.</p><br><p>And indeed, if you took those four prior numbers\xe2\x80\x94just the <em>a priori "non-anthropic"</em> probabilities\xe2\x80\x94and applied Bayes's theorem, you would, indeed, update to <em>P(data|not FAI)</em> ~ 10\xe2\x88\x9225, 10\xe2\x88\x9230, 10\xe2\x88\x9240, 10\xe2\x88\x9250... which would be, indeed, the posterior probability that the Great Filter has survived all the way to Earth.  (This is because the "data" we see has the form "Lifes on Earth predicted", which, translated across space-time, is of the form "Life on all Earthlike planets predicted".)</p><br><p>What if I get another piece of data, the data "The Great Filter is on planets that have liquid Earth"?</p><br><p>Then the posterior probability <em>given</em> UFAI is less than 1021/\xe2\x80\x8b2, and the posterior probability <em>against</em> UFAI, <em>P(data=x|UFAI)</em>, is more like 10\xe2\x88\x923.  (This assumes the prior probabilities are roughly similar and I update by Bayes's theorem.)</p><br><p>And thus the probability of UFAI is reduced by a factor of 10\xe2\x88\x923, as the Bayesian update on the Great Filter implies a very likely Great Filter indeed.</p><br><p>The magical button is now a little <em>too</em> persuasive. Suppose for the sake of argument that the magical button will show "Lifes pan(on Earth)" with probability 10\xe2\x88\x92100. But let's suppose there's one last piece of surprising data before the Great Filter is over:  The observation, "No intelligent species has visited Earth up until now, nor ever could have, on any of their alien planets."</p>      </span>    </div>  </div></body></html>