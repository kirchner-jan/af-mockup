<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        What is the point of learning rationality?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">eli_</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>What are the goals and benefits of learning what the rational thing to do is? I think this might be one of the most important questions which LW has answered, but I haven't seen that answered sufficiently. It's a fairly important question when we're trying to figure out how to shape society's response to AI Risk.</p><br><p>A common argument against having the discussion at an LW level is that "there's already lots of people who are rational people\xe2\x80\x94we don't need to talk about rationality again." Unfortunately, I have found that there is some truth to that. However, I think that it is the case that the vast majority of rational people either live alone, lack a sense of belonging or belong to a subculture where their "rationality" comes up only on Less Wrong. My proposal for the next few posts will be to figure out what you want to get out of rationality, why aren't you getting what you want, and how we can improve. I will talk about a few broad answers below. Other posts will follow which will include discussion on specific subtopics of learning rationality.</p><br><p>Here's how I understand what I'm getting out of rationality:</p><br><ul>
<li></li>
</ul><br><p><strong>Being able to solve interesting problems:</strong> My problem solving skills have gotten a lot better since I started reading about Less Wrong. I'm able to notice when solutions are missing, and when the problem isn't really one of the sort I need to solve. I'm much better at getting the best solutions of the available ones.  </p><br><ul>
<li></li>
</ul><br><p><strong>Improving your decision making:</strong> Another thing is that I've found that I am able to actually make decisions which I would be less likely to make if I had not read the information on Less Wrong. It seems like in real life, you're much more likely to do the right first choice, and think much more carefully about decisions. I think there's more value in having the ability to actually do things which are good for you.</p><br><br><ul>
<li><strong>Solving difficult problems:</strong>  While I can't actually solve any difficult real world problems, I think it is easy to see why I might not even have been able to solve them, if someone had explained the problem to me. I see a lot of people make decisions that they wouldn't be less likely to take if they had been given more information on the situation, so if you had told people about the AI risk earlier, they might have chosen to do what they now do.</li>
</ul><br><br><p>Of the above, 2 and 3 are the reasons why I'm even interested in learning rationality (others: 3, 4).  I want to find out whether I can find people like me (who care about AI safety /\xe2\x80\x8b rationality), or whether the best reason to learn rationality is if you have an interest in the social goals of rationality (e.g. making yourself better at solving problems /\xe2\x80\x8b making the right decisions).  In the case of rationality, we know that these social goals can be achieved by a variety of methods (e.g rational argument, logical decision theory, etc).  What Less Wrong does, is try to point people towards the best method of achieving these goals (i.e. the "rational" method).   However: I think the best way to achieve the social goals is to spend less time on "rationality" than on "just trying to be right", since there are many more opportunities to try to be right than there are opportunities to learn about how to be right.  To give a few examples:</p><br><ul>
<li>In a case of AI risk, I think there are much fewer opportunities to be absolutely certain that certain things will happen, so the most important thing is a general knowledge of the different positions and arguments on the issue.</li>
</ul><br><ul>
<li>I think that being a good person on a regular basis is a higher likelihood to want to change society in a direction that is closer to what a future with superintelligent AI than most AI risk.</li>
</ul><br><ul>
<li>I think knowing exactly how to break an argument in a way that increases its probability of being right helps with the problem of people having beliefs which they feel the need to defend.</li>
</ul><br><br><ul>
<li>The social goals and rationality goals are very similar\xe2\x80\x94both are goals which don't require having high ability (e.g rationality) for achieving them.</li>
</ul><br><ul>
<li>Most people do not care about anything so complex as AI risk/\xe2\x80\x8baltruism because they get very little practice in doing so. We don't have a very good model of how complex problems should be approached.</li>
</ul><br><p>And I think there are probably lots more. Overall I think the social goals are much more likely to be the main reason for people with a certain personal goal X to learn "rationality" on LW (i.e how to make decisions when they're less likely to be right).</p><br><p>It seems to me that if I was just solving problems in my life or helping others (without having a specific interest/\xe2\x80\x8battention in the problem of AI safety), then maybe I didn't feel like "learning rationality" was worth my time. That would mean I could solve the problem more effectively, or at least not waste my time learning what I didn't really need. Similarly, I don't care for the problem of political or existential risk that I will be less likely to tackle if I learn about "rationality".  But it seems like one can have a particular interest in the problem that would lead them to have more interest in "rationality" from a social goal perspective.</p><br><p>(I have a strong belief in the idea that rationality is not necessarily "about" winning; in the course of pursuing a goal you are often going to be less successful than you might have been. Also I think that learning "rationality" may make you less likely to try hard things, in favor of trying less effective things. I like to think of learning "rationality as a process of learning how to decide when to try hard things and when it is better to do things in an effort to be right").</p><br><p>A few follow-up questions for people who have similar goals to myself:</p><br><ul>
<li>What other goals do "learning rationality" help you with? I think it is common to have a specific goal in mind (say, "helping others") and "rationality" helps with that goal. However, what else do people with social goals in rationality care about, or what sort of goals tend to help more with "rationality"?</li>
</ul><br><ul>
<li>What is the most important thing I would have learned by reading Less Wrong? One thing I would like to learn is how to be more confident about what I believe; specifically I want a general sense of how likely my beliefs are to be correct in the first place, and in particular I would like to know how to be more accurate than I am currently.</li>
</ul><br><ul>
<li>What areas have fewer learning opportunities to help with your social goal? I think there are a lot of areas where being "less wrong" comes up regularly, such as the arguments, biases, and fallacies section of the wiki. I think a lot more of my time goes into these areas than into the ones which have a large amount of reading, but I'm not completely sure. In fact I don't understand how it could be otherwise, since most areas of my life which are important to me don't come up very often. If someone else has a sense of this, I'd be grateful to hear about it.</li>
</ul>      </span>    </div>  </div></body></html>