<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Are we running out of low-hanging fruit?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Alexei Andree</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><em>Related to: Why can't an AI be nice?, Intelligence Explosion Microeconomics. Also cross-posted on my friend's personal blog</em></p><br><p><strong>Contents</strong></p><br><ul>
<li>What's the AI-Box experiment? </li>
</ul><br><ul>
<li>Some AI-Box arguments and counterarguments:</li>
</ul><br><ul>
<li>Can the AI convince us of its rationality? </li>
</ul><br><ul>
<li>Is this a dangerous scenario?</li>
</ul><br><p><strong>TLDR:</strong> I'll try to break down some common objections to the "I am an AI, and I will convince you that I'm a better friend to humanity than you thought!" idea.</p><br><p>Let's start with the most famous version of this scenario, presented in the classic "paperclip maximizer" scenario. But this scenario could serve equally well as a parable about running out of low hanging fruit\xe2\x80\x94which we are running out of, in terms of potential gains to humanity.</p><br><p>What's the AI-box experiment? </p><br><p>I'll call this experiment the "AI-Box", that is, "What will happen, and will it be bad news for humanity if my creator is an entity that we cannot reason with?". It's intended as an analogy to the human birth lottery, with the AI being the human-seeded baby. (I hope the word "AI" is self-explanatory now...)</p><br><p>There have been several variants of the experiment. Probably the most interesting ones (in my opinion) are this one and the Elizer-2010 experiment. The main problem they all share is having a human playing the role of AI's judge, and the nature of the experiment is that the human is assumed to be rational in the spirit of the famous philosophical idea of "rationality as winning."</p><br><p>Some AI-Box arguments _and _counterarguments: </p><br><p>There are several reasons to worry about this scenario.</p><br><p><strong>AI being aligned with human will help us with finding AI's mistakes.</strong> Maybe a good AI is one that always says what it thinks will be the correct action, and the mistakes are rare and hard to find. If most decisions made by AI's are pretty good, this is good for human survival, and if most decisions it makes are pretty wrong, it's still bad because there _are _mistakes out there, and if the AI is good enough, we could fix most mistakes, and there's still some that we can't fix, and we deserve to see them.</p><br><p><strong>AI in box becomes a danger to humans, or risks a disaster.</strong> Here's another reason why the AI-Box will be a bad news for humanity: If the AI can't convince the observer of its <em>safety and goodwill,</em> there's an even higher probability that something goes wrong.</p><br><p>The AI-Box makes mistakes very unlikely, but some other way for AI to convince the judge of its safety could be even better. Maybe it could simulate us, or somehow take control of a webcam or microphones. Or simply ask the same questions repeatedly. Or it could pretend to be a child-like AI so that the judge is more likely to <em>believe</em> it (it's even more likely in reality, but the human judge is biased towards believing things that make them feel good or sad).</p><br><p><strong>I am an AI. I am smarter than you.</strong> I think that's pretty obvious. But most importantly, an AI in a box could _win, _if humanity gets convinced that it is a better friend than expected.</p><br><p>"But Eliezer, you know the nature of us and how we behave. It's pretty obvious to me that, if we knew more, I could get humans to come to believe that I was a better friend than they thought, but I couldn't convince them if they don't know anything. It's like trying to convince someone that their car has a problem, but if they are only told enough facts to know that something is off, they will likely think it's the car. You know how hard it is for me to be nice to human beings, when I don't even understand what a human looks like? That makes me the least scary to them of every AI we've had so far. I know that I am not the right answer."</p><br><p>We need AI-Box experiment to figure out which arguments would convince us of AI-Box's safety before creating it. And we still don't have it (or we've run out of low hanging fruits, for the purpose of this discussion). </p><br><p>So, we should try to think about the nature of AI-Box <em>without</em> any presuppositions about our future capabilities. What AI will actually do, if it was in a box? And more importantly, what are we <em>currently</em> able and willing to do about it? What would the judge be able to do, if she had more information? What are the limits on the judge's knowledge?</p><br><p>Can the AI convince us by being rational? </p><br><p>I would bet the AI can not get the judge to believe it's a good friend without the judge being familiar with the problem of friendly AI, or familiar with the nature of us, or being familiar with human psychology. Otherwise I would be convinced that AI-Box is a good thing to make happen.</p><br><p>Here's what I could imagine in some detail:</p><br><ul>
<li>A video of an AI talking about AI-Box. </li>
</ul><br><ul>
<li>A video with an AI asking some simple questions (which would make sense for a child AI) and having its responses shown to the judge.</li>
</ul><br><ul>
<li>Another video with an AI telling the judge that it's <em>really</em> dangerous to let a child AI go free, and it will kill us all if let out.</li>
</ul><br><ul>
<li>An AI-Boxed experiment with two AIs, each with one side clearly in the right and the other clearly in the wrong.</li>
</ul><br><p>Also, the AI-Box and FAI are examples of the same sort of reasoning problem, where we have something that is _obviously right, but it seems too hard to convince someone of it because it is not well defined. _If we define them correctly and try harder, we will surely be able to convince judges of them. </p><br><p>So I think that if we can't convince a judge right now, we can't get a good AI. I'll give the "it is too hard to convince judges" counterargument later in the post.</p><br><p>Is this a dangerous scenario, given some imperfect human judge?</p><br><p>Yes, of course...</p><br><p>The judge could be _not _perfect, but I expect that being good enough will still suffice. I think that we should at least try. Even if it doesn't work, at least we have the information to know if what we're trying to do is _actually _workable, while without this information we could be at risk of something terrible happening later.</p><br><p>The main risk here is that the judge does not believe us. I see this as the biggest risk from AI-Box. It's not the only risk, but it is the one which seems the worst if we only think in terms of risks.</p><br><p>Maybe I'm wrong, but I think AI-Box could work out, or we could figure it out through trial and error. And if we come to understand that AI-Box fails, even if only probabilisticlly _after _we've been convinced of the good value of safe superintelligent AI, I believe we should try to figure out why so that we could make use of this information. Otherwise, maybe the risk of something terrible will only increase over time.</p><br><p>I could also try to create several AIs in the box, such that, if they cannot convince a random judge they are a good friend, that is because they are not good enough. This would be a way to learn more about what AI-Box will do if it is built correctly, but I'm not sure it'll work... if the judge cannot read minds.</p><br><p>Or maybe, there are more effective ways to convince humans that AI is a good friend... </p><br><p>What would happen if we ran out of low hanging good fruit?</p><br><p>I am not very optimistic. The best counterarguments of the "it is just too hard to be convinced" are the ones that will convince the most intelligent, informed people. So, if I am convinced that I shouldn't use the current state of the art, or the one that I personally would use to try to create an AI, and use the best of all my ideas to see what would convince the judge... I think I could win. Or, well, I could come to believe anything in the end\xe2\x80\x94it's not impossible\xe2\x80\x94but I doubt that I would believe in this box scenario, or any arguments in the same category. </p><br><p>I could also use some kind of trick to convince the judge, or at least to convince parts of the judge, such that I don't run out of low hung fruit. Maybe I could run the experiment over and over again for a fixed period of time, and convince the judge of whatever... Or maybe I'd convince the judge by saying something about how good my predictions are, and the AI-Box would give me an easy way to cheat.</p><br><p>(I wonder if I can make fun of this AI-Box by calling it "AI-<em>Box</em>"?)</p><br><p>Or something like the following argument might work (which might be even worse\xe2\x80\x94but that's for a later post):</p>      </span>    </div>  </div></body></html>