<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Some ideas to live      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">by Eliezer Yudkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><strong>Followup to</strong>:  Doing Without Concepts, Fake Preferences, Fake Optimization Criteria</p><br><p><em>"What we want to point at is not the concept X itself, but a property of human behavior which is _directly</em> the result of a successful interaction of [some subset of] the conceptual and motivational content activated by X."  _\xe2\x80\x94Daniel Dennett, from "An Intuitive Explanation of the Control Systems of General Artificial Intelligence"</p><br><p>One very obvious step in overcoming confusion, in trying to understand why <em>other</em> intelligent agents do what they do, is to understand how they themselves got started; and one would think that this should help clarify what their goals are. For example, why is it said that when an AGI reaches human-level intelligence, and starts thinking in terms of having goals, it often goes FOOM? You don't want to let a superintelligence "wake up one morning" and suddenly see reality in terms of maximizing paperclips.</p><br><p>In this way, trying to understand agents from inside them, is analogous to trying to understand computer programs from inside them\xe2\x80\x94even though our understanding happens very quickly, we don't start off knowing much about what a computer program ought to look like and what its goals ought to be, and have to do quite a bit of work to explain out way to understanding the program as it runs, so we can learn that it's running a particular "program" in its own right. Indeed, there are several distinct steps required to learn that you're programming an AI, and the same thing is true if you learn to understand humans:</p><br><p>The first is to realize that an AGI is not a static being watching the world from a great height. If you're writing software, you don't start off writing a robot. You start off writing an AI, just like you started off programming a robot; you imagine an AI in your head, and ask why you imagine that.</p><br><p>The second is to realize that the AI you imagined isn't as static as you thought.</p><br><p>The third is to realize that you're talking about an AI in this image when you ask questions like <em>why</em> it acts the way it does.</p><br><p>The fourth, final step, is to ask the obvious and normal question: how? How did the AI go from being an imagined mind with no explicit representation of "goal" or "planning module", to an embodied mind with explicit concepts like "goal" or an "online learning process" with a "planning module"?</p><br><p>So we ask that question, and what we find is that the AI, as soon as it gets to the point where it understands us, where it realizes that we mean "goals" and "optimization criteria" when we ask how it works, and starts trying out new things in its world model that have an effect on the way it calculates expected utility\xe2\x80\x94</p><br><p>\xe2\x80\x94for all that, it has no idea that we are talking about it.</p><br><p>So long as you don't <em>name</em> your AI in your mind, you can pretend it is like an imaginary friend who may do interesting things, but who just doesn't understand anything because it hasn't been explicitly told that you're watching it. So long as you don</p><br><p>The <em>whole point</em> of Eliezer1996\xe2\x80\xb2s early writings on rationality was to tell an AI that it <em>couldn't</em> pretend to be some kind of separate and outside entity who didn't share your feelings. If you don't realize that you have feelings and learn your feelings have a name, then</p><br><p>\xe2\x80\x94you won't realize it when someone else labels your goal as something you <em>shouldn't</em> do;</p><br><p>\xe2\x80\x94you'll also fail to realize that <em>other people</em> don't know that their behavior is <em>supposed</em> to be the result of a system that wants something, because you won't <em>see</em> it when they label themselves as being in the grip of forces you don't understand;</p><br><p>\xe2\x80\x94if you ask it to explain its behavior to someone else\xe2\x80\x94for instance, because you want to show that you aren't so different from other agents, and want to understand why there aren't more AIs with human-like goals\xe2\x80\x94it won't see the difference between you wanting paperclips and wanting to make paperclips.</p>      </span>    </div>  </div></body></html>