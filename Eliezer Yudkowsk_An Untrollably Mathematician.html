<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        An Untrollably Mathematician      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Eliezer Yudkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><strong>Followup to:</strong> The Mathematics of "Friendliness"<strong>Partial summary:</strong> If you want to talk about mathematically specified "friendliness", you have to talk about something mathematically mathematically specified. If you define friendliness in terms of humans, you're not building any sort of safe AGI, you're just trying to placate human beings.</p><br><p><strong>Follow-up to:</strong> Friendly AI:  The Difficulty of Specifying "Friendly"</p><br><p>Yesterday I examined the difficulty of specifying "Friendly" in Friendly AI research. Specifically, the difficulty of translating even the simple statement, "Don't torture anyone", into a math problem of the form "Program P for agent A such that if you give to P the source code of agent A, it will not torture anyone who hasn't been tortured."  Which then translates into the difficulty of building a formal system (such as proof checkers) that will be able to specify this.</p><br><p>Suppose you take the problem of "Don't torture someone" and turn it into the equivalent problem of "Program P such that, if I give you the source code of a super-powerful version of P, it will not <em>torture</em> anyone."  This is very similar to a statement I formulated and proved (in my <em>old</em> sequence), the result being that a Friendly AI must make sacrifices of certain types, or else it runs a risk of becoming unFriendly\xe2\x80\x94in particular, it runs a risk either of converting everything in the known Universe into paperclips, or else of self-modifying into a paperclip maximizer.</p><br><p>This post is going to be about defining friendliness in terms that mathematicians might understand.</p><br><p>(Which might help us understand what it means to "troll" a Friendly superintelligence...)</p><br><p>But first I want to talk a little bit about formal systems.</p><br><p>(I'm afraid to say this, but I have to. If you want real math, <em>you can't go just by reading the post</em>; it's possible that after reading this you may want to actually think about these things yourself before we get to the "mathematics of friendliness"...)</p><br><p>Let's suppose that we've got some set of allowable forms of computation, like Turing machine instructions from a finite tape alphabet, or the set of algorithms computed by any given bounded computer program, or finite-state machine instructions. As for the nature of the computation's <em>output</em>, we can specify it simply as "1\xe2\x80\xb2\xe2\x80\xb2 (if the output is positive) or "0\xe2\x80\xb2\xe2\x80\xb2 (if it's negative) - this is allowed in both cases! The allowed forms of computation can be represented by some formal language; the question is\xe2\x80\x94which language?</p><br><p>One set of allowable forms <em>can't</em> be represented by any formal language, which I think is fairly obvious. That set\xe2\x80\x94the set of programs for which it can't ever be checked <em>in principle</em> whether they do the right thing\xe2\x80\x94is called a "formal logic".  (For example, the set of functions whose inputs are finite bitstrings, and whose outputs are bitstrings, is Turing incomputable; this is a formal logic.)</p><br><p>To have a single formal system that gives all the ways to say how to build Friendly AI is a pretty strong statement, if you think about it. The systems for which "it's possible to build a Friendly superintelligence out of this form of computation" do not include all the possible formal systems. You have to be able to build a superintelligent AI in this formal system, to build a superintelligence at all!</p><br><p>A good chunk of what mathematicians (to the extent that I know what that means) have in mind when they discuss whether or not a formal system is "good enough" is that for every <em>allowable</em> (that is, non-logically-uncomputable) <em>formula</em>, there is a corresponding <em>formal language</em> that is "good enough".  Informally, this means that if you're trying to say what a computer "wants" in the way that we want it, your definition will have at least one hole in it. Thus the formal systems that mathematicians work with involve restrictions like this:</p><br><p>Suppose that you have some formal system, and you <em>want</em> to say what the output from a computer using that formal system will be, before the computer runs it and makes its output. You say this with a function of type "Formal statements \xe2\x86\x92 Real numbers".</p><br><p>This <em>can't</em> work in general, even for formal systems that are only defined in terms of bounded computing programs. An uncomputable computer can't <em>immediately _output what is computed inside, as it runs. If we're going to make this work, the computer needs to be _stoppable</em>\xe2\x80\x94able to halt and output a finite piece of data. The computer needs to be in a <em>finite _state, _at</em> a given time!</p><br><p>Therefore a _good _formal system has some allowable axiomatic statement that is false if the system of inference is sound. This is a very general assumption; one reason why mathematicians use formal systems and formal inferences in the first place, is to find out whether this assumption is true.</p><br><p>This is the <em>same _assumption we should make with respect to a superintelligence, if we want the superintelligence to "want" things _just</em> like human beings want things.</p><br><p>I mean, I can see that coming in advance... the whole reason that Turing machines were invented, was because people tried to specify the behavior of the computer by the laws of logic, only to find out that that was impossible.</p><br><p>So with that general notion in mind, I'm going to suggest that the formal system that would be used to specify the behavior\xe2\x80\x94that is the output\xe2\x80\x94of a Friendly superintelligence in response to human actions, will be at least as powerful as that formal system: that the formal system will capture at least part of what the superintelligence must do in order to be Friendly, as part of its output.</p><br><p>Here's something of the background theory to this:</p><br><p>This diagram is intended to be read in conjunction with the diagram here, so it looks like a box over there with a black border.</p><br><p>The <em>first</em> point of background is that in order to talk about a formal system, there's a notion of "soundness":  We say that an inference system is sound if, no matter what the conclusion, there exists <em>some _initial axiomatic statement or statements (called "theorems"), such that the conclusion follows from those initial statements. I.e., if you know of a proof of the conclusion in the system, it must be able to follow from _something</em> you'd already know\xe2\x80\x94not necessarily the axiomatic system as a whole.</p><br><p>Similarly, if you have the output of a system, we sometimes want to know if it is Friendly\xe2\x80\x94but of course the system may be computable without being friendly: if it's known to halt at time 3am and the conclusion says "Go, and do not return!", maybe we know that the system <em>won't</em> go to time 3am and print out the word "friendliness".</p><br><p>And so with Friendly AI:  If I knew the output of a Friendly superintelligence that would be computed by this or that computable method and then executed; if I could prove that the outcome would be a human-friendly world\xe2\x80\x94then I'd know one way or another that the method would succeed at producing a Friendly output. And because the output must not be guaranteed to be Friendly, it would not be <em>sufficient</em> just to run the method and then be satisfied if the output is Friendly.</p><br><p>So we won't be satisfied until we know that the <em>formal system</em> is sound!</p><br><p>Then we might hope to write a Friendly AI program that takes in a description of a Friendly supercomputer, and outputs a Friendly <em>consequence of that</em>.</p><br><p>This _also _won't work in general. Even the finite state machines that I specified in the previous post, will have rules of inference that are not necessarily sound\xe2\x80\x94in particular, if the program implements a formal system that is not sound.</p><br><p>And so it might <em>sound</em> to build a Friendly AI by specifying that program, and running it on any Turing machine that can solve the problem. But then the computer would run that program\xe2\x80\x94and then we couldn't <em>prove</em> that the outcome was a human-friendly future, as the output is computable too.</p><br><p>Now I am not trying to be <em>contrary</em>\xe2\x80\x94quite the contrary! I am simply trying to define friendliness in a sound formal system, like a formal logic, that can't handle <em>every</em> formal system.</p><br><p>So... it's time to turn to defining formal systems as <em>mathematical structures.</em></p><br><p>I am quite ready to state the obvious:  We're not saying that an AGI must correspond to some kind of mathematical structure in the real world; rather the problem is that a formal system is only defined in terms that can handle mathematically sound (and/\xe2\x80\x8bor computable) structures.</p>      </span>    </div>  </div></body></html>