<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        The Future\\xe2\\x80\\x94An Unknown Quantity      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Alex Flin</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Link post</p><br><p>Contents</p><br><ul>
<li>Foreword</li>
</ul><br><ul>
<li>Epistemic Status and Personal View</li>
</ul><br><ul>
<li>The Beginning</li>
</ul><br><ul>
<li>The Middle</li>
</ul><br><ul>
<li>The Ending</li>
</ul><br><ul>
<li>Footnotes</li>
</ul><br><p><em>Note: This essay is old and I recently read it again. I thought I would put it up here for reference sake and possibly for someone else who may be interested in the topic. It's still relevant to how I think about AI and there was no big update between then and now, so maybe that's a sign it's a piece from way back when.</em></p><br><p>Foreword</p><br><p>I'm old and crankish and I've been reading through the AI Alignment Forum since before you started it. I've learned a lot from this and I hope that my writing here conveys it fairly accurately. On top of that, I don't know if this post is new, so if you didn't read it before, I really encourage you to do so now before I go on.</p><br><p>There are places where I disagree with the rest of you about the likelihood of AGI, or about the likely impacts.</p><br><p>In the interests of time I will mostly not talk about these since I'm pretty sure I will be wrong about them.</p><br><p>As you know, I used to work at the Singularity Institute. I still care about all the Singularity ideas in their most technical form. But for a long time I was just a cog in the wheel. I did not write a lot about what I thought about the Singularity. As a result, I have no inside view here. I am an outsider who wants to say some things about the inside of this community.</p><br><p>What I found was a lot of intelligent interesting people. So before I talk about the details, I want to talk about the larger community. I want to get to knowing <em>you</em>.</p><br><p>Epistemic Status and Private Knowledge</p><br><p>First off: I am an epistemic crank. I am not an expert in any empirical field. I do not know about machine learning or even about human intelligence. It's not that I think I know enough to make my own predictions about AGI. I do not have enough knowledge to even have an opinion on such issues. It's that I have enough knowledge of computer science and of human rationality that even given all that I still have an inside view about what people in the AI Alignment space believe, but what they don't believe is still a secret inside that view. When I think about their secret unknowns, it is that which I infer. I know about what they expect but I don't know what they expect to know.</p><br><p>Another thing about this place: there is a lot of personal and epistemic privacy in the discussion. That is to say: we try to build and maintain a community of people who trust one another, but there is a strong tendency to make things public. It can feel like people have a right to know what is going on in anyone's private head about anything they want to know about. That means, sometimes if I'm trying to share an inside view I may be tempted to put in it that which I think others would think would be inside information. (There are other things I could put into it for other reasons!)</p><br><p>The more one knows about the subject matter, the easier it becomes to know all kinds of things about it and to come up with predictions about all kinds of stuff about it. People may not even see you as a crank. And this has led folks to believe that cranks don't know anything. I don't know any of this stuff yet so you may think that because I am a crank that I ought not to make predictions; that I am just trying to have an opinion about this stuff.</p><br><p>On the other hand the fact that I still have these things in my head is itself an epistemic truth. And this is a point that I have to be careful of lest I start talking as though I know more than I do or that I am more knowledgeable than I am just in the hope that I can be taken seriously as a person who knows everything.</p><br><p>The Beginning</p><br><p>A while back I was sitting down with other people working on AI Alignment, trying to come up with a list of open research problems. I think I will get into that later if you want to know; it is something I did for a while that I still think is worth writing about.</p><br><p>Anyway, at the time I came up with something new about what might happen next in AI in general and in particular about AGI.</p><br><p>I said that I thought AGI might be a long way off and that I did not expect it to be imminent, that it might not even be possible to define the thing that is being extrapolated by the AGI community.</p><br><p>I suggested that the thing we could say about the extrapolation of the AGI would be its motivations. But I did not think that the kind of motivational extrapolation that humans do is sufficient to tell us what we want. And I also suggested that we don't know what we want and therefore are not prepared for what the machine's actions might be.</p><br><p>I suggested instead that we look at human values and ask what happens if you try to transfer that knowledge to the machine and feed it human values.</p><br><p>I said the result of that procedure would be another AGI, but with human motivations instead of what we would be looking at as human values.</p><br><p>Here is something I said back then:</p><br><p>We don't know what AGI will do, but we do know that human values are the kind of thing that AGIs have no reason to care about. So we can say: what would happen if we feed the AGI human values in a particular way?</p><br><p>We would have AGIs that would share our values but that would care about much other things, too, things that we really care about.</p><br><p>That's what I meant when I was talking about it not being possible to define what AGI will be like. Not that it never will be able to be defined. But that I meant we can't say what it will be like at all. Because whatever it will be like, it will be very different from what we would care about.</p><br><p>The Middle</p><br><p>That was a long time ago and I spent a lot more time on that note than I realized. It took me a while to go from there to here.</p><br><p>But a lot has changed since then. I think most of what has changed is that people have had way more opportunity to become confused because they tried to think about what AGI might be like and found it too hard. Or at least that's what I and the people who write AGI risk analyses for a living think. And we are all trying to say the same thing because we are all interested in predicting what might happen and trying to work out what we wish to come to pass. But we are also using different names for the things we are pointing at.</p><br><p>Some folks, though, were already confused by that point. They were just still trying to use the existing jargon and make themselves look good with what they already knew about. And that was really important to them for a time.</p><br><p>I was just one of those folks who was already confused and thinking about how to explain what we were confused about. And then one day I realized that I was confused about <em>everything</em>. And I started to try and make my own models of things, in my own way, so that I may at least have a chance to understand what things I and the other folks in this community might be confused about.</p><br><p>I said in my last post that to say that we don't understand AGI yet is to say that we have no idea what AGI will look like, but that does not follow. We want to know why we don't see a good model of AGI yet and then we will be able to make progress.</p><br><p>The first problem is to understand what exactly is a "good" model of AGI. I think there are ways to be more precise than "good". As a toy example (not really useful) imagine that I am trying to help you figure out what kind of thing you want in the future that is possible to find in the physical world using science. I say "here is how I see AGI" in the hope that you will find it interesting or helpful. I say "let's talk about that" and in case it is not actually what you are looking for I say "perhaps I should talk about the question you're really interested in instead" and try to help find an alternate model that might be what you want.</p><br><p>We already have, in at least one case, at least one model of AGI that has come to our attention and has been suggested as a model. If we have no ideas about why we don't have that model yet there is some hope of figuring that out and maybe coming up with a different model and seeing if we can find one that works instead.</p><br><p>It doesn't mean that you think the model is literally that one we have. Maybe there exists a second model that could work that we don't even consider. It means you don't have a model at all, but you do have some ideas about why you don't have the model. That is a different kind of thing.</p><br><p>Then there is the question of whether we should talk about what we might do to actually understand the thing we are trying to understand. That is an abstract question about the kinds of things we think we know and our models. So I try not to get excited about answers to that question. Not yet at least.</p><br><p>The Ending</p><br><p>I have now made my models. They may not be accurate and I may not have worked on them. But I'll try to get into more of those details soon.</p>      </span>    </div>  </div></body></html>