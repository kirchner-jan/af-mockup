<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        The AI Timelines Scam      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Scott Alexande</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>__Related to: __Timelines, Intelligence Explosion, and Other Such Things</p><br><p><strong>Summary:</strong> AI researchers use very similar techniques to reach very similar results regardless of what system is on the path to artificial intelligence ("AI"). Some people have argued  that AI will be developed before 2030. This argument is based on the proposition that because AI researchers use similar methods, their results are likely to be similar. I will argue that AI researchers  use different methods and may have reasons for doing so, and that  therefore they can't be considered interchangeable with one another.</p><br><p>AI researchers tend to assume certain things about what artificial intelligence should and should not be able to do. For example, if you want a program that can give you good medical advice about a complicated medical diagnosis, researchers tend to want it to have knowledge about the disease, it have the ability to learn more about the disease if it so chooses, and to be able to give a good prognosis and to avoid recommending harmful drugs. If  you want a program to be intelligent enough to operate on your computer, then  you usually want it to know a lot of facts about how computers work and  be able to manipulate them at some level.</p><br><p>For purposes of this discussion, assume all of those things are important criteria. Unfortunately, a lot of the work done in AI is not focused on these criteria. Almost all the work done on AI is about using the same "cognitive" tricks over and over again, and using those tricks the "correct" number of times per trick.</p><br><p>I'll take one example as a model to show how researchers can come to seemingly similar but in reality very different conclusions. There are a lot of papers published about image recognition. The idea is to use many "cognitive tricks" to recognize a variety of objects from various angles. For example,  you can combine the information from several cameras of different angles to locate the objects in the field of view and then use a "carving" algorithm to break a blob into shapes, which are then recognized by a machine learning algorithm.</p><br><p>In one paper, a group of researchers used seven hundred such "cognitive tricks". In another paper, they used a single trick. In another, they used a different kind of algorithm and didn't explain it at all. And on and on it goes.</p><br><p>All seven hundred tricks were based on exactly the same mathematical ideas. For one reason or another the group was studying the same objects using the same angle of camera angle. I would guess that most researchers would agree that the group's "cognitive tricks", whatever they were, did a lot of useful image recognition and that even if the one trick they chose wasn't as effective as other tricks, it still probably works for something that might not have reached artificial intelligence yet.  (There may also be other arguments for choosing one over the other among the researchers involved; for example it may have been used to achieve some special milestone in AI capabilities.)</p><br><p>But once we have image recognition (and even if we don't, it's interesting for purposes of this argument), then it seems like we have a very good candidate for AI research: figure out what image and how it looks are predictive of object position and orientation in other images. Once we have this  image recognition code, it's not too difficult to figure out how to use it in a whole bunch of other image recognition papers, using a wide variety of angles of camera and different approaches to image "carving". And once we have a few of these image recognition papers using it, we can build on those by adding more image recognition methods to it and combining them to get better recognition results. So  one might reason from here that even if image recognition isn't really a very powerful mode of AI (though I think that's unlikely), it's an important and useful and therefore probably necessary mode for AI that's less powerful than human level.</p><br><p>It does seem that at some point in some AI research project we might hit a wall. In other words the code could be built out of so many "cognitive" rules that the idea of it being able to have knowledge or learn anything or manipulate the world isn't the important thing anymore\xe2\x80\x94what's important is that it can pass some tests. Again, for various reasons, maybe some tests are more useful right now for achieving a special milestone in AI, maybe some tests will have more relevance in the future after we hit better and better ones, maybe some tests won't matter until some test is achieved that requires mastery of the sort of thing humans did when we built the first AI and those tests may not show up until decades later.</p><br><p>Anyway, once the researchers at DeepMind have achieved an AI that passes any tests they've set up, that's it, we have artificial general intelligence. Until they've done so, nobody knows that maybe AI is going to be developed before 2030, and the only reason we might have a guess at a date is because someone published it on an internet forum and now lots of people are posting guesses about dates they think AI is probably going to be developed.</p><br><p>Here's another example on the other side of the political spectrum. In his recent book <em>Singularity Hypotheses</em> Robin Hanson cites a paper from 2000 discussing the feasibility of AI within his lifetime based on a particular cognitive architecture. The paper did not find that it would be possible to create AI this century, but gave this as a <em>possible</em> scenario for the future. A couple of years later, Eric Drexler wrote a book in which he argued (using a different kind of AI architecture) that AI would happen soon and by the year 2010. This was published in a widely-circulated journal of the Institute of Electrical and Electronics Engineers, even though it did not find that there was any particular reason to suspect that it would be able to happen soon. Again, for some reason it seems to have proved popular in the circles of futurists and transhumanists, so it's hard to find an actual expert who was shocked by it.</p><br><p>This may be the closest thing yet to a consensus on this issue among the major research communities studying AI (well, at least among the major researchers who have given their work to publication). But Hanson's paper and Drexler's paper are two very different methods of talking about AI, two very different sets of possible outcomes. If Hanson and Drexler were to have the same discussions about their work, or were to be reading the same books on AI, you would not have to be either an AI researcher or a researcher in AI ethics to be alarmed.</p><br><p>Obviously Hanson is not the only futurist who has claimed we'll get an AI soon and Drexler is not the only AI researcher who has responded that we might be wrong for completely different reasons. As far as I can tell there's no evidence about which view is wrong and which view is right among the AI researchers who aren't futurists or transhumanists. This is a sort of self-fulfilling prophecy going on: if people who are interested in futurism talk about AI soon, they will continue to do so no matter how long it takes us to actually make one, and whether we make one or not, people who care about AI ethics will respond the same way anyway. There isn't a serious argument about AI being developed in the next fifty years going on right now among AI researchers who aren' the most interested in futurist or transhumanist ideas.</p><br><p>So this is the kind of situation that gives me pause about the "intelligence explosion" concept and other similar ideas in AI where a bunch of futurists come together in the same room and discuss an idea and then make a claim about how it will lead to the creation of AGI soon. The claim is that they're using one particular method of thought to analyze the idea and so they'll come up with one particular way it will go down, and another bunch of futurist will come along and respond with their own argument that is based on one different method of thinking and so on. There seems to be no reason why one group of futurists should be closer to reality than another.</p><br><p>So I think we can take this argument that Hanson and Drexer have the same argument about AI, take it seriously, and find in our own research a large amount of evidence that we aren't going to get AI in the next twenty or fifty years. We aren't even close. (I don't know if a Hanson-style argument could make progress on this if we just made a paper on the level of Drexler's argument. What I've written suggests that the Hanson-style argument is wrong, but I don't know why.)</p>      </span>    </div>  </div></body></html>