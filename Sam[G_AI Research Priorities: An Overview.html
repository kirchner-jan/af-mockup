<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        AI Research Priorities: An Overview      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Sam[G</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>Core AI Goals and Values</li>
</ul><br><ul>
<li>Learning Values</li>
</ul><br><ul>
<li>The "Learning" Paradigm: Imitation, RL, or Learning?</li>
</ul><br><ul>
<li>Value Learning</li>
</ul><br><ul>
<li>Value Extrapolation</li>
</ul><br><ul>
<li>Interim Rewards</li>
</ul><br><ul>
<li>Miscellaneous Other Research Areas</li>
</ul><br><p>This is the sixth post in the AI Research Considerations for Human Existential Safety (ARCHES) series. In it, I provide a high-level look at AI alignment research from the perspective of a typical academic. You can find the other posts in the series here.</p><br><p>One possible approach to the challenge of making AI systems aligned with user values is to study the abstract principles through which values can be learned, extrapolated, and embedded in reinforcement learning systems. This is a fairly broad space, with plenty of overlap, and is often referred to as "AI alignment", with multiple, overlapping research areas falling under its umbrella.</p><br><p>But what exactly is "Alignment"? One possible framing is to think of it as referring to the problem of making AI systems that act in ways that are <em>good according to human values</em>, even when those AI systems don't yet understand human values as well as we do.</p><br><p>How do we solve this problem? As the example of chess shows, we can train AI systems to understand human values and do actions that we endorse, or we can try to create AI systems that help us understand human values better. I consider all of these to be variants of <em>value learning</em>, and will in no way be surprised if you don't immediately think of it that way. Let's begin with a high-level overview.</p><br><p>Core AI Goals and Values</p><br><ul>
<li>The goal of AI alignment research is to ensure that AI systems that are developed, trained, or used often go about executing a <em>properly defined</em> <em>core set</em> of goals and values.</li>
</ul><br><ul>
<li>__Core values. __These are the values a core set AI would pursue if it had <em>full</em> or <em>unrestricted knowledge and control</em>.</li>
</ul><br><ul>
<li>__Core instrumental values. __These might be instrumental values like self-preservation or efficiency, or they could be instrumental values like resource acquisition, status, or the ability to get what one wants.</li>
</ul><br><ul>
<li>__Modulo. __For a few reasons, it's reasonable to make and keep distinctions between core values and core instrumental values. For example, most ways of acquiring resources could be said to be pursuing <em>efficient</em> instrumental values, even if they're ultimately <em>instrumentally</em> <em>causing</em> resource acquisition. We could thus make a distinction between core values that are core <em>because</em> of instrumental reasons, vs. core values that are not necessarily core <em>because</em> we care about them instrumentally, but are nonetheless core.</li>
</ul><br><ul>
<li>In addition, while some instrumental values like power are in some senses "natural", in the sense of being useful for achieving a wide range of goals, other instrumental values might be specific to a very narrow context. Thus, for example, self-preservation is a core instrumental value in general, but only if we assume that an agent is limited to existing physical systems, and has no ability to instantiate new systems anywhere in the physical world. If instead you are limited to existing virtual realities, then self-preservation has no instrumental value for reasons of self-preservation. For the purpose of studying AI alignment, it's thus often useful (and in some cases necessary) to make distinctions between core values that will apply given one particular physical or virtual reality setup.</li>
</ul><br><ul>
<li>__Modularity. __In a sense, the core value problem is simpler than the core instrumental value problem: a core AI system with perfect knowledge can always "get on with" pursuing one single utility function.</li>
</ul><br><ul>
<li>But there are some core values that are so difficult to specify precisely that we'd need to specify a <em>host of</em> <em>precisely specified</em> <em>values</em> before it would be possible to have a single AI system pursue a single core set of values with any confidence. It can't be enough just to specify that the AI system is "corrigible", which itself is only defined to a high degree in terms of human approval, or something similar.</li>
</ul><br><ul>
<li>For a similar reason, core values can be difficult to describe precisely, and thus the core value problem can be substantially harder than core instrumental values.</li>
</ul><br><ul>
<li>In general, this is not surprising: in many cases, if you have a <em>particular</em> <em>precise</em> <em>description</em> of what you'd want an AI system to do, but no <em>specific</em> <em>prior</em> on how to ensure that system will do it, then you <em>still</em> need the AI system to figure it out for itself. To think about "the core values" is almost exactly the same as thinking about "the core instrumental values" for a specific AI system. (Note that we might still have a very strong prior on what core values to have.)</li>
</ul><br><ul>
<li>__Outer and inner misalignment. __This means that even if we have a list of desired values, there is no guarantee that our agent will figure out the right values given only the information we've given it.</li>
</ul><br><ul>
<li>A particularly obvious example of outer misalignment is our list of preferences over things like <em>chess</em>, <em>money</em>, and so on. These can encode almost any values, and if the AI isn't very knowledgeable yet it's still going to take that fact to be evidence in favor of its choosing the right values.</li>
</ul><br><ul>
<li>Other outer misalignment examples are harder to notice, like the difficulty of learning one's values "just by listening to other people", or the lack of motivation to spend days or weeks learning about a topic if you <em>already know</em> what your values are. This is obviously bad for people learning about AI and other existential risks.</li>
</ul><br><ul>
<li>An inner misalignment example would be the fact that <em>if</em> we try to teach an AI system to pursue the right values, but somehow it doesn't <em>learn</em> the right values, it's at least equally likely that the system's motivations could be such that even if we give it some goal to pursue, it develops very different motivations from what we intended.</li>
</ul><br><ul>
<li>This could also be seen as an example of outer mis-alignment, of an AI system <em>not</em> being convinced of a particular goal or set of goals given only the information it has.</li>
</ul><br><ul>
<li>And as before, these examples are hard to notice, because they are often things about which people already have strong intuitions.</li>
</ul><br><ul>
<li>One example of outer alignment is our list of values over <em>instrumental</em> <em>policies</em>, even if we're not completely sure what a policy is yet. (For a similar point, see here, here, and here.) For example, if an AI system is trained to pursue instrumental policies that optimize a specific reward function R, then in order for the AI system to pursue that instrumental policy, even if we aren't sure what R is, it needs to believe that <em>that particular instrumental policy</em> is <em>the right one</em>. This point will repeatedly crop up.</li>
</ul><br><p>Learning Values</p><br><p>The "Learning" Paradigms: Imitation, Reinforcement Learning, and Learning from Human Feedback</p><br><ul>
<li>These three approaches represent three different paths to making AI systems that are trying to figure out what they should be trying to do (at least a little): </li>
</ul><br><ul>
<li>The __imitation paradigm __says that AI systems will be trained using supervised learning, or unsupervised learning where the label comes from human data (but still in the form of AI-generated explanations). </li>
</ul><br><ul>
<li>The __reinforcement learning paradigm __posits that some AI systems will be training in an environment that simulates future plans and rewards them for better execution. This could be with a particular utility function, or some other "criterion" that determines what actions the AI system takes.</li>
</ul><br><ul>
<li>The __learning-from-human-feedback paradigm __seeks to build AI systems based on information that humans can provide in a way that the AI system could <em>accurately</em> use to inform its own decisions. Here the humans are "teaching" the AI system, as opposed to providing supervised learning data or demonstrations.</li>
</ul><br><p>I'd argue that in many cases each approach might even be an instance of the other two, depending on a particular context. A supervised learning system that's trying to predict a human's action could be considered to be learning by imitation, and a reinforcement learning system that's being trained to maximize a score can be considered to be trying to use human feedback (to optimize that score). As for imitation learning, it may have been possible to have a supervised learning approach to imitating humans, but since the advent of robotics the reinforcement learning approach has prevailed (or perhaps we should say the last word and call it "learning-from-robot-demonstrations"), at the cost of requiring significantly more human data to get the same type of data.</p><br><p>A general principle, perhaps: imitation learning is usually a special case of reinforcement learning, which can also be a special case of learning-from-human feedback.</p><br><p>Value Learning</p><br><p>There are many different approaches to value learning, but they all fall within one of 4 broad categories. These include:</p><br><ul>
<li>__Reinforcement-learning-decision-theory approaches. __This kind of work falls within the category of "imitation-learning-from-demonstrations", in that the label is provided by a human (albeit a hypothetical one). This work can thus be seen as <em>imitation of expert advice</em> by a system that does not have access to information beyond the label provided by the adviser.</li>
</ul>      </span>    </div>  </div></body></html>