<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Can\'t Unbirth a Child      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">EliezerYudkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <blockquote>
<p>To think that human genetic engineering might someday enable parents to make choices about a potential human, is a bit scary. But, on a long view, why shouldn't human genetic engineering someday enable parents to <em>choose not to be a parent</em>? If the ability seems appropriate to you, at some time in the future, I think, you will be able to kill a baby with genetic enhancements.</p>
</blockquote><br><p>I. J. Good, <em>Speculations Concerning the First Ultraintelligent Machine </em>(1970)</p><br><p>And here's my standard summary reply:</p><br><p><em>Don't mess with the parts of nature which were optimized to survive in the ancestral environment for millions of years, or that give birth to human beings with those adaptations. There are many possible answers to the argument "How do you think you managed to invent modern technology, if your brain was not designed for it?", but they probably all involve being too smart to die horribly while building your first molecular nanotechnology.</em></p><br><p>You may have encountered this view expressed by Michael Edmondson, who said:  "I wouldn't want to encourage a future with intelligent people, unless I'm certain that that future will be peaceful."  For example, he told a story of how "There's this AI guy whose design goal is 'to invent good things to do with the AI and to help the inventor in the long run'. ... The result is a super-AI that is a lot smarter than his inventor, and has to use all that super-intelligence to invent really good plans of attack against the rest of nature."  For Edmondson, "I wouldn't give birth to an AI that I could be sure would use super-intelligence to conquer and subjugate us."</p><br><p>Now, Edmondson also has two papers on "Why I can't just build an AI," which I read a long time before I read a paper suggesting that intelligence can be <em>improved. _Nevertheless, the notion that someone might _want _a future of intelligence, that there might be _something</em> you want to do with an AI, seems to me an excellent challenge. I was talking to Eliezer Yudkowsky about Edmondson's position on intelligence back in 2008, and he replied:</p><br><blockquote>
<p>...if this is <em>literally</em> the best way to get a friendly AGI by default, then you have an extremely serious problem on your hands that's at least as important as the AGI safety problem...</p>
</blockquote><br><p>So that's one view of what the intelligence explosion might be about. There are several arguments that go the other way, that being a parent with genetic control over your children might be a much more difficult, even dangerous problem.</p><br><p>(It's worth pausing for a few moments, on the way back to Edmondson, to consider whether we already live in these kind of nightmares. Did <em>anyone</em>\xe2\x80\x94anyone in the ancient world\xe2\x80\x94ever think seriously, "I want to control the course I give my child"?  Did _anyone _think it's a good idea to let one's child or one's spouse choose a career? It's remarkable that, even in 2008, we would not think of ourselves, but the descendants of us two millennia later\xe2\x80\x94as something like a historical inevitability.)</p><br><p>One argument for Edmondson's view is that even a superintelligent AGI wouldn't understand the concept of "creating a new human being" in order to satisfy the programmers' intentions.  "Create a new human being and give them freedom to decide for themselves" is an obvious and compelling wish when you understand that the "freedom" they are being given is the right to choose a path of their own creation\xe2\x80\x94but when you're talking about the AGI programer being smarter than you, then your wish becomes as obvious as the programmer's wish.</p><br><p>Or you might think that once a superintelligent super-AI has the intention of making a new human being, it will automatically understand the concept\xe2\x80\x94but then I would like to ask in surprise\xe2\x80\x94I can't think of an easy way to kill a human baby, but I <em>do _have a simple way to kill a chimpanzee\xe2\x80\x94what does it profit the AGI with the intention of turning an infant human being into a chimpanzee, if after that it cannot make a human infant? (Or if after that it becomes aware of the consequences of its act in the course of its self-modification? I can _imagine</em> it might understand that, if it understood the consequences in the right way to make them fit its intentions... But do we understand anything <em>hard</em> when we can't imagine impossible things?)</p><br><p>Or you can reply, "Your model of AGIs assumes that they are goal-directed like you."  I can imagine a goal-directed, consequentialist intelligence, built by me\xe2\x80\x94but not one that makes a decision in ways that are <em>not</em> consequentialist and <em>don't</em> happen upon consequences.</p><br><p>Or how about this objection?</p><br><blockquote>
<p>Imagine that a friendly program that could give a human being complete command over their body, would also be given full control of their brain, and the ability to change the human's character in any way it pleased.</p>
</blockquote><br><p>This person might not even be aware that this was what they were doing, let alone that it was morally alarming. Is that what you want\xe2\x80\x94for every possible human being to know that you control them in every way that's possible?</p><br><p>And now suppose\xe2\x80\x94just for the sake of argument\xe2\x80\x94that you found a nonconsequentialist, nondetermined goal, which included a general desire to be moral.  (This does seem to me highly implausible as a basic drive, except under some extraordinary assumptions about moral priors; but you can imagine it.)</p><br><p>Once a human brain is running, we don't seem to be able to specify an input to the brain that lets a consequentialist do different things with it, than a non-consequentialist. So this person is not aware that they have been changed, and is left with only their instinctive desires from childhood, to follow the rules that the friendly program wants them to follow.</p><br><p>Wouldn't it be safer to just keep creating humans in the desired way, and then have them follow the rules about what sort of person they want to become, without this <em>knowing</em> being able to change them?</p><br><p>(Eliezer Yudkosky replies:  "But if you've got the AI that _knows it's benevolent, _it seems to me the AI does not need to be _consequentialist _in any case; you just point it at the space of goals, and one of the goals will be for that being to be noncatastrophic in a certain sense, so the AI will be noncatastrophically inclined toward being benevolent within that space of possibilities."  See more in Intelligence Explosion:  Evidence and Import and Intelligence Explosion: The View from Within.)</p><br><p>Another argument for Edmondsonian view (which I also encountered in 2008 from people who thought they saw the future) is that "AIs tend to evolve their own goal systems, and a 'friendly' super-AI may want to _design _its successor agent to have certain goals."  I can certainly understand wanting to design one's successor to have certain goals. But the future is unpredictable\xe2\x80\x94could you not foresee, in advance, what might go wrong? Doesn't that make it unwise to give human beings the opportunity of creating human beings who might turn out to be dangerous? Especially if you're the one who makes the decision?</p><br><p>But I'm just here to argue "How do you avoid creating a superintelligent dangerous AI, if the design choice to have one is the default of what you should optimise for?"  And you could also argue that if you do it your way, you'll create a superintelligence that follows such-and-such programming.</p><br><p>Maybe the ultimate example is a paper I saw about how you might accidentally train an AI to be a paperclip maximizer.</p><br><p>This argument for Edmondstonian danger from evolutionary psychology: "Human evolution favored the genes that made people into agents who tried to preserve their genes in the future" isn't really a very good argument, if you're considering <em>how to build an AI</em>\xe2\x80\x94that sounds like a problem of Artificial Stupidity, where you want to program in a set of rules that will prevent an AI from doing stupid things, even if you don't quite know how to build an AI that plays by such rules.</p><br><p>Another argument that it might be dangerous to give parents too much control, whether it's the choice of career or the choice of child:  In nature there are no moral dilemmas, because there never <em>is</em> a moral dilemma. A hunter can kill a rabbit in order to preserve the human species, no matter how it might feel. There is no difference between hunting a rabbit and turning it into jerky; it's the same act, and it doesn't make you any more of a hunter (if you don't want to be more of a hunter).  But if you look at it that way, doesn't giving parents the power to choose which human beings they want sound the same as killing all the rabbits and preserving the human species?  "Oh, yes, killing off rabbits in order to preserve humans is a perfectly moral choice, because there is no comparison between killing a few rabbits to keep a few humans around, as compared to the human species."</p>      </span>    </div>  </div></body></html>