<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        General AI Strategies\\xe2\\x80\\x94A Roadmap      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Robin Hanso</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>This post describes a strategy for figuring out how to build non-trivial general-purpose AI. The strategy is simple, though somewhat long; so I have written a longer exposition including some of what follows. Some of the strategies mentioned here may have been introduced previously by others in the LW community; even so, I've written this post mostly from scratch.</p><br><p>I also recommend reading the discussion here and in this paper on related issues.1</p><br><p>Why to use AI strategies</p><br><p>Our ability to influence how AI systems turn out depends a lot on what methods we use to design the AI in the first place.</p><br><p>The strategy I describe here is for someone who knows how to do AI in practice to attempt using "pure" AI design to build a very general AI. (Pure AI design is what someone would do if they were allowed to ignore whatever past AI techniques they have already learned.) To do this successfully they would need to (a) have developed enough trust in AI techniques as good enough that they are confident those AI techniques are likely to allow this to succeed; (b) understand enough psychology to predict likely reactions to some designs; (c) have already studied the relevant fields with sufficient care to understand how AI methods are used, what they can do, where they can be used, how hard they are to be used, and what they likely will be used for; and (d) have developed enough competence to carry it out on relatively short notice.</p><br><p>In the following post I'll discuss some reasons why this strategy might be difficult or risky. I think this strategy's usefulness depends mostly on how difficult it is, how risky it is, and how helpful the alternative strategies are.</p><br><p>Reasons why this strategy might need to take a high-risk approach</p><br><p>Below I'll summarize why I think this strategy is high-risk, and briefly discuss some alternative strategies. I think the primary reason is that this strategy aims to develop an AI so general that it will have a large and persistent impact (or if you prefer, a large and persistent influence in how AI systems continue to be used). If an AI develops enough competence to do this and the strategy seems promising then an AI so competent should be used to directly determine how AI systems continue, no one should be able to ignore what the AI does as they may wish to later, and it is very very difficult for people not involved to do anything to influence the process of development and then use of such systems to predict their likely reactions and use.</p><br><p>I will now briefly summarize some alternative strategies that may offer safer alternatives:</p><br><ul>
<li><em>Pre-delegate most AI work</em> to a small number of AI experts (e.g. the people who created modern chess engines). This is more risky but may make it easier for others to directly influence events in technical AI research.</li>
</ul><br><ul>
<li><em>Sneak peek</em> of AI development via some other less-general method. For instance, perhaps some AI experts can take a year to think up a secret AI strategy and then present their results to AI researchers using some non-general method. If they get good results, then those AI experts could publish the strategy; AI researchers could then perhaps ignore it entirely. Or perhaps a small group of AI engineers could work alone on a small project to try to break some key technical insight and use the result to build a general AI. If the result was promising, they could publish the result, and AI researchers could use that to predict what other AI researchers would do. Or a group of AI developers could perhaps try to set up some other similar setup and then publish in more detail what they did, to try to gain influence.</li>
</ul><br><ul>
<li><em>Not develop AI</em>, if other AI strategies had clearly failed. This is not only not necessarily riskier, but may be riskier (since some people could perhaps continue to improve AI techniques to some extent in order to help with AI strategies).</li>
</ul><br><p>Reasons this strategy might be risky</p><br><p>As you might expect, as I have described the strategy above, many people would strongly oppose using it. The main reason is that it would create an AI with (on the view of many) huge influence in how AI is built, while also giving it a very unclear input into its actions. It's hard to predict whether it would lead to desirable AI or not; thus it's hard to evaluate its performance before an accident happens. It's hard for AI researchers to ignore what the system is doing, and thus they may not be able to predict their systems' behavior. And it makes it more difficult for AI experts to evaluate other AI strategies, thereby making any attempt to coordinate on any AI outcomes even riskier.</p><br><p>Many other objections could be raised, but I think the two above are the most serious. So I will elaborate more in the final sections on both (a) what is needed and (b) how it could be done.</p><br><p>More objections and elaborations on more details</p><br><p><strong>(a) Some argue that the scenario is impossible anyway because it is too easy to understand the AI and predict its behavior so no AI could possibly be so powerful without being also quite understandable, and then it would no longer pose a threat.</strong></p><br><p>My strong intuition is that even if it's technically very hard and extremely complex to understand, it should be possible to model it in terms of (perhaps several different classes of) simpler models. If one could model it in terms that could be modeled, then it could be understood. Or at a minimum it would be predictable in at least some contexts.</p><br><p>I expect the main barriers to understanding such an AI would be things related to how the AI is implemented and what resources it has. I don't see a good reason why we couldn't try to understand such an AI in terms of how it is constructed, but have to deal with whatever remaining complexities may remain.</p><br><p>Note: if an AI has an internal representation that includes the model of its own mind\xe2\x80\x94for instance, it has a representation of its own consciousness\xe2\x80\x94that representation can be understood in terms of the internal representation of any system with that property. This means an AI that is implemented this way should be understandable.</p><br><p>As I have said, there may be many other barriers, but these seem solvable (with the help of AI theorists) assuming AI designers are able to make a basic AI strategy reasonably understandable, know they must understand it, and then decide whether to risk using it. (I strongly suspect that even if the basic strategy is understandable many will still worry that if it is dangerous, no AI can possibly be so dangerous.)</p><br><p>However, my intuition is that if a basic AI strategy is not understandable, it would not be easy enough to come up with a good alternative strategy using a very different approach. If there is a basic understanding of AI at a given time of development, that understanding may allow a reasonable alternative strategy to be designed that does not have internal representations of conscious minds or minds-with-representation. (Note: this would at first appear to be more promising if the basic strategy was more like GPT-3 and less like chess-playing engines or image classifiers, but in fact the latter two classes of algorithms do seem to be understandable in this light.) But I don't see any good reason why an AI with an internal representation of a mind should not be understandable, and I strongly suspect such an AI can be if its basic strategy are understandable in a form that can (and is) understood.</p><br><p><strong>(b) Some argue that what the strategy would do is much less dangerous than what most AI researchers expect, so that even if we don't know what we're doing, we will find it less dangerous than I describe above.</strong></p><br><p>I will give some elaborations below on two points related to this objection:</p><br><ul>
<li>The argument here seems to be that because I describe an AI so general (or at least more general than the average in AI work today) that it is more useful than I anticipate, then it would have to be less dangerous than anyone (or some group of people) currently expect. But if that argument is even partly true, then I think it's wrong. While it would indeed mean the strategy would have much less of a potential influence to be able to directly determine how something is used after we've already built it, it also would presumably have a huge influence to be the strategy that best fits what will be good for humanity, or at least its continuation. If we just use the 'best fit' strategy, it should be able to survive and thrive in the long run by improving itself or by figuring out how to improve itself (i.e. using some AI techniques), which should eventually mean that more people would use it and the AI become more powerful. This strategy also seems more dangerous simply because it is more powerful and hence more likely to be used. It also seems likely to'self improve' too and not just 'cooperate with' and I have nothing clear to say on how such a strategy ought to behave before it develops a self that is vastly better at AI and that can more effectively optimize for the future. Even in other contexts, it's easy to imagine there being a period early in human history when various groups were competing, the best strategy for them was essentially to cooperate with the group they were facing, and by the time they had some chance of winning, other groups would have also had access to the best strategy and so on. It seems a large part of the story of humanity's continued existence is the ability of groups to compete, to sometimes cooperate with rivals, and to eventually use various forms of more cooperation to more effectively compete against each other again.</li>
</ul>      </span>    </div>  </div></body></html>