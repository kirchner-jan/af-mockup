<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        A Reply to Yudkowsky on Existential Risk      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Eliezer.Yudkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><strong>Followup to:</strong> Where Recursive Justification Hits Bottom</p><br><p><strong>Reply to:</strong> Beware Anecdata and Personal Experience as Evidence</p><br><p><em>When it comes to Friendly AI, the most precious thing in the universe is intelligence, and the second-most precious thing is time (to optimize a Friendly AI).  But a third-most precious thing in the Universe, by many orders of magnitude, is human ingenuity.</em></p><br><p>(If you've never heard this observation, it's because your civilization is currently in a phase of extreme crisis, and you're not thinking about anything else\xe2\x80\x94which if you understood it, would make you horrified. When you can use other forms of intelligence for the solution of your own problems, then, well, you <em>do</em> have Friendly AI on your mind. If you have a Friendly AI <em>system</em> for the problem of Friendly AI, then it's because you <em>have</em> a certain amount of slack. If you didn't have enough slack to solve Friendly AI first, you had other problems that <em>needed to be solved first.</em>)</p><br><p>Friendly AI <em>can</em> be seen as a recursive self-improvement loop, and I do see an analogy between the case of FAI and the case of AI. As you might expect, the recursive loop can only be solved correctly if you have an adequate amount of <em>general epistemic</em> slack. A recursive loop can be resolved in any other way, if the <em>resolution</em> is carried out in a certain way that causes the resulting AI to have a certain amount of <em>general cognitive skill</em>, not a certain amount of general epistemic slack <em>in advance of encountering the problem</em>. This is what Eliezer's earlier post\xe2\x80\x94and its followups\xe2\x80\x94trying to warn against. People who try to design FAI systems with an amount of <em>general general epistemic</em> slack, and who solve the FAI problem by running them with less such slack, will fail.  (And they will do so at their peril.)</p><br><p>To put it more harshly, though it's not a very pretty argument, if you try to design an FAI system on an <em>ad-hoc</em> basis, but you fail, you deserve to lose your life; and if you design an FAI correctly, then when you build an FAI system, the system needs to be built with <em>general general epistemological</em> slack at the solution, and not with special FAI-specific epistemic slack.  "Intellectual horsepower" is useful, but not enough in itself. You need enough slack to <em>actually do research</em> which will help build the FAI system, especially if you're building the <em>research-supporting</em> part in advance of creating the FAI part.</p><br><p>(I'm not saying it's <em>unsafe to try</em> to build AGI at all, any more than I would say that driving a car is a good idea. But we shouldn't be <em>unsure _that we couldn't build AGI in principle _and</em> that we'd screw it up in practice.)</p><br><p>I'm not saying that there's <em>no such thing _as a Friendly AI system that's supposed to be "designed _by a sufficiently advanced system of metaethics"</em> and that will therefore be Friendly if constructed by a sufficiently advanced group of FAI researchers. But I would like to have a Friendly meta-ethical system before creating a Friendly FAI system, and would be highly skeptical <em>as to whether or not _it might be _provisionally</em> helpful in the early stages of FAI research to create certain subagents that would be Friendly if constructed in a sufficiently advanced metaethical system, as a kind of "self-enforcing" moral bootstrap. Even if such a thing is theoretically possible and even if such a thing could accomplish a lot of good, I would not at all expect any agent created in this way to be Friendly. I wouldn't be so naive as to think that even after the creation of enough Friendly agent-parts at the lowest possible level plus some amount of design-and-debugging work, we might be home free. I would expect many of the subagents created in this way\xe2\x80\x94especially if the designer used the wrong design language, or if they tried <em>too much</em>\xe2\x80\x94to be _far _from Friendly (because their designers had too little slack).</p><br><p>To put this less sharply, I think that anyone who tries to solve a Friendly AI problem by generating their parts (including the design language) through some <em>ad-hocus process, _and then trying to design _a sufficiently similar future _to that process, is going to have problems that they're not going to be able to predict in advance. They're not going to have time to correct the problems they see in advance. It's not like they have time to do research to avoid falling into them. In the _ad-hocre _process of generating their design code, they already have _an amount</em> of epistemic-cognitive-generality that they don't have in advance. So if they run into problems that don't follow straight lines of design space, they're not going be able to point to those problems in advance and run with confidence that it will work. If their system doesn't <em>work,</em> it's their system; and the world is still full of sentient beings who could die as a consequence. If they can't even <em>tell _problems they're running into ahead of time, then they really don't have _any</em> slack whatsoever that could be drawn upon by a proper FAI solution.</p><br><p>It's just that in real life, as well as on the page of fiction, it's more often the case that people try to build FAI systems on an <em>ad hoc</em> basis, without first having enough slack to solve FAI as an exercise in metaethics. Then the self-modifying FAI code they wrote runs into problems that their ad-hocre reasoning failed to foresee in advance. It <em>doesn't matter</em> if they themselves can't point to those problems ahead of time; and they can't, any more than a competent self-modifying system in general can "point to" things that are in its future light-cone that it can't see yet.</p><br><p>As a general principle, you really can't expect an <em>adequate</em> future to self-modify. A self-modifying future will often be full of internal problems that it doesn't even <em>know</em> it has. And because you yourself don't have enough slack, the self-modification can't actually run without getting some initial help from a more general metaethical process which might fail.</p><br><p>(A lot of people seem offended by this general viewpoint I'm presenting. Even Friendly AI seems like a way of avoiding the real, meaty problem of metaethics, to them. I try to counter this in the comments to the following post. Feel free to post answers to those objections; I will address them in turn below the line.)</p><br><p><strong>EDIT:</strong>  Wei says, "This sounds an awful lot like the problem we tried to avoid by coming up with an FAI in the first place."  Actually it's not <em>quite</em>, because in FAI a self-modifying agent (the "superintelligence") still does a lot of generating its future through its own processes\xe2\x80\x94especially before it gets enough slack to solve itself. This is one case where I'm being less harsh than I would be if my goal had been to produce <em>just</em> an FAI design. (And yes, I could in principle think of designs <em>less _harsh that would have no internal problems, and just wouldn't run under the conditions a human brain has in the real world, or even less. Which isn't an excuse; you have to start with some kind of design, even without running it at all, before you get a solution at all.)  But the process for generating a self-modification is _just plain dangerous by human standards.</em></p><br><p><strong>END EDIT:</strong></p><br><p><strong>EDIT 2:</strong>  And another followup to this post:</p><br><p>http://\xe2\x80\x8b\xe2\x80\x8blesswrong.com/\xe2\x80\x8b\xe2\x80\x8blw/\xe2\x80\x8b\xe2\x80\x8bh6/\xe2\x80\x8b\xe2\x80\x8bdefining_ai_alignment/\xe2\x80\x8b\xe2\x80\x8b3up/\xe2\x80\x8b\xe2\x80\x8b</p><br><p>A problem for FAI is that you can't just code a system in English to do what a human would tell it to do, or write an English description of what a human desires, because the two things don't have enough in common to ground out to a single thing. Even if you start with a human, or a set of humans, it would be problematic to define "good enough" on their behalf, not just by code, but by meta-code or "moral philosophy."</p><br><p>An idea here is the "Hanson-Yudkowsky AI-Foom debate", in which Eliezer Yudkowsky and Robin Hanson argued over whether it was possible, even in principle, to "design a self-modeling intelligence" which "improved itself".  Robin said it's not possible, because the problem would keep recursing down to the fundamental level, never really arriving at a point of positive feedback. Eliezer said it's possible and we might find "a solution by default", since "we are not clever enough to see a general solution that is not present".</p>      </span>    </div>  </div></body></html>