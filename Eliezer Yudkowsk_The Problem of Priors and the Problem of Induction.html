<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        The Problem of Priors and the Problem of Induction      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Eliezer Yudkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><strong>Sequence summary</strong>:  In Solomonoff induction, the probability with which a program <em>truly</em> generates some sequence of observations can be computed from the definition of Solomonoff induction. A Bayesian agent updates its probability estimate each time it gathers new evidence, but by the time you have the right code and have executed it enough times, you can know more with high probability than before. There is Bayesian uncertainty, then, about which code is correct and which sequence of observations actually occurred\xe2\x80\x94we can call this the <em>problem of induction</em>.</p><br><p>So far as I can tell, the problem of inferences about the past is not much discussed on this list, and I am very interested why that is. I have also said that one of the major strengths of AI is that it has the opportunity to learn from history\xe2\x80\x94we learn how to do science, and make weapons, and so on\xe2\x80\x94because our ancestors, without looking back, made those mistakes over and over again without even realizing it was a mistake.</p><br><p>So far, then, I haven't heard much discussion of the problem of induction in AI or its relevance to the problem(s) of AI alignment. I am happy to discuss these things!</p><br><p>There are two major problems of induction: the problem of prior probabilities and the problem of induction. We'll talk about those separately.</p><br><p>Let's start with the problem of induction\xe2\x80\x94the problem of estimating what happens before we have more data. And let us imagine for a moment that all the human experiments so far have shown that the Sun will rise tomorrow (a premise I am prepared to challenge).  And let us make another assumption on which Yudkowskian inferences are allowed to go: the AI must be very good at the task of Bayesian probability theory <em>without ever running the program</em>.</p><br><p>In Solomonoff induction, we can talk about programs which are "similar" to each other, and use the <em>Kolmogorov complexity</em> of programs which output similar output sequences\xe2\x80\x94the size of our shortest description of these programs. So suppose we now specify exactly how large a program we need to describe as a list of bits, in order to get a program with probability less than some 1/\xe2\x80\x8b2^length. So if we want our AI to compute a program that will return the next observation in the rising-Sun sequence, then for programs that return the next observation it suffices to just specify the observation we want to predict, rather than specifying the entire rising-Sun sequence. And I would guess that we can make our AI that good at predicting future observations by looking up programs that output such observations, rather than by actually running the programs ourselves. (For example, the program that takes as an input the number of seconds since noon, and returns noon, will be shorter than the program that gets the day of the week and returns noon.)</p><br><p>So from the point of view of the AI trying to write correct programs, the problem of induction will seem pretty easy. You just look up the program that will return whatever data you're interested in, and then that is the program you run, and that's the program that does the prediction. The problem just looks easy from the point of the AI programmer, who doesn't run any programs; it's easy because we're not running any programs ourselves!</p><br><p>As an example of how this can go wrong, consider the following story (with all sorts of flaws I'm deliberately leaving unspecified, but which you can fill in).  Suppose there's a problem with inductive AI predictions, in which the AI has been trained to make good predictions after each <em>hour</em> but not after each <em>minute</em>.  So if someone turns on the AI before noon, it may make the correct prediction for a few minutes, and then go on being the perfect Bayesian\xe2\x80\x94until nine AM, when the true time happens to be nine. And maybe the problem is that the AI has not seen enough examples of the Sun rising on the other side of the Earth where the Sun was supposed to rise, where the other observers waited till nine to give themselves time to run home, etcetera. So when it finally sees the true Sun moving in that direction, it sees it's wrong\xe2\x80\x94and it predicts the Sun to move even further to the right when the true time arrives, because it doesn't have <em>enough</em> evidence yet. It has a <em>prior</em> which says that the Sun doesn't go off to the right, but it's wrong. We should say that the prior is <em>incorrect</em>, according to Bayes's Theorem; the Bayesian doesn't understand the concept of <em>pre-knowledge</em>.</p><br><p>But what does it really <em>mean</em> to say the AI's prior was <em>incorrect</em> in this way? Is it a statement about <em>which code is correct</em>, or is it a <em>description of the state of the world</em>, an abstract possibility that could be true or false? Suppose the AI's prior does not look like a correct description of the world\xe2\x80\x94it doesn't have the <em>shape</em> that represents the true state of affairs in the world. This sounds very close to something I heard one AI researcher describe AI as being used <em>for</em>, as opposed to <em>as a vehicle for</em>; the main task of AI was to optimize the shape of an ever-expanding system of rules, while remaining <em>as a tool</em> rather than as a goal unto itself.</p><br><p>Or consider the case of an inductive AI that can't predict anything <em>after</em> 9AM, but correctly predicts everything after 7AM. If you now think of the problem of prior probability at a <em>programming level</em>, it makes perfect sense. You can't program the AI to make very precise predictions after 9AM; there is simply no description that you could write down for a set of rules such that the output of the rules agrees with observation seven before noon, but disagrees with observation eight before two p.m.  You are trying to write a description-of-the-world which is true for the observed world (and does not contain the word <em>before</em> nine, but <em>after</em> seven).  If you don't specify that, then the output of your program is just a copy of the output of your previous program, so it must predict exactly the same thing <em>after</em> seven as <em>before</em> nine.</p><br><p>Now, on to the problem of <em>induction</em>.</p><br><p>Suppose I'm driving an early car, and the problem of inductive AI prediction comes up: the AI has been given no time at which it can make predictions, but it must make predictions about the car that will be in the air at a given time. How can it do <em>that</em>?</p><br><p>In Solomonov induction, you can ask how long it will take to compute a program which given the universe that just went by outputs the next observation\xe2\x80\x94including all previous observations, as far back as the computer has stored them. And if this computation isn't infinite, we can also ask <em>what probability</em> the right answer is, given that the right answer has not yet occurred, by Bayes's Thesis.</p><br><p>Here, we would be tempted to ask how long it takes to find out that an algorithm which worked on a particular problem would keep working even in the next universe along. In Solomonov induction, we can assume something like the time needed to store the universe goes to infinity, since all programs are run for eternity. If the only universe that can be computed is one that takes an arbitrarily long time and never finishes, Bayes's The-sis no longer applies, and we are left with the problem of <em>discovering the algorithm</em>, which is often <em>easier</em> than trying to solve the problem yourself.</p><br><p>But in a <em>deterministic</em> world, finding out whether a deterministic system works right-now works the same way <em>whether the true laws of physics are stable</em> or whether they are <em>explore a bit, then settle down again if you're wrong</em>. The Solomonoff prior says that there are lots of alternative programs which compute the same output as the true <em>world</em>, just because the true program is long.</p><br><p>In the deterministic version, the only way to infer that the machine output is <em>part</em> of the <em>true</em> world, is for it to <em>actually</em> occur when there's a big rock around.</p><br><p>In short, you can learn about the truth from your experiences, but you must have <em>more evidence than you were led to believe</em>.</p><br><p>So induction in Solomonoff induction is <em>very</em> difficult. This is the case whether the induction is the "right answer", or <em>if the next thing you see happens to be the right answer.</em> When you're not given a hint that your machine will work as a computer in the next round, you can't really be sure in advance that it will, or will stay the course that it seems to be on. But with an <em>artificial unprovability</em>, we are in a very different kettle of fish.</p><br><p>The problem is that we can define a <em>sufficiently powerful AI</em> as being able to solve more-and-more-useful problems. Solomonoff induction makes no mention of the question of what the AI is doing\xe2\x80\x94what goal it is pursuing in designing its successor AI's design. It's <em>not</em> doing anything other than producing rules that produce good predictions. So the Solomonoff induction problem may look like being able to write the right program that solves the problems it's handed\xe2\x80\x94but it's not doing anything other than the right program that would produce good predictions, if there were a way to hand it <em>that</em> problem.</p>      </span>    </div>  </div></body></html>