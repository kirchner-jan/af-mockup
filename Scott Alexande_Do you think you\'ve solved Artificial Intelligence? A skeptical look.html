<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Do you think you\'ve solved Artificial Intelligence? A skeptical look      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Scott Alexande</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><em>This post started out as one of the "Let's try to use Bayesian evidence to solve a hard problem" pieces in the sequence that I hope to get out later, but is instead about how hard it is to be skeptical of other people's answers to your own problems. I will be posting more about the difficulty of skepticism in a couple of weeks, but for now I just want to do a couple of things, which I hope will give a flavor of the problem:</em></p><br><p><em>In the original post on a possible solution to artificial intelligence, Eliezer suggests that if you train a computer to play chess by simulating a chess grandmaster then the computer will outperform a human at chess once it starts to outperform a grandmaster. Since then people have gone on to suggest lots of different solutions to the AGI problem. I remember one guy who said that just by randomly changing parts of the AI's code, you could make it play a "better" game than Deep Blue. Deep Blue had evolved an intricate system of rules and heuristics to achieve a particular level of performance, but random changes could change the entire system, or a different combination of rules, and get you to a higher level. That sounded convincing, and when the guy posted on it with actual, executable code, I decided to play along. It turns out that the solution is that you have to know the rules and heuristically predict things in advance in order to use trial and error to improve them.</em></p><br><p><em>The point of this particular example is that you can have your cake and eat it too. You can say a system like Deep Blue is an "artificial intelligence", and then be able to claim that your own AI is just as clever as Deep Blue without having to try to make it work. This is the exact same strategy a human would use to try to come up with an AI, and it only fails when the programmers haven't thought of all their clever tricks in advance. We don't want AI researchers who know the solution and are saying "Oh it seems difficult, so I can't try to make an AI. It's too ambitious, like my own!" The important thing here is to realize that an AI which is a hundred times as powerful as Deep Blue would still be doing things other than trying random mutations in search of a better solution, and that it will be just as much of an intelligence as Deep Blue was.</em></p><br><p><em>But I'm getting away from the point. As good as Deep Blue is at chess, Eliezer's idea also predicts that in a hundred years, a computer will out-generalize all of humanity. And that doesn't sound like it would happen by random mutation. If you think about this, the probability that a random change to the human genome\xe2\x80\x94or to something other than the human genome\xe2\x80\x94which improved intelligence by 100% was missing, would be less than one in a billion. On the other hand, it's trivial to construct mutations which give a computer 100% increase in intelligence. Any program you have could be written as simple instructions to turn on an extra piece of hardware. What are the odds that an actual, natural mutation (from human to slightly improved human with additional hardware) would work out? Less than one in a hundred billion, if I'm not mistaken.</em></p><br><p><em>A final point: If you have a question about artificial intelligence, the only question that is really important is "How hard is it?" and not "what algorithm do we use?" Deep Blue was impressive because it was very hard to program a chess computer. But if you had to program your own AI, you could probably just use heuristics from the outside, and do it without any major insights. You could just use anything and call it AI if it beat the best human. Why can the best human beat your system, and why does evolution stumble upon it? It may be a fluke, or it may be because the people writing the software were smarter because they knew more about computer algorithms or because they weren't as stupid as I am about them. It probably hasn't even anything to do with how intelligent the algorithm is \xe2\x80\x93 if you're the sort of person I was a few months ago, the algorithm you use probably doesn't matter except inasmuch as it helps you understand how hard it is for evolution to stumble on intelligence. The hard part is being able to recognize when you've found a "solution."</em></p><br><p><em>That isn't to say that Deep Blue and his successors are useless \xe2\x80\x93 they may still be able to help you achieve goals that can't be accomplished by simply knowing the rules of chess and applying heuristics. But it's important to remember that both Deep Blue and his predecessors were built using trial-and-error. If they worked after they were built it was a miracle. If they don't work, it wasn't. Deep Blue came close to being an AGI \xe2\x80\x93 it just failed at chess.</em></p><br><p><em>Maybe you can argue about whether a particular intelligence isn't, at bottom, a rationalist. But if you find yourself thinking that an "artificial" intelligence should be able to solve chess by randomly looking things up in a book and trying different things, you've already lost the argument. It's not that Deep Blue worked from day one because it was really rational. It's that Deep Blue worked because Deep Blue got really smart by looking around in a book, and the book was really clever by using trial-and error \xe2\x80\x93 Deep Blue works because of the _difference</em> between being clever by trial-and-errors and being clever by trial and error, and by trial and error. Deep Blue doesn't even have to get his or her <em>own</em> code changed; Deep Blue can be improved by looking it up in a book. It's a huge difference. A Deep Blue improvement could be as simple as "Deep Blue was smart after he was made" or "Deep Blue was already smart before he was made" -- but that's a little harder to imagine than "Deep Blue got smarter when he was looked up in a book." The point is that Deep Blue is the difference between intelligence being genetic and intelligence being a combination of an algorithm and a clever book about chess. A human and a chimp are on the same level at chess, but the difference between human and chimp intelligence is a lot bigger than the difference between Deep Blue and a computer running a random algorithm._</p><br><p><em>To me, this feels like a really big deal. Being able to recognize when a solution has been found requires a lot of smarts. It seems more likely that the difference between human level mind design and super mind design is what gives someone the smarts to recognize an answer to their own question, or to recognize that a question is actually about knowing how to do something, and that the answer to that question is in the book they looked it up in. Deep Blue wasn't able to recognize that it could get better chess by modifying its code. He could recognize that the book he read said chess was a really hard problem and he thought that would make sense to look up. But it has never occurred to him that there's a difference between being smart by trial-and error and being smart by trial and error from changing your programming.</em></p><br><p><em>So this is another of the lessons I learned while writing this sequence. People are right to be skeptical whenever you try to answer a hard question by saying "Just program a computer to think about it" or by saying "Just modify the AI's code by trial and error to do better." If the only answer is "Maybe someone will figure this out eventually?" then you can't actually answer your question by using a computer, or by writing an algorithm or by making a better book; you have to use trial and experiment, or trial-and-trial and-error, or trial-by-chance. Just be sure you don't take that same strategy to solve AI, because your own AGI will still be using trial and error to solve problems, and it's just the difference between trial and error from different sources which makes you able to recognize the fact that a solution exists, even when you don't know what it is. And don't tell me, either, that Deep Blue had just an excellent book and that the problem was just the problem of the book. Then the book-writer, by the same process of recognition, will figure out what Deep Blue was able to get by trial and error because it has to be different from what you get by trial and trial-and-er-error from reading a book. There's no reason to think that Deep Blue is that much more intelligent than myself, or that Deep Blue was smarter than others with an inferior book. And as Deep Blue shows, a sufficiently advanced book-reading AI isn't necessarily an AGI.</em></p><br><p><em>Now here's a general lesson I've learned while doing this sequence on problems in AI: If you have an Artificial Intelligence, that doesn't have all its problems already solved, it's possible to be really good at computer programming, without knowing that you have an Artificial General Intelligence inside you. The book-reading trick I talked about above is one example of this. Knowing something just inside is usually not enough to be the first to solve it \xe2\x80\x93 often you just have to ask the right question in the right way.</em></p>      </span>    </div>  </div></body></html>