<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Where Would You Be Without Us?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">John S. Wentwort</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>The "Where" of AI Alignment Research</li>
</ul><br><ul>
<li>Where AI Alignment Researchers Would Be Without Us</li>
</ul><br><ul>
<li>AI Alignment Research as Global Coordination Problem</li>
</ul><br><ul>
<li>AI Aligned Researchers in 100 Years Without Us</li>
</ul><br><p><em>This post was written in partial response to the 2019 book review on MIRI and the Future of Research on AI by Abram Demski.</em></p><br><p>The "Where" of "AI Alignment" Research</p><br><p>"What is an aligned AI system, anyway? Why should we expect that to be feasible when so many systems seem hard or impossible?", you may well ask. In fact, questions of this type are probably the most common question asked of people in the field, so I'll start there.</p><br><p><strong>One of our jobs is just answering this question. We answer it in different ways, depending on where the discussion happens. But most discussions happen in the context of what I call the "Where" of the AI Alignment research.</strong> I describe this as "Where" because it is a physical space in which AI Alignment research happens, and because there is often disagreement about "Where" (i.e., space) within a field. Note that at least within AI Alignment, this is different from "What are the research questions we want solved, anyway?" and "How can we solve the questions that we want solved?"</p><br><p>The "Who's Important" questions are almost always answered in the context of a particular research project or a particular "Who's Important?", and so may not be thought of as a "Where" question, in general. But, for example, it is helpful to answer who/\xe2\x80\x8bwhat/\xe2\x80\x8bhow questions with regards to a sub-solution to AI Alignment (i.e. a piece of work to <em>solve AI Alignment in an aligned way</em>) in order to understand the sub-solution.</p><br><p><strong>Many discussions happen in the "Where," but other than that, those discussions are often at cross-purposes with each other within the same field. This can cause misunderstandings and frustrations amongst researchers\xe2\x80\x94perhaps the researchers' most important jobs in AI alignment may be to make the field understandable, to make the field clear and legible.</strong> This is one of the reasons why <em>many</em> researchers in AI alignment are so interested in building common knowledge, or standardization, or clarifying the terms we use, or having our work stand up to peer review. It is one of the most important reasons why the Alignment Forum exists (and even the Alignment Newsletter in its original form and later versions was intended to facilitate communication across research areas).</p><br><p>I will use these terms, "Where" and "What are the Alignment Research" questions, interchangeably.</p><br><p>__I'm personally most interested in the Where question for AI policy and strategy, __but it is also clearly relevant to the more object-level AI Alignment research, or what I'll call meta-research. A major issue with AI Alignment (and many other fields) is a lack of clarity not only about what field/\xe2\x80\x8bquestion needs to be solved, but how. How do you determine which tasks need to be done, and whose tasks need to be determined from the initial project, or how to break the initial project into individual component tasks and then outsource/\xe2\x80\x8bdelegate work to others? And, more abstractly, how do you know which aspects of a particular research problem should be tackled in parallel or in sequence (i.e, is the problem in need of theoretical research, or empirical research, or a combination thereof, or what) and when it is that research might end? We try to answer this question within the overall framework of "AI Aligned and a good future for all", but even within this framework this question is highly contentious, in my experience.</p><br><p>Where AI Alignment Researchers Without Us</p><br><p>How might the Alignment research world be different without a critical mass of AI Alignment researchers (i.e AI Alignment researchers that have a particular set of skills, background, or expertise necessary to think about the questions?) What would be their places and responsibilities within the field? Is research that could be AI Alignment-centered, but that doesn't involve them, or involve them only for some aspect of the research, or something else?</p><br><p><em>Some</em> aspects of an AI Alignment team might be less important than others. If there is no one person or type of personnel who has the skills and background to think about agent foundations all day, one of the AI Architectures groups might simply have to spend several whole years thinking about agent foundations, and others can just ignore it. The key aspect is that someone must be doing the agent foundations work, somewhere.</p><br><p><em>(Note: This is not an original question for us at MIRI or FHI, and I'm less interested in what the positions of research/\xe2\x80\x8bmanagement staff might look like in the world without us as research leads, and much more interested in what the questions of the field might look like, if this were to be one of the positions, especially of the researchers _themselves</em>.)_</p><br><p>This leaves us with the question of, "What would that look like?"</p><br><p>AI Alignment Research as a Global Coordination Problem is an initial, if messy, attempt at answering this question. A few thoughts:</p><br><ul>
<li><strong>Research tasks and skills are often highly specialized</strong>. If you are a good theoretical computer scientist, and there is a whole group of people specialized in AI Alignment at a top-ranked US research university, you <em>may</em> be able to get a job on a team of people who work on Agent Foundations or AI Architectures or whatever, but you probably can't get one, for another specialized group with a different focus. You may be able to get involved, in some aspects, but you won't be doing all the tasks, and your primary contributions to Agent Foundations may be in the theoretical work that is required, because much of the work done on actual algorithms is done by people with other skillsets.</li>
</ul><br><ul>
<li><strong>People in these communities are in different places.</strong> We are not talking just about the US and the West here\xe2\x80\x94alliances and clusters of researchers in different places in the US and the rest of the world are likely to exist and be relevant to these particular communities. In general, the nearer the Alignment "neo-"-community's physical locations (in a country, in a city, etc. - I use "community" to mean something like "people who do or talk about something interesting" and not a particular organization or organization in the sense of a legal entity) happen to be, the larger the community, the more likely that the people who do work on Alignment and other relevant topics are able to get a foothold to solve their particular research problems.</li>
</ul><br><ul>
<li><strong>Some communities have more of a concentration of people in one place</strong>. If the AI Alignment community in the USA has an especially good record of helping each other, it might be the case that a group of people in one location (e.g. in Berkeley, CA) is able to form their own team there, because they are able to find all of the other people. This may not be possible in all locations, though.</li>
</ul><br><ul>
<li><strong>This is a coordination problem for the people who are located in different places to coordinate with each other.</strong> If a team of technical researchers are spread out over 100 institutions, a single team of researchers in the same location (e. g. UC Berkeley team) may be able to form their very own team that does research. This is a coordination problem <em>for all existing teams in different locations</em>, not just teams in one location. Each such team that is located in a physical place (and possibly within a particular legal jurisdiction too) is likely to want to expand into their own team that does work in a particular area of AI Alignment research and is able to, if there is any demand for that.</li>
</ul>      </span>    </div>  </div></body></html>