<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        The case for aligning narrowly superhuman models with our values      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Scott Garrabran</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>I.</li>
</ul><br><ul>
<li>ii.</li>
</ul><br><ul>
<li>I.1</li>
</ul><br><ul>
<li>I.2</li>
</ul><br><ul>
<li>I.3</li>
</ul><br><ul>
<li>I.4</li>
</ul><br><ul>
<li>I.5</li>
</ul><br><ul>
<li>ii.1</li>
</ul><br><ul>
<li>ii.2</li>
</ul><br><ul>
<li>III.</li>
</ul><br><ul>
<li>IV.</li>
</ul><br><ul>
<li>V. </li>
</ul><br><ul>
<li>VI. </li>
</ul><br><ul>
<li>VII. </li>
</ul><br><ul>
<li>VIII.</li>
</ul><br><ul>
<li>IX. </li>
</ul><br><ul>
<li>X. </li>
</ul><br><ul>
<li>XI. </li>
</ul><br><ul>
<li>XII. </li>
</ul><br><ul>
<li>XIII. Conclusion</li>
</ul><br><ul>
<li>XIV. Appendix </li>
</ul><br><p>This is a post in a series that is meant to be read in order, so that you can see how the previous parts build on each other.</p><br><p>In this post, I argue that there are two related tasks that ML techniques are likely to be able to accomplish.</p><br><p>The first task is to allow us to build AI systems that have a reasonable expectation of being aligned with human values.</p><br><p>The second task is to allow a system  to do well at a particular task while being aligned with human value in that particular case.</p><br><p>This post will argue that when we get systems that are only slightly better  at a task than humans, or that perform better than humans at that task, these two tasks are the main ways in which we will be able to see how a system is aligned. As such, we should have higher confidence that our alignment techniques will succeed the smaller the gap between the system and our values.</p><br><p>(This may sound surprising. We have good reason to believe that human values are malign. Most current techniques that try to align ML systems with our values fail. We seem to be stuck with a "race for AGI" where we have to be less constrained to have a better chance of succeeding.)</p><br><p>There is a more subtle consideration which will also lead us to place higher confidence that "AGI alignment theory will work" at smaller margins. However, the point from the last post makes that "if we want alignment we need more conceptual progress", so I don't want to be too misleading about this. I also want to note that I am not arguing that alignment is easy, far from, I do believe that "if you try even a little bit alignment will get easier". However, I do think that aligning a system that is only slightly above human level would be an easier task.</p><br><p>This post is also very much a response to Ajeya Cotra's draft report on the Alignment problem.</p><br><p>I will explain why I am optimistic about AlignNarrowly.</p><br><p>In the case that "AlignNarrowly can be solved", I will argue that there are ways in which the fact that our models are super human at the task that they are trying to do is a positive: </p><br><p>(a) it is probably a positive thing that our models are slightly worse than human in some ways.</p><br><p>(b) It is probably a positive fact that we can align each other given that we are not completely sure  how our systems are going to fail.</p><br><p>(c) It seems likely that the fact that our systems are only moderately superhuman in some ways is a very strong argument for the chances that human values are very likely to be malign.</p><br><p>This post only covers the first claim.</p><br><p><strong>II.</strong></p><br><p>I will state a more formal version of the claim. Let A be a system that has a reasonable expectation that it is aligned with our values, and let a be the system that it is aligned to. For a specific decision problem. Define the "cost" c(a) of aligning a system to a specific decision problem a to be one plus the percentage reduction in expected loss of the system, where loss is measured according to the human's values. If the cost is negative, it means that at least that part of aligned a can be replaced by  with an unaligned a. The cost is not necessarily 0%, but must be positive. </p><br><p>Fix some decision problem. </p><br><p><strong>Claim</strong>: For most decision problems, the cost of aligning an aligned narrow system to that decision problem is non-negative. In particular, if a is aligned and </p><br><blockquote>
<p>Cost of aligned a is positive, then </p>
</blockquote><br><blockquote>
<p>If c(a)&gt;0, then </p>
</blockquote><br><p>(<strong>Conclusion:</strong>) At least it is more likely that a is aligned than it  would be without this knowledge.</p><br><p><strong>Proof:</strong></p><br><p>Fix some decision  problem. Let S=(S,I) be the Solomonoff distribution on all programs. Fix some error model E\xe2\x88\x88\xce\x94O\xcf\x89. For, we define the cost function.</p><br><p>We define the probability of a program  as</p><br><p>.</p><br><p>Note that  will always be positive by the definition of a reasonable system and the fact that c(a) is defined to be non-negative.</p><br><p>Assume Cost of A is non-negative and the reduction is positive. Since  it is reasonable, our A system either gives us an unaligned version of S that also does well on the decision problem, or it gives us access to  as well as   such that it gives us a positive amount of expected reduction. In the first case, we already have our aligned version of  if that is possible at all.</p><br><p>Otherwise,  is both a reason for optimism and a strong argument that our system is malign, since if the cost were negative,   would be our aligned S. </p><br><p>I am very confident that we will never have the second case with alignment.</p><br><p><strong>I.</strong></p><br><p>This is a stronger version of the fact that aligning models is a positive. </p><br><p>What is important here is that if you  and your model does not do well, then the loss caused by that failure is not as bad as the loss of unaligned A  (which does do well). </p><br><p>I do not claim this is a particularly good argument for aligning narrowly superior agents. I am primarily arguing that it is an argument in favor of aligning.</p><br><p><strong>II.1</strong></p><br><p>I will try to expand on the idea that being aligned with human preferences will help with solving many difficult alignment problems.</p><br><p><strong>IIa.</strong></p><br><p>The first argument above is that alignment might help other alignment. It is not the whole argument, but it is a major part of why I think alignment is a good idea: namely the fact that it reduces the amount of alignment research that needs to be done by giving us an aligned agent when we run out of other ideas.</p><br><p>As such, I will try to give an explanation for how the alignment of a slightly superior system is related to the alignment of a much more superior system.</p><br><p>In many cases it is useful to use very different models that are better in different ways  than our current models. One can think of these models as representing different kinds of concepts to solve different kinds of problems.</p><br><p>For example, if you are an AI programmer, it is not useful to have to understand both the concepts that give a neural net its good behavior (which you can think of as "why" the neural network is good and what it can do when you give it a good training set) and the concepts that give it good behavior (which is a lot harder to understand because it means you have to understand how  works). It is much easier for the programmer to have only the concepts of why the neural network works, while having the concepts of why  works (i.e.. a big lookup table).</p><br><p>I would expect that we would get systems that have better and better explanations about why their decisions are good, while simultaneously getting systems that are much better at making good decisions.</p><br><p>There is an interesting parallel here to the way humans do mathematics. First you learn all the theorems and lemmas that are needed to prove a specific theorem and lemma. As a result, if you understand the proof, you can reproduce it almost mechanically. In contrast, if you do not understand the proof, then you probably still can not reproduce it fully, but it will be much harder to do so.</p><br><p>It seems likely that once we get good algorithms for aligning non-trivial systems with human values (especially if they are allowed to learn more and are given a new set of environments to get better at achieving their goal), this will allow us to align systems more capable than  with our values.</p><br><p>Some of these powerful systems might also be aligned by learning about the human values and following them. For example, a system that is aligned to make good choices might start by trying to understand the values and then making good choices based on them. </p><br><p><strong>I.2</strong></p><br><p>Since   is aligned, this means that any  that is only slightly worse than  will have a cost of aligning  to the decision problem  that  of +X for some positive X. This means that for any non-trivially difficult decision problem  that we can run, we can create an 'aligned', and the cost of alignement is not that bad, so 'aligned' is a reason to prefer.</p><br><p><strong>I.3</strong></p><br><p>If  is aligned but  is not, then  and only  because   will be in a position to replace  with  and have an  that is only a bit worse than  if only.</p><br><p><strong>I.4</strong></p><br><p>In a sense, I am arguing that it is very easy to have a system that is a little bit worse than a human at a task, but is aligned. In a sense, that is a very strong endorsement of alignment theory.</p><br><p><strong>I.5</strong></p>      </span>    </div>  </div></body></html>