<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        How Should We Make Moral Judgments      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">John Wentwort</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>Is Morality Preference?</li>
</ul><br><ul>
<li>Is Morale Preference?</li>
</ul><br><p><em>(In my recent post on utilitarianism, I mentioned "morality" as a potential term in a utility maximization problem, to talk about preferences. After pondering some of the implications of "preference", the most straightforward term I hit upon was _meta-preference</em>: <em>a preference about what sort of preference is being made</em>. When we come to make such a preference about the object level, can we really think of it as <em>merely</em> a preference? I think the answer is yes: the fact that any <em>human</em> makes this preference and any <em>human</em> is <em>influenced</em> by this preference indicates that it cannot be reduced to an object level preference\xe2\x80\x94it is rather a meta-preference. __With this terminology, we can see several types of moral intuitions\xe2\x80\x94for example, moral claims that what is "morally right" (imply the right thing to do in the appropriate situations) or "morally wrong" (imply a wrong thing to do in or situations that deserve a frown from most of us) - all are meta-preferences (as far as I know). But this isn't meant as a criticism of meta-preferentialism, as a form of meta-preference is obviously a legitimate concept to draw from and learn from. Rather, an alternative way of looking at and understanding things that has been helpful to me is that the brain builds things up from below: it starts with a series of moral intuitions (which themselves are derived from moral intuitions), each a meta-preferance, and eventually works them up to the level of preferences (which we can call by other names in other contexts: morals for me; morals for others with different values). Thus, instead of interpreting "morality" strictly as preferences, we can instead think of it as preferences about preferences.<em>)</em></p><br><p><em>(Epistemic status: I haven't looked into moral intuitions themselves, just how they're expressed. This is a guess at how we can make the transition from meta-preferentialism to a system that implements and learns from moral intuitions. If you've read the sequences before and think this is obvious, feel free to skip to the last section. See also this comment by Robin Hanson.)</em></p><br><p>Is Morality Preference (In this post) or Morale Preference (In the linked post)?</p><br><p><strong>I.</strong></p><br><p>In his post on utilitarianism and <em>Homo Economicus</em>, Will MacAskill writes:</p><br><blockquote>
<p><em>[MacAskill] is asking what type of preferences it would be rational for H to have. But what's rational for the _actual _H, and what's rational for an idealised agent, are very different questions.</em></p>
</blockquote><br><p>(Also, I'd say the "type of preferences" Eliezer is asking about is more like "type of meta-pre-ferences we should have", as opposed to "type of preferences", which should be reduced to the former.)</p><br><p>I disagree. I think the meta-preferencialist view on morality gives a nice way of interpreting Eliezer's statements. You don't get a clean-cut issue here. But here's my model, which you may find helpful in interpreting Eliezer in ways that make sense (I tried to avoid any assumptions I'd have about Eliezer's mind or intent, so there may be some room for dispute here in how to read Eliezer's words as evidence for or against these assumptions).</p><br><p>Like any other goal, morality is an instrumental goal for some reason, in this case, to predict the behavior of other agents and act intelligently with that goal in mind. Let's call this agent-level preferences. In other words, morality is to some extent <em>the purpose we give to ourselves</em>. This means you can't have a preference about "this is the purpose", rather those preferences are meta-preference for how you implement our own preferences. In this case, the preference is a preference about utility functions and other decision procedures and algorithms. Thus, even a human that doesn't explicitly think that their moral behavior depends on meta-prefereces like utilitarianism, still has those preferences encoded somewhere in their utility functions and algorithm implementations. As for the Eliezer example: the human may believe they have those preferences, but those preferences aren't the source of their actual behavior. So, the difference between wanting utilitarianism to be "the purpose" and wanting to act like consequentialists with their utility function is not significant. The latter is merely encoded in their preferences about <em>how to act</em>, which is clearly a preference about meta-prefrence.</p><br><p>On the other hand, the idea of a moral "type" is clearly important, both in the sense of a concept humans try to reason about and in the sense of an ontological classification of what is. If you have preferences about meta-preference, for example, you can model the behavior of similar agents better. You don't need to treat these agents as <em>hypothetical abstract objects</em> that don't exist; they exist in your model of human behavior, the real human brain is made of neurons with their own utility functions encoding this abstract reasoning about possible agents. And that is of course an abstraction, so human models may not always be accurate. However, just using human models means that you don't need to know exactly what those preferences are, just that humans do them.</p><br><p>The moralities of different agents will differ in various ways. This is the classic "person-affecting views" vs person-neutral views, as well as consequentialist vs deontologist vs virtue-ethicist views to name a few. On this model, these do not matter at all when you are deciding what course of action to take for yourself. In this sense, all agents are basically expected-utility maximizers. However, to choose between what action to take, your preferred decision-procedure is a preference to encode in your utility function, and even if you are uncertain about what it is, it is encoded in your preferences over other preferences, which themselves are encoded in your utility function. Thus, moral intuitions about the right and wrong things to do will influence behavior.</p><br><p><strong>II.</strong></p><br><p>In the linked conversation, MacAskill and Soares discuss an analogous issue with utility maximization (MacAskill: I know, we haven't talked about it.) It is also a question about the best decision-procedures for agents (MacAskill) or decision-procedural utility functions (Soares) to learn from to act in real-world scenarios. So this is a question about moral and meta-preferal intuitions. They argue that this is not a good answer.</p><br><p>I disagree. To me, this is clearly a question about the utility functions one uses in the real world. If you don't know your utility function, you are lost. Even if you have other preferences you can act on, those preferences are only going to help you act if you have enough information about your utility function.</p><br><p>(This is not a full solution, in the sense that you still need to figure out exactly what utility function you have for various situations. Humans certainly do it in various ways, but I'm not sure they do "the best thing" on any principled basis.)</p><br><p><strong>III.</strong></p><br><p>I have a somewhat more formal formulation of this that I can point to, but I'm still not fully convinced. I don't feel like I've fully developed this model so much as cobbled together the pieces of some other model in some other order.</p><br><p>First, how much utility do we give to our preferences about our preferences? It seems like a straightforward thing to call "meta-preference" (or "meta-prefence"), and it clearly does have some weight. To use MacAskill's terminology, do we take a ratio between the preference and other preferences? Or do we take it as a percentage? Something else like the former (at least, this is the approach I think would be used to define things like "sadistic" preferences (which are anti-utility preferences, that push away from what we want), or "hedonistic" preferences (that push towards what we want)?)</p><br><p>Next, how do things like utilitarianism and deontology influence our actual behavior? It's not completely clear to me, this connection has been discussed on LW before. Is there something analogous (but more complex!) to expected utility maximization that you find useful to have in mind? What other preferences can you add to your model that give you some idea of what to do? On the utilitarianism and deon-topology parts, Eliezer and other LW people seem to emphasize the importance of knowing what your preferences are. But for me, the question is slightly different. I expect that I can come up with some preferences that are instrumental for something I want to do (e.g. wanting to know how to act in some situations) and which push me to act in different ways. But there are other preferences about how to act (like utilitarianism) such that it won't help to try to explicitly state those preferences in my decisions, unless my preferences give me information about those preferences (which will not happen in most cases). I want to act in some ways, but this doesn't mean I just need to find preferences that I expect will help me in those ways. I need to find preferences about how to find preferences, so I need to formulate my preferences somehow, even if I am just trying to figure out how to act.</p>      </span>    </div>  </div></body></html>