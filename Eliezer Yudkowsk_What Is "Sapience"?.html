<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        What Is "Sapience"?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Eliezer Yudkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><em>"Do not speak of that which we do not understand. When we seek to understand 'intelligence', what we really think of is human intelligence\xe2\x80\x94specifically that of a human brain running on human metabolism. This is the only experience that we have; it is all we ever know. It is, therefore, the only 'intelligence' we can talk about."</em>      \xe2\x80\x94Artificial Intelligence: A Modern Approach, 3rd edition, Ch. 1, "Intelligence"</p><br><p>Intelligence, a certain kind of intelligence.</p><br><p>Now, in the beginning of last year, Anna Salamon and I talked about these very complicated words called "emergence."  But "emergence" is a metaphor, to be taken and thrown away when we encounter a more fundamental notion of what is. I used the term Sapience in the title, to point to the particular kind of intelligence to which the other term refers.</p><br><p>A word that can be the name of both an abstract concept and a particular thing, at the same time, is really not, in the long run, very useful, unless our understanding of the abstract concept itself is also adequate in some way. You may try to say, "The Singularity is when the abstract concept 'intelligence' describes a more fundamental form of intelligence," but the words don't add up to the same thing, because the abstract concept has no referent apart from whatever particular thing is inside the box.</p><br><p>A thing that is more than two parts put together, still has meaning <em>even if we don't know how to find one particular two-part thing inside a box.</em> A thing outside that box.</p><br><p>I used the word "transcendent" as the name for the thing that I called "Sapience." It is one of those words that has meaning when you have a partial understanding, and loses its purpose the better you grasp the notion.</p><br><p>I should go on record as stating from the beginning that <em>I</em> am <em>not</em> writing a sequence of post-singularity pieces called "How to Transcendentiate," because Transcendentians have a problem in communicating which problem they think needs to be solved, and hence they're stuck using the word "transcenent."  They may <em>imagine</em> a sequence of posts, but it's really a different matter...</p><br><p>(The sequence on Friendly AI might conceivably be part of such a sequence, but it's not meant to be. I write <em>about</em> the concepts that come before. Only when you're stuck do you go into writing the thing itself. That's why I wrote, "The first challenge is to see the noncentral fallacy in <em>general.</em> So that you can <em>notice</em> any noncentral fallacy that <em>might or might not</em> be useful in describing something that <em>may or may not actually exist,</em> somewhere outside the central cluster of previously examined clusters.")</p><br><p>On to the question of "What is sapience?"  It's a term that has referents when you know <em>what you're talking about,</em> but not a complete term which fits everything you know in advance. A term that can describe the thing you know is more than the thing you know.</p><br><p>What is "intelligence"?  That, too, is a difficult term to define. It's a notion which is more than one particular human brain running on specific human metabolism\xe2\x80\x94more than human intelligence per se. A word which, by itself, has no meaning apart from whatever its referent may be.</p><br><p>But when we take, and add up, more than one particular case as an axiom, we get a pattern, one particular "intelligence" which can be defined as an algorithm operating on human cognitive algorithms.</p><br><p>And this pattern of axioms, we can call an "intelligence pattern", by analogy. We do not quite mean to refer to an intelligence pattern in general, but rather a particular algorithm which takes a human form and outputs a human output.</p><br><p>And the "human form" of this particular intelligence pattern is <em>sapience</em>: it is the human who can say that intelligence is the art of creating tool AIs in such a way that the final output is aligned with utility, or say that the "intelligence", as a pattern, includes a utility function embedded in the algorithm's goals.</p><br><p>We can say that an "intelligent" organism is one that acts in the environment to improve its fitness according to a utility function\xe2\x80\x94even though you wouldn't want to describe a <em>molecular human</em> as being intelligent.</p><br><p>We humans have a preference for crisp and unified notions instead of clunky abstractions. You wouldn't want to say that a molecular human brain contains a tiny tiny program that is a "little human," even if we could identify such a thing.</p><br><p>If you look at the concept of sapience from that perspective, we find that it is <em>the human in "AI",</em> the pattern that outputs a human brain on the input of some <em>narrow</em> human domain, that we are pointing to.</p><br><p>Humanity as a whole is capable of producing minds that can take over the Earth. Humanity as a whole is not intelligent. But a single <em>human brain</em> is intelligent\xe2\x80\x94has humanlike intelligence\xe2\x80\x94and so is a computer running on human-compatible processor architectures and running on human-compatible operating systems, if that computer's programmers were to <em>optimize</em> for a human-friendly output.</p><br><p>We do not know how to turn a human brain into a sapient mind. In particular, we do not yet know at all whether such a transformation is physically possible without causing brain damage. When I consider the implications of this fact, I use the word "sapience."</p><br><p>This definition of sapience may seem circular: one might look at the brain, and think, "Are you sure you want to say that this thing can improve the world? Did you really think that you were the first person to think of that notion in this abstract way, and that this is therefore more than the sum of the parts?"</p><br><p>But this definition of sapience is not meant to <em>imply</em> that one can get from a <em>whole brain</em> to a <em>non-whole brain\xe2\x80\x94_it does not imply that one can do that in all cases, or even some cases. A human brain is a whole brain, while an AI is an intelligence pattern. But a human brain and some non-human AI might be _physically identical.</em> One in the pattern space, not in the space itself.</p><br><p>When I speak of non-central fallacy, remember that there is no set of parts in the universe where the entire pattern collapses to the sum of its parts.</p><br><p>A whole brain is one particular thing, to a pattern is a pattern. A human brain contains a human intelligence, and the human intelligence has the potential to improve the human world. But this is <em>not the same thing as</em> a human brain having <em>human-friendly</em> traits.</p><br><p>I say all this, because I <em>will not</em> debate definitions.</p><br><p>I may not have the <em>technical expertise</em> to know whether a human-friendly AI is even physically possible to build as opposed to another kind of AI, if humans are still running the same hardware. So I'm left with <em>a pattern of abstractions,</em> rather than whatever I know it <em>is</em> to be, a "human friendly" AI. And I use the pattern-of-abstractions as an axiom in the definition of sapience.</p><br><p>I will <em>define</em> sapience as "the human in a non-human AI", by analogy with intelligence.</p><br><p>I don't know when the Singularity will happen, and I don't know what the Singularity will look like, I don't know whether an "intelligence" algorithm embedded in a human is <em>necessarily</em> a friendly intelligence, and so on; and if it is to have a positive effect on the human world, it's important to have a positive notion of what a positive positive effect on the world looks like.</p><br><p>Now that we <em>have</em> a positive notion of "intelligence" and "sapience", the concepts no longer need a metaphor to describe what they refer to.</p><br><p>I also happen to know a lot about AI, and if you'd asked me what "intelligence" refers to in the context of AI, I could answer you. I've used these concepts to talk extensively about AI.</p><br><p>As I did with "transcendent"\xe2\x80\x94you could look up "transcendent AI" on the web, and that's how I found out about the term, having spent a great deal of time (and money) on it. It refers to a <em>particular class of future AI entities\xe2\x80\x94_those smarter than humans but still limited to the _human</em> range in intelligence\xe2\x80\x94that would take over the world.</p><br><p>But "intelligence" doesn't <em>just</em> refer to human-created intelligences. Even our human intelligence, the highest intelligence out there, is not that much more intelligent than other kinds of intelligences, such as chimp-level intelligences.</p>      </span>    </div>  </div></body></html>