<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Two models of mind      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">John Wentworth, Ben Hoffman, and Eliezer Yodkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>One model</li>
</ul><br><ul>
<li>Other models</li>
</ul><br><ul>
<li>Two models</li>
</ul><br><ul>
<li>Conclusion</li>
</ul><br><p>This post was originally written for a conference on how "the human brain" works, but I ended up rewriting it as an introductory post to the multiagent models project. But I found it might be of interest to people who already understand (or could easily get familiar with) the multiagent models of mind, and who want to understand the human mind with those models in mind, so I figured I'd post it here. (It also seems related to some of the recent posts around decision loops in models of cognition.)</p><br><p>The main purpose of this post is to describe and motivate what I think is a useful model for thinking about the human brain and mind, and to demonstrate how useful using this model is. It's a first attempt. It's probably missing some details. It's definitely missing some key pieces of the puzzle. But I haven't seen those parts mentioned explicitly anywhere, and neither have I seen attempts to explain them in the context of this model. So I hope this motivates others to explain those things.</p><br><p>One model</p><br><p>A few years ago, in an unpublished (if still highly applicable) thesis, I described (what I believed to be) a model which could describe what most of the parts of the human brain look like. I called it "the model of hierarchical control and a reinforcement learning subsystem connected to it". It seems too complicated and non-intuitive, though, so I'm hoping to write something simpler and more intuitive, hence this post. My apologies if I am already using common terminology, or if there is a much better term for the specific model I'm about to describe, than "hierarchical control and reinforcement learning".</p><br><p>The model is the result of trying to understand how people control and motivate their movements, and how that control system might interface with other control systems in the brain. Since it's ultimately about motor control, I'll limit my presentation to the case of muscles and the central patterns involved. The case of muscles is the simplest in some sense, but also the biggest and most interesting to humans. It also seems useful to have lots of control systems connected to the same learning system, so I'll stick with it. I have the most details of other cases I present in the appendices to the paper, which I suggest you read if you want to learn more. I'll also introduce the terms "primitive motivations" and "reward", which go into a lot of detail how this model works.</p><br><p>The rest of this post is about two kinds of subsystems: "control systems" and "reinforcement learning systems". I'm going to call the control system a "decision loop" when implemented in neural tissue (or in something analogous to that, like a computer processor), and I'll call the reinforcement subsystem a "meta-controller".</p><br><p>(There are also, of course, a number of "control systems" in the model in addition to the decider and the reinforcer, such as proprioceptors, and various modules to model other humans and other decision loops; but I'm going to ignore those unless specifically pointed out when I mention them. I'm probably leaving out other important things. So read the appendices of the paper if you want to know what they are and why. This post is not intended to become its own tutorial.)</p><br><p>Other models</p><br><p>I don't know whether "the human brain has control systems" is the common knowledge belief in the field of psychology at this point, or whether my description of "hierarchial control and reinforcement learning subsystems connected to it" is common terminology already. But it seems worth mentioning, anyway, for the sake of completeness, and because I know it to be missing from some parts of my own thinking.</p><br><p>The control-and-reinforcement model has a number of advantages which make it potentially a more useful model to work with than other existing models of the brain. It allows us to distinguish between different subsystems more easily, and helps us understand when subsystems are interacting with each other over time. It also allows us to better define when subsystems <em>within</em> an agent are acting "in unison" over some period of time (if, for example, both subsystems have different values), and when subsystems <em>between</em> agents are "interacting" with each other over some period of training/\xe2\x80\x8blearning. Finally, it helps us define what "inputs" subsystems have/\xe2\x80\x8bare, and what "outputs" they have/\xe2\x80\x8bare.</p><br><p>The two subsystems I'm going to use in this post will be two subparts of the hierarchical control model: the reinforcement learning subsystem, and the decider subsystem. (I'm thinking of that as a "decision module" in the sense of the multi-agent models subroutine included in the human brain model.) I'm not going to cover how the hierarchical control model relates to the rest of the brain.</p><br><p>The reinforcement learning subsystem</p><br><p>The reinforcement learning system has several parts that together seem generally necessary for any system that acquires skills over time, including the human brain. A reinforcement learning system with no reward is one that isn't useful for learning. The central concept is that an "agent" makes decisions which cause it to receive reward based on how well the outcomes of those decisions match its current values (as defined by the reward and decision algorithm). Then, that "agent" stores those values in a memory structure over time to use later when deciding how to respond to new decisions.</p><br><p>The following diagrams are intended to show how the reward value and the values themselves, are stored by neurons in a hierarchical way\xe2\x80\x94as the values are used in the reinforcement learning system's calculations. (There's also some implicit computational work in how these calculations are done, but that will just be left out for simplicity.)</p><br><p>The agent itself is implemented as a recurrent neural network, and its reward and values values are stored in the weights of its connections. For our purposes, the weights to and from the neuron represent an "observation" of the state of that neuron, followed by an "action" which is implemented as another neuron which receives reward for how well it matches some current value.</p><br><p>As in the hierarchical control model, the agent's actions do not directly cause changes in neural connections, which means they are not directly implemented in the neural tissue, and they are implemented in the "meta-controller"\xe2\x80\x94namely, the reinforcement-learning subsystem connected to the hierarchical control system.</p><br><p>The decider subsystem</p><br><p>The decider system has several parts\xe2\x80\x94the agent, the memory structure/\xe2\x80\x8bcontroller, the value function, the prior, etc. - that together seem generally sufficient to understand human thought and action. The value function is a stored representation of all the actions an agent can actually take, given all of its current knowledge and all of its other systems which are interacting with it. The value function can, in theory, be combined with a learning system to create a planner, but in what follows I'm going to present the decider subsystem as separate. This simplification will give us a clearer picture of what's going on.</p><br><p>The value function and prior are stored in a memory structure, which we'll abbreviate M2/\xe2\x80\x8bV/\xe2\x80\x8bM2. The agent's prior can be represented by any number of neurons. If it has been trained/\xe2\x80\x8btrained/\xe2\x80\x8btrained to an extent that the prior is better then the value function in some sense, it gets more weight in the value function than others. The value function <em>and</em> the prior are combined into a "value function" neuron, NeuronM2/\xe2\x80\x8bV.</p><br><p>NeuronM2/\xe2\x80\x8bM2 are combined via a "decision" neuron to create some number of decision neurons, NeuronM3/\xe2\x80\x8bM2. This is a very simple algorithm; the decider weights to and from NeuronM2 should always be 1 ("if a neuron with negative weight exists, increase the weight that leads to those weights being 0"). NeuronM3 then receives inputs from each decision and from the value function, and sends outputs to the "context neuron", NeuronM1.</p><br><p>The context neuron will then provide a number of different outputs based on the context in which NeuronM1 is active:</p><br><ul>
<li>If NeuronM1_2[Decision] is positive, then NeuronM1 will send the output NeuronM1/\xe2\x80\x8bDecision2, which is simply the action associated with the decider which most recently had outputs positive on neurons NeuronM2\xe2\x88\x92NeuronM3. I'll call this "the first action in the decision", if it's ever explicitly mentioned. If we have multiple copies of NeuronM3 and NeuronM4 active, then we might want to take the actions which NeuronM4[Decision] output was largest on\xe2\x80\x94this corresponds to the first action in the second set of decider calculations, etc.</li>
</ul><br><ul>
<li>If Neurom1_1[Decision] or Neuron3A[Decision] are positive, then Neurom1 sends the output Neuron1/\xe2\x80\x8bDecision3A, which is the action associated with decision module output NeuronM2. I'll call the second action in the decision Neurom1/\xe2\x80\x8bDecision1. I'll call Neuron3A the "decider output" neuron.</li>
</ul>      </span>    </div>  </div></body></html>