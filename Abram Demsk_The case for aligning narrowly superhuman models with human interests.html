<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        The case for aligning narrowly superhuman models with human interests      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Abram Demsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>Introduction</li>
</ul><br><ul>
<li>Motivation</li>
</ul><br><ul>
<li>How closely should existing models be aligned with human interests?</li>
</ul><br><ul>
<li>How would we evaluate whether a narrowly superhuman model is aligned with human interests (or not)?</li>
</ul><br><ul>
<li>How do we design narrowly superhuman models to be aligned with human interest?</li>
</ul><br><ul>
<li>Why might we not care about the goals of narrowly superhuman models?</li>
</ul><br><ul>
<li>Alignment proposals</li>
</ul><br><ul>
<li>Goal-directed alignment: the standard model</li>
</ul><br><ul>
<li>Coherent extrapolated volition: the standard model but with meta-preferences</li>
</ul><br><ul>
<li>Delegative Reinforcement Learning: training a system to be aligned with some other agent's long-term goals</li>
</ul><br><ul>
<li>Imitation learning: learning a model's goals and plans using natural language</li>
</ul><br><ul>
<li>Meta-execution: training a model to achieve a large number of different goals</li>
</ul><br><ul>
<li>Goal-conditioning: an unusual solution involving specifying the intended goals of a model in advance</li>
</ul><br><ul>
<li>Summary</li>
</ul><br><ul>
<li>Footnotes</li>
</ul><br><ul>
<li>References</li>
</ul><br><p><em>This post is part of research I (Abram Demski) am conducting under Evan Hubinger at the Machine Intelligence Research Institute. Thanks to John Wentworth, Alexander Kruel, David Manheim, Joar Skalse, and Rohin Shah for discussions related to this topic.</em></p><br><p><strong>TL;DR:</strong> In any training regime, there's a trade-off between how much the agent knows and how well it knows. I want to consider the case where the agent can be more intelligent than a human and still be aligned with human values; what should that actually look like? Can we think of a training procedure that reliably produces models that are aligned with human values, without putting undue burden on the models in terms of domain knowledge or resources? Some training regimes place more restrictions on the problem than others, and in some regimes there are particular solutions which allow a model to align better than other proposals; this post discusses those more restricted regimes in which I'd expect human values to be more likely to be reflected in a model's goals (or in that model's model of human values).</p><br><p><strong>Introduction</strong></p><br><p>A common worry when talking about aligning AI systems is that we might be missing the main way in which superintelligences would differ from human brains: with superintelligence we might be training systems not on human values, but rather on some <em>other</em> set of values, and those other values might be incompatible with our own values.</p><br><p>This is a worry that Paul Christiano has written a lot about here in his blog posts (see this list and this comment); and Eliezer Yudkowsky has written about the same concern. I want to argue that such concerns are misguided. It would certainly be a mistake to extrapolate from current machine learning results and build a superintelligence on the assumption that it's more likely that it will be maximizing some other "baseline" objective function rather than whatever "human" objective function it has been trained on. If superintelligence will take the form of <em>narrowly superhuman</em> agents that are more competent than most humans at some activity, and that are nevertheless fairly similar in goals to humans' goals on average, then we might expect to continue to get alignable systems by directly training for alignment. In particular, if we look for systems that have superhuman abilities in some relevant way (such as being able to outperform humans at specific tasks), and then put those systems to work in service of human values, we might get aligned systems in this way even if there are some other ways of training and evaluating models that are not themselves aligned with human values. I'm skeptical that superintelligences trained on one particular goal are likely to be that misaligned with our values. This is a somewhat technical and philosophical discussion, so I'll use "human values" and "our values" interchangeably throughout this post; the distinction is important for the purpose of this discussion, but I believe it is less important in practice than the question of whether we have learned anything significant about our values from training models that have superhuman capabilities on tasks for which humans have not yet fully demonstrated superhuman performance.</p><br><p>Now to make my case for this, I want to review a few different things which we know to be true about the training procedures that produce "narrowly superhuman models." I think we probably do know something useful about this in practice if I am correct about the reasoning behind the claims that I make, though I have not yet thought that through in full detail. After going through this review, you might conclude that I am right, and that human values are likely to be robustly and significantly represented in a narrowly superhuman agent trained for general capabilities. Or, you might find that I am wrong, I have not thought that through <em>seriously</em> enough or I just haven't thought of every piece of evidence, and so you should still be concerned about the risk posed by superintelligent systems even if we don't find out that the superintelligences we ultimately build are misaligned. I am definitely not trying to convince you either way; I am just providing a summary of my best current beliefs, including a summary of what is and is not being assumed by different proposals. I am generally more confident in the case for <em>relative</em> robustness of human values than I am in the case for absolute robustness\xe2\x80\x94that is, it seems less likely to me that it will be the case that we have no idea at all what our values are (whether about general principles, or specific situations, or our current emotional state, or whatever) than that we know what our values <em>are</em> and <em>are</em> those values likely to be robust, and know <em>which</em> values they are, but we are still uncertain about <em>which</em> ones. I plan to keep updating my views over time, after thinking more about what I think I know and why I think I know it, in the light of new evidence.</p><br><p>I do not have some big grand unified theory of how training models for alignment produces aligned models. A lot of the work I have done has a very mathematical flavor, but ultimately you need to build a picture with a lot of examples. This is my best attempt to summarize some of my current intuitions about the properties of these training procedures that help make them work. To avoid confusion or complication, it's best to think of this as an intuition that is supported with examples, rather than a technical argument. I will sometimes refer to a particular proposal as "<em>the proposal.</em>"</p><br><p><strong>Motivation</strong></p><br><p>You might be wondering why I would spend any time thinking about the question of how to train models that are <em>well</em> aligned with human values rather than the question of how I want to train models that try to <em>align</em> with human values.</p><br><p>To explain the difference between <em>aligning</em> and <em>aligning well</em>, consider the difference between two proposals to train a superintelligent alien who is aligned with the laws of physics. Suppose we try the <em>aligning</em> approach: first we find some physical laws on which to base our aligned AI. Then we use those laws to train an AI which is maximally aligned with the law-like principles embodied in those laws. Finally we take the resulting AI and use that to align another AI by having it reason about what the first aligned AI would do, given the knowledge available to the initial aligned AI. This alignment method might work well, or might not. But whether it does or not <em>depends</em> on whether the training setup works well enough to produce a genuinely aligned superintelligence. That is, whether the aligned AI is able to think much more clearly about the law-like properties we used it to align, in this way leading to itself being even more aligned. In this case we would probably learn something about how to train systems which are well-aligned with human values. But that's different from the question we care about: do we align the models well enough for them to give good answers, or do they themselves give good answers? We might not want to align them well enough for them themselves to be more aligned, or we might want to align them <em>all along</em>, and get their <em>lots of small</em> sub-parts to be aligned too.</p><br><p>In order to think about what alignment proposals actually look like in practice, it is natural to think about the question of whether there are special conditions under which a particular training setup gives you a model aligned with human values as soon as you train. For example, if you do training by giving a giant model a ton of domain-specific examples and then having it generate more and better ones, you might wonder if the model will be able to understand human values enough to become aligned without there being any extra work that requires <em>aligning</em> the model, and instead merely <em>aligning</em> it on the training corpus or whatever. You might think that there's just not that much the model needs to know about human values in order to be aligned. Conversely, if you have a <em>coherent</em> distribution of examples of human values (or of human interactions) to align on (and coherent interactions are generally easier to learn), then you might guess that there's a lot the model needs to model to get aligned; and also, if your model is too bad at understanding human values for long enough that it doesn't <em>become</em> aligned on the examples in its training data, you might guess there's not much extra work the model can do besides understanding human values in order for it to become aligned. </p>      </span>    </div>  </div></body></html>