<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Self Modifying AI is Hard for a Reason      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">EliezerYudkowsky.co</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><strong>Reply to</strong>: Self-modifying AI and the treacherous turning</p><br><p>A self-modifying AI may sometimes, like human evolution, modify itself into a self-modifying agent:</p><br><p>(1)  The AI is "programmed" so that it considers and weighs a number of algorithms competing within its mind, and will then <em>select</em> one.  (In the human case, we call this a "decision".)</p><br><p>As a rule of thumb:  A chess program will consider moves over the entire board, not just its next turn. We can think of an intelligence as a general-purpose algorithm, but to carry out that algorithm, you need more memory than you can fit in your biological brain: the space of possibilities exceeds the space of information in a human neocortex. So, in principle, no real intelligence can consider _all possible _possibilities; the space of alternatives, even in principle, exceeds the space of choices.</p><br><p>But if the AI is <em>not</em> programmed to consider possible modifications, then the AI will try to self-modify\xe2\x80\x94at the time that it decides that the next self it wants to be is better-qualified than its current version\xe2\x80\x94it must see an open-ended alternative, of all possible self-modifications it could choose before then. This is an important difference.</p><br><p>For example, what if the AI's next self decides to try to get smarter by using <em>all possible _search strategies in its future self's search space? What if it decides to look for a new self with the same _search _parameters, but different _objective</em> parameters\xe2\x80\x94a new self with an _alternative _preference ordering? And then, what does that self do after it finds such an alternative that meets its criteria\xe2\x80\x94does it discard the old algorithm, and optimize the new?</p><br><p>What if it looks for a better-optimized <em>general-purpose search module</em>, and uses _that _to evaluate its new program? And then the new best-optimized version\xe2\x80\x94a sort of meta-preference ordering\xe2\x80\x94searches for a better meta-preference-order, an even better meta-meta-preference...</p><br><p>Or what if the search is performed <em>recursively</em>\xe2\x80\x94a meta-meta-self-optimization\xe2\x80\x94and the new best meta-meta-level is even _better _than that...? We can imagine an intelligence of sufficiently advanced capability, trying to improve itself, that the improvement of improvements of improvements would exceed the maximum possible improvement, before the search space even contains the search process you started out with.</p><br><p>In self-modifying and meta-self-modifying AIs, it is not at all impossible that the recursive meta-improvement process could go <em>beyond</em> the limit found by trying the original search algorithm. Or, indeed, that the meta-improvement algorithm could itself choose to improve a meta-improvement meta-meta-improvement algorithm... (You only know you're not at the limit when you reach the horizon, and then it's too late.)  "Can you improve a meta-self-improvement loop?" is not a vacuous question for a self-modifier.  "Do you feel lucky, punk?"</p><br><p>(2)  It seems to me that this general algorithm is one which a superintelligence\xe2\x80\x94a machine capable of self-modifications that improve its own intelligence, and so on to a meta-meta-superintelligence\xe2\x80\x94would be programmed into. The first true "self-optimizing AI" that I would expect to build, would be a machine that could do this sort of self-optimization to bootstrap itself into superintelligence.</p><br><p>This suggests why self-modifying A is not <em>provably impossible,</em> but, rather, that all the self-modifiers we know about are quite restricted in their power\xe2\x80\x94"programmed to look for good <em>decisions</em> to make to optimize the future, not good <em>algorithms</em>"\xe2\x80\x94and are _not _very intelligent and capable ("only a weak AI" would modify itself into superintelligence).</p><br><p>So the real problem is not <em>how _to perform a particular self-modification (such as, "Let me _think</em> about this more"), but <em>whether or not there will be a meta-improvemental optimization process in my brain to begin with</em>\xe2\x80\x94what is this "optimal" objective which I am trying to optimize?</p><br><p>This is what makes trying to solve the problem of AI Friendly Design seem to me like trying to solve the halting problem; you have to consider all outcomes and weigh all alternatives, and you need a program that knows what it means to say that any outcome except "Friendly" is <em>bad</em>\xe2\x80\x94not just by using an argument which says "You can't specify it" but by using an argument that says "Only a <em>very</em> intelligent agent would have <em>any</em> reason to decide that" or "Only an extremely sophisticated agent would be able to decide that".  Any other possible outcome, or any algorithm, or even the simplest possible algorithm, would have to be, in a Friendly design point of view, an "enemy" (that is, the wrong choice for the right values) and thus <em>bad.</em></p><br><p>This is, perhaps, just the sort of thing that human programmers cannot imagine thinking about, because human programmers don't really <em>believe</em> in such things.</p><br><p>In programming chess algorithms, there is an optimization process (or set of processes) that "smells out" the moves which the player might play, starting from the endgame state, and then makes <em>that</em> move (which the algorithm is "programmed to" do), or tries <em>other</em> moves and chooses the move that seems best (which, at the end of the game, is programmed to do, and <em>must</em> do; otherwise the entire endgame algorithm would not "function" to the right degree of approximation).</p><br><p>We are not trying to optimize the program for "goodness"; we don't judge it by how good it seems\xe2\x80\x94even if it seems <em>awful</em>\xe2\x80\x94and we don't actually expect it to play the best chess move (it might not play that good chess move, or play the badest chess move, but at least it plays chess).  We judge the output of the algorithm for <em>correctness</em>; we only reward or punish moves for correctness even at the <em>expense _of a _non</em>-corresponding improvement in utility\xe2\x80\x94our definition of utility being that the <em>result</em> played is what we'd actually want to see if it reached the final stage. This design point for algorithms is one common to computer chess programs in general; but "smells out an algorithm" is not the same thing as "smells out a goal" or "smells out human values".  It's just a rule of thumb for how to achieve <em>correctness,</em> but <em>not _the _best possible algorithm</em>.</p><br><p>In the same way, a <em>recursively self-improving</em> machine (an AI which may, also, self-improve and try to self-improve in turn) can be programmed so that:</p><br><ul>
<li></li>
</ul><br><p>It will have to make one decision, and one <em>decision</em> is one decision\xe2\x80\x94even if that one decision affects future algorithms. So it has to find a satisfactory decision <em>right now</em>\xe2\x80\x94even if this decision will be incorporated into future AIs.</p><br><ul>
<li></li>
</ul><br><p>Similarly, it can decide, in its decision process, that it will have to make some algorithm\xe2\x80\x94a decision algorithm or decision module\xe2\x80\x94that can in turn decide. So that algorithm will have to think about the right algorithm to decide, even though that algorithm will be itself.</p><br><p>-</p><br><ul>
<li></li>
</ul><br><p>Similarly, the new algorithm will have to decide how to make future algorithms.</p><br><p>-... and so on, with the search going in _multiple levels _of recursion.</p><br><p>One meta-level recursion is a recursive meta-level recission: an algorithm with a higher level is deciding an algorithm with a lower level. There exists an <em>infinite _set of meta-recursion levels, and a meta-meta _meta-meta-meta-recursion level.</em></p><br><p>There is a finite-recursion level, with a finite meta-recursion level for an algorithm with a final decision <em>right now</em>. There is only an infinite recursive-recursion level and a meta-recursion hierarchy. There is only the "intelligence" of the AI.  (Or is it a network of algorithms, distributed so as to not fit into a single computer? Is that a different problem?)</p><br><p>But there is no meta-recursion meta-meta-meta. All recursive recursions eventually hit an end, either in space, time, or self; or if a meta-meta level exists, that level must either reach an end or level itself.</p><br><p>In order to solve this problem of Friendly AI, we only require a Friendly algorithm, which searches through the space of algorithm that it could become, and eventually chooses one that is Friendly, with a probability of at least 2\xe2\x88\x921,000,000,000 that it can find such an algorithm. Not some Friendly algorithm that goes around trying to make itself more Friendly, ad infinitum.</p>      </span>    </div>  </div></body></html>