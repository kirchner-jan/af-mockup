<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        A Taxonomy of Open Problems in Philosophy: Philosophical Stagnation      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Wei Da</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>The First Level (Phil)</li>
</ul><br><ul>
<li>The Second Level</li>
</ul><br><ul>
<li>The Third Level</li>
</ul><br><ul>
<li>The Fourth Level</li>
</ul><br><p>This post was inspired by Scott Alexander's post, How Philosophy Could Be Great, and Carl Shulman's excellent discussion of open problems in physics. In what follows, I'll list some common questions (which often end up subsumed by their neighbors as special cases), attempt to provide a taxonomy which groups the questions together, and then propose some new research questions which expand our current understanding beyond the three listed so far (namely, the questions in the first three levels).</p><br><p>My current main claim is that most of these questions can and should be separated into as many branches as possible at each level.</p><br><p>The First Level (Phil):</p><br><ul>
<li>Logical Uncertainty. Many philosophical discussions concern problems posed by reasoning under logical uncertainty. Are the solutions found in this setting optimal, or do they fail to produce any results in more ordinary settings? Is logical counterfactual reasoning possible?</li>
</ul><br><ul>
<li>Updatelessness. Many philosophcial questions concern how an agent can be said to have "knowledge" when that knowledge is represented by an un-observable computer algorithm. Can we avoid certain pitfalls of ordinary reasoning with unknown objects of reasoning? If not, are there ways to avoid running into trouble that aren't specificially designed to avoid some particular fallacious reasoning?</li>
</ul><br><ul>
<li>Epistemology. Are some forms of knowledge justified independent of their representational content (i.e. independent of how much computation it took to represent the knowledge)? If so, what is this justified knowledge representation? Can this justified knowledge representation be represented in an idealized computational framework? Are some forms of justified knowledge represented by computer programs which act directly on the world?</li>
</ul><br><ul>
<li>Philosophy of AI and Other New Computationalist Problems. Since the invention of computers, many new problems have been formulated regarding computation. Some of these problems were discussed in the context of AI but may have relevance for philosophy in general; for example, does Bayesian probability theory work well for reasoning under logical uncertainty? More generally, do certain properties of the idealized computational frameworks discussed above hold? Should computer scientists use those properties to constrain the designs and goals of their programs?</li>
</ul><br><p>These questions may, at least to some extent, have "philosophical" importance. That is, they may not have solutions in the full philosophical sense, but they do have important consequences on how we do philosophical research. It seems likely that understanding these consequences and finding better solutions than are currently used in philosophy (or in contemporary AI), would be valuable contributions to the art of philosophy.</p><br><p>The Second Level:</p><br><ul>
<li>Artificial Intelligence. Many recent developments in artificial intelligence involve building computer programs with certain beliefs and preferences. For example, AlphaGo uses mathematical models of Go to perform Go-based decision making. As the AI systems become increasingly sophisticated, and the knowledge which they have (or what they learn) becomes more and more rich and complex, will these systems exhibit emergent goals, behaviors, and decision making which are unlike what we humans were "designed" to exhibit in certain ways, and which we may consider to be problematic? Will we develop effective methods of controlling and preventing such emergent behaviors? If not, what are the barriers that stand between us and the creation of powerful AGI agents, or between AGI agents and other powerful AI agents?</li>
</ul><br><ul>
<li>Philosophy and Intelligence. This includes studying the nature of the processes which produce human-level general intelligence and reasoning, or the processes which produce higher levels of intelligence, such as humans with access to the internet. The most obvious questions this area relates to might involve examining how human intelligence and reasoning relate to computer intelligence and reasoning. Are human abilities in some ways a fixed design choice or emergent one? Is a formal theory of intelligence useful (or even possible) to the study of human-like reasoning? </li>
</ul><br><ul>
<li>Artificial General Intelligence. Will we eventually create machine AGI capable of making use of its vast, un-interpretable world-model and the computational resources available, for the sake of achieving its goals, without requiring any human intervention beyond the initial inputs of a formal program? If so, what are the features of its initial design which make it work in these ways rather than others? Can the AGI's design be improved in a "safe" fashion? Or are we obliged to "trust" whatever initial design AGI's implementers choose?</li>
</ul><br><p>The Third Level:</p><br><p>Other Things Philosophers Care About: It seems that the field is rather diverse, including areas like mathematics and physicalism other than philosophy. I would be very interested to hear what are the problems you feel have the greatest potential for being solved. For example, can we come up with a safe, computationally effective way to approximate various kinds of idealized reasoning? Do you believe there exists some "correct" way of thinking about the external world and its relationship with your mind? What kinds of theories correspond to an idealized reasoning framework?</p><br><ul>
<li>Artificial General Intelligences: Will we eventually create a relatively safe and useful AGI? How should we build one? What properties must its "initial version" have (given the computational resources available at that time)? Will we have the time and opportunity to incorporate any insights gained into how to create more safe versions of this initial version of AGI? </li>
</ul><br><ul>
<li>MetaPhilosophy + Metaethics. More generally speaking, what kinds of problems do we have a right to expect progress on? One natural response would be to say that progress in these areas is limited only by the limits imposed on the most general problems we can formulate, and can therefore proceed without reference to <em>specific</em> problems in these areas. However, if there are no limits to how big a question one problem gets until it must be "solved" in a sense that allows progress on the problems of meta-ethics and meta-philosophy, then we may simply end up with a sequence of increasingly large questions, all of which contribute to some common ground (such as idealized reasoning). If there are such limits, we may need the field to divide itself up into various subfields for reasons of practicality. More generally speaking: is it possible to achieve common understanding, or at least cooperation, among scientists with very different philosophical viewpoints simply by giving them a shared language? Given an idealized computational model, is there some set of properties we can specify which makes it a uniquely good model for (idealized versions of) the problem of philosophy? I.e., are there properties which if possessed by all models will lead the idealized agent to agree to accept <em>their</em> model as better than the other? What kind of information can we extract about a shared idealized agent from its behavior in the presence of a wide variety of different programs, given only a description of that agent within a single model? In other words, is it possible to do some kind of reasoning using an idealized computational environment to obtain a general understanding of what a human-level idealized agent will care about, given only a full specification of that agent within that computational environment?</li>
</ul><br><p>Of course, there are bound to be some real-world limitations or differences between the idealized computational environment and the real world, which will not be completely irrelevant to our efforts to apply the results to human level reasoning. We can, to some extent, design AGI's to meet certain constraints, and to some extent AGI's are just computers hooked up to some particular environment in which evolution happened, so it seems worth trying to figure out what we can learn about the idealized world.</p><br><p>I think that these questions are reasonably well defined as separate questions even if they appear to be closely tangled up. I suspect that different people might come up with different groupings. This taxonomy, while intended to illustrate that these questions and many others are important and are more or less well-defined, is of course subjective. My goal is to provide enough of a structure around these questions and their questions so that people will find them interesting, but at the same time I think people should feel free to consider the individual questions in isolation.</p>      </span>    </div>  </div></body></html>