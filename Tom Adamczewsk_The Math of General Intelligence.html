<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        The Math of General Intelligence      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Tom Adamczewsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Intelligence is an extraordinary phenomenon occurring in biological organisms across a wide range of taxa: fish, shrimp, birds, mammals, and even some invertebrates such as the honey bee. In humans, the range of human performance is extraordinary with IQs ranging from 120 to over two thousand. If you want to explore the field and build one you've got some wacky, crazy ideas\xe2\x80\x94such as using a group of ants as a whole\xe2\x80\x94but for the purposes of this essay, I shall assume that the most intelligent humans on earth are human-level.</p><br><p>The question to ask is whether this level of intelligence is possible in a physical substrate, such as a computer system, a brain. If so, is it possible in theory, or would it be an expensive, complicated and unstable endeavor? If there is no theoretical way to create a system of this type, how close do we need to get anyway? This raises the question\xe2\x80\x94if we can't go full FAI, how close do the capabilities we <em>do</em> have need to be, and at what theoretical limits can one reach?</p><br><p>Intelligence was discussed by many contributors to Less Wrong, but none have raised the question of its theoretical possibilities, except for the topic of intelligence as a whole. I myself never thought of that question before, and if I had, it would have been too soon to write about it\xe2\x80\x94before I started researching it. I have to ask my wife how it is. She has spent a couple hours writing an essay about how intelligence can be viewed as a set of cognitive algorithms or cognitive tools, and that this understanding makes the idea of a general intelligence far more coherent than the old idea of "an intelligence that is smarter than humans on average." As she explains, most intelligent agents I know of seem to think that intelligence is a more or less fundamental property of things.</p><br><p>My paper is meant to bridge the gap between the two: intelligence as a set of algorithmic knowledge, and the idea of the general intelligence, such as an AI, or an android, which can perform a large variety of tasks better than any human can.</p><br><p>To be useful as a reference, this essay has to be general. It had to be able to explain a "general intelligence" to anybody without being specific to one particular architecture. It had to talk about the theoretical limits of a system of this kind, and not discuss what's possible with our current architecture. Finally, it's hard to discuss intelligence at all in terms of computer systems, so the article has to be general in terms of any conceivable substrate of intelligence. In this respect it may end up resembling a book review. That is, it's a summary of a paper, in terms of intelligence, and the paper is supposed to be interesting and intelligible to anybody, not just to people familiar with a particular area.</p><br><p>Some people may find the paper useful if they already have a theoretical understanding of intelligence, and want a more technical description of a "general intelligence," but the goal is to be an accessible introduction. I have already done my best with my own understanding, but even if the paper seems insufficient, I hope to fill in the last few paragraphs at the end of the document with links and references to my own sources and to those of others working in the area of "general intelligence."</p><br><p>One of the things I did to avoid going down rabbit holes of various theories and hypotheses was by starting out from a somewhat simplistic, but perhaps interesting approach. Specifically, I argued from the outset that intelligence, not being fundamental, could, by definition, be created by intelligent agents. These agents could be programs running on a digital computer, or human-like artificial intelligences using tools such as neural nets or genetic algorithms. In what follows, I name these various possibilities. An artificial general intelligence, or AGI is one that would be generally intelligent; a neural net is a particular type of artificial intelligence, running on a digital computing machine. An AGI would be a general and intelligence is any particular one of these artificial intelligences.</p><br><p>The more sophisticated the artificial intelligence, the better we can expect it to do at various tasks. I argued that by analogy we can expect human intelligence to reach some particular level, if we are able to create an artificial intelligence that possesses roughly that level of general intelligence; and when we come closer to that level, expect this to happen much quicker, as digital computers surpass traditional mechanical ones in speed.</p><br><p>I then used a simple, heuristic estimate for the rate of progress of digital computers. Since we have a historical precedent, the Industrial Revolution, let us suppose this rate translates itself to how fast we can expect AGI to progress. It was an optimistic idea, perhaps overly so, but this way of thinking has lead to interesting insights about the nature of intelligence and the limits of digital computing, even at its earliest stages.</p><br><p>In this article I shall take my discussion from the point of view I've developed: intelligence being a product of both a machine, such as a digital computer, and a substrate which is not necessarily human brain tissue but any material in which a computational substrate runs. Although my discussion of the topic will focus primarily on AI, it is also applicable to general intelligence: if the substrate is human brain tissue, the term "general intelligence" applies; if it is not, the term "artificial general intelligence."</p><br><p>My definition of general intelligence requires that the system be able to perform the sorts of complex tasks which distinguish humans from other animals:  the ability to manipulate objects, the ability to communicate verbally, etcetera. By "more or less," I mean that this ability can be achieved at a substantially greater cost than it would be to achieve it in an animal, but is, nonetheless feasible within the space of possible digital computers. For example, it can be done on a digital computer which costs more money than a human brain.</p><br><p>I should emphasize here that I am not writing about some hypothetical, far-out idea about the future, but instead I am trying to figure out how much the current technology of "general intelligence" could do at the theoretical limits of what's possible. Of course, this limits the future; but it is only a short distance along this road, as today's computers can do many things which are impossible on a non-digital, mechanical computer, such as fly the plane, or walk to the moon.</p><br><p>I further assume that we already have some sort of general knowledge about intelligence, and that we already know to be approximately at the theoretical limit for the creation of human-level computer program, so that it would not be too soon to attempt to come up with a digital implementation.</p><br><p>For the purposes of my argument, and to simplify the discussion, I shall ignore the issue of self-improvement on the part of AGI. That is, I shall assume for the purposes of my discussion that an AGI would need no internal improvement to improve its performance, but a particular sort of improvement to become more and more generally intelligent. It shall be argued that, in that case, if an intelligence is capable of improving its algorithmic knowledge of the subject matter at hand, then it is in principle able to improve its intelligence in general; the only limit to its improvement is the knowledge it starts with, in the form of its algorithms. By itself, this is probably not enough to reach a level of AI performance, but it will improve quickly with use, and, more importantly, it will suggest certain theoretical limits, if any exist. My argument then is that some sort of AGI is possible, and probably that one of the highest levels of performance of such an AGI could be expected to develop between now and the time when this sort of AGI can be built.</p><br><p>The second assumption is somewhat less obvious, although not entirely false. I shall argue that, in order to make AGI even theoretically possible, one or more of two facts must be true:</p><br><p>(A) One or more breakthroughs in computational theory or technology, or some fundamental improvement in the architecture of a computer. If this particular breakthrough in either theory or technology had been predicted before, we should have been able to see it coming years ago; more likely, it is of the character which occurs only once in a few decades. The other alternative\xe2\x80\x94that there is some breakthrough which occurs only once every thousand or so years\xe2\x80\x94is unlikely, if not impossible. I shall assume here that we can rule out a breakthrough of the kind described in the first paragraph\xe2\x80\x94assuming that this paragraph is even possible to describe. We must then argue that one\xe2\x80\x94or more than one\xe2\x80\x94of the following must also be true:</p><br><p>(B) A breakthrough in a different type of technology, which will be needed to make such a breakthrough in computational theory, or make it occur more rapidly. In this case, we are dealing with a breakthrough in the kind of technology, such as a chip, which is needed in order to make the breakthrough in computer architecture more rapidly possible.</p>      </span>    </div>  </div></body></html>