<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Can AGI be contained or aligned?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Stephen_Lyn</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>Why align AGI to us? What's the difference between "alignment" and "friendliness"??</li>
</ul><br><ul>
<li>AGI as subprocesses in the brain</li>
</ul><br><ul>
<li>What does that imply for Friendliness?</li>
</ul><br><ul>
<li>Does this model rule out Friendliness? What about subprocess transparency??</li>
</ul><br><ul>
<li>What kind of AGI is possible?</li>
</ul><br><ul>
<li>How hard would it be to "align" an AGI?</li>
</ul><br><ul>
<li>Could the process of Friendliness emerge from the development process of an AGI? </li>
</ul><br><ul>
<li>Summary</li>
</ul><br><ul>
<li>Footnotes</li>
</ul><br><p>__Excerpts from the forthcoming book:___ _</p><br><p><em>The Alignment Problem</em> </p><br><p><em>(Edited for the book)</em></p><br><p>Why align AGI to human values? What's the distinction between "alignment"\xe2\x80\x94a process to guarantee that any AGI we may create will have our values\xe2\x80\x94and "friendliness\xe2\x80\x94a process to ensure that any AGI created will be benevolent towards its creators"?</p><br><p>The goal of this book is to create an accessible intuitive understanding of the challenges of creating an AI which is both aligned and Friendly to us. We want to avoid the type of conceptual and philosophical problems that have plagued much of the discussion of AI safety in the past. We can do this by using metaphors and analogies from the natural and social sciences.</p><br><p>We want to create a new kind of thought experiment for AI safety, where you can imagine what might happen if you did something that you shouldn't. This helps you avoid conceptual pitfalls by focusing, as much as possible, on specific details of an imagined situation you should not expect to ever actually encounter. It also allows you to make your thinking less abstract and more concrete.</p><br><p>We'll also introduce a new concept, "subprocess transparency", which is important for clarifying the nature of Friendliness. Understanding the brain's internal functioning is crucial to understanding its motivations and the sorts of decisions it will likely make, so we need to know how its inner algorithms work. Knowing and manipulating the transparency of different levels of intelligence in the brain will be important when we're trying to understand Friendliness and how the problems of Friendliness arise at each stage.</p><br><p>Finally, we'll be using these new abstractions to clarify key problems in Friendly AI, including:</p><br><ul>
<li>How might we know when a thought experiment has sufficiently detailed a particular scenario for us to understand how an FAI might be able to behave in highly reliable and robust ways in that scenario?</li>
</ul><br><ul>
<li>How, exactly, would we go about testing and calibrating AI in this new class of problems using thought experiments, or "thought experiments in AI (TiAIs)"? Can we get a sense of how much reliable and robust performance will be possible on this new type of problem?</li>
</ul><br><ul>
<li>How would we ensure that any such system has truly reliable and robust goals?</li>
</ul><br><ul>
<li>Can we say anything about the difficulty of this problem, or what properties would make this problem easier or harder?</li>
</ul><br><ul>
<li>How precisely do we need to understand and predict the inner workings of AGIs in detail in order for them to be considered FAIs?</li>
</ul><br><ul>
<li>How precise would we need to understand the inner workings of our own brains to be able to predict how AGIs will behave?</li>
</ul><br><ul>
<li>Is this the sort of issue which will have an easy or hard solution? What is the relationship between our models of how our own minds work and our models of the way in which we expect any potential AGI to work?</li>
</ul><br><p>Summary</p><br><p><strong>Summary of the book </strong></p><br><p>We want to understand the challenges in AI safety from a new conceptual lens that takes the form of thought experiments, or <strong>thought experiments in AI</strong> (<strong>TiAIs</strong>).</p><br><p>This book presents these thought experiments in the context of the brain, a complex system composed of many subroutines, some of which are capable of having beliefs and desires towards things in the world.</p><br><p>You can imagine a superintelligence <strong>AGI</strong> (<strong>AGI</strong>) which is built with some component or part for performing certain types of reasoning. We can imagine a set of components or subroutines (let's call them <strong>subprocesses</strong>) that are involved with any important decision-making process. These components or subrutines interact and may be capable of having beliefs towards things.</p><br><p>We can imagine that each component or subroutine in an AGI might have a different level of intelligence. Most components/\xe2\x80\x8bsubroutines are relatively stupid. But even a component or subroutne as relatively intelligent as an average human being might not be sufficient to form accurate beliefs and desires about matters outside of it. Therefore, we need to be able to talk about the level of intelligence a component or a subroutine in a superintelligence might have with confidence. In order to achieve this, we will distinguish four different levels of intelligence on which to build these thought experiments. We call the level of intelligence the <strong>component's____ __global intelligence, the level of intelligence of the subroutine at runtime, the level of __component____ self-awareness</strong>, and a more abstract notion of <strong>component self-understanding</strong>.</p><br><p>The global intelligence, <em>G </em>, of a component is the degree to which it has knowledge and understanding of the world. The level of intelligence under consideration, <em>nf</em>, is the level of intelligence that the component obtains at any given time about its environment in particular situations it might be placed in. The component's self-awareness, <em>SA</em>, is the level of consciousness or comprehension with which it is capable of having thoughts and making decisions about itself and its environment. Lastly, the component's self-understanding, <em>SU</em>, is a level of intelligence that it obtains about its current situation, whether or not the situation is part of its environment.</p><br><p>These four levels of intelligence come together in an <strong>AGI</strong> as a hierarchy of systems that can collectively work in the most intelligent way to achieve some goal. These intelligences may be as high as an average human, as low as a rock, and may be situated anywhere between those two extremes.</p><br><p>The challenge in AI safety is creating a system of components that collectively exhibits the right level of intelligence such that their behaviors are aligned with the goals of the system's creator(s). It should be clear that global intelligence is an imperfect criteria for this because some subroutines may be capable of thinking about the world in highly intelligent ways. Therefore, we can imagine that a system of components can also have <em>G</em> and <em>nf,</em> <em>SA</em>, and <em>SU</em>. We can also imagine that a given component or subrutine may only have <em>SA</em> on a specific level. In this way, the task of ensuring that any system exhibits an <em>nf &gt; G</em> is the same as that of ensuring that any component or subruture <em>does not have SA</em>. For example, a brain-like system of subroutines that all possess the same level of <em>nf</em> and <em>G</em> would have to be safe. But we can imagine a less intelligent system that may still have <em>SA</em> all over. To make sure that all components are safe, we first need to decide on the range of <em>nf _and _G</em> at which safety is likely to fail and that a system composed of such components is not safe.</p><br><p>As we move down the layers of the hierarchy, the global intelligence requirement becomes more and more stringent. For example, in order for a system composed of highly intelligent components that are <em>not all</em> capable of obtaining <em>su</em> about their situation, the degree of <em>nf &gt; SU</em> that the system might obtain would have to be sufficiently severe such that its global intelligence would become unsafe. Thus, the level of global intelligence of a system of components is related to the level of global safety. To the extent that an intelligence is able to understand how and why it acts, the global safety of the intelligence depends on the global safety of its subcomponents. In the case of an AGI, global safety can only be determined by the global safety of each of its many subcomponents.</p><br><p>An AGI is a global system (not a single global subcomponent) of components. We can call a level of global safety applicable to an AGI's components and subsystems the AGI's <em>level of safety</em>, or just <em>safety</em> for short. It could also be viewed as the level of safety achieved over the duration of an AGI's existence as an AI. This duration could be the entire history of the AGI, perhaps as an immortal life-form that will live to a very old age. Or this duration could be based on how many times the AI gets reset or modified.</p><br><p>We might hope that, if the safety problems associated with the development and use of an AGI are discovered early on in the creation of that system, that the resulting AGI can be controlled to some extent or at least observed over the length of its existence and that it can be made to follow the original goals of its creator(s).</p><br><p>But we can't expect this to be possible when we think of the subcomponents in an AGI as having levels of intelligence. It is easy enough to see that a given AI might only have global safety with regard to some of its subcomponets. And even if it does, it will not necessarily do so with the same degree of safety with regard to a subcomponent as it would with regard to the same subcomponent if it were completely isolated from the rest of the AGI.</p><br><p><strong>A Thought Experiment: ____Subprocess-Level AI Safety </strong></p>      </span>    </div>  </div></body></html>