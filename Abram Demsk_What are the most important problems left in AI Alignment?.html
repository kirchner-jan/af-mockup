<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        What are the most important problems left in AI Alignment?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Abram Demsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>This is a summary of some of the papers I've reviewed for AI Alignment. This is not a list of the papers I plan to review next.</p><br><ul>
<li>Introduction to AI Alignment https://\xe2\x80\x8b\xe2\x80\x8bintelligence.org/\xe2\x80\x8b\xe2\x80\x8b2018/\xe2\x80\x8b\xe2\x80\x8b04/\xe2\x80\x8b\xe2\x80\x8b25/\xe2\x80\x8b\xe2\x80\x8bintroduction-to-ai-alignment/\xe2\x80\x8b\xe2\x80\x8b</li>
</ul><br><ul>
<li>This talk does a good job of laying out many ways that AI alignment is important as well as a big picture strategy for building aligned AGI</li>
</ul><br><ul>
<li>Human Compatible https://\xe2\x80\x8b\xe2\x80\x8barxiv.org/\xe2\x80\x8b\xe2\x80\x8babs/\xe2\x80\x8b\xe2\x80\x8b1612.03240</li>
</ul><br><ul>
<li>This is an alignment approach focused on the problem of creating aligned AIs, that can reason about the human problem space. There's a lot of detail in this paper, which I'll just briefly summarise, and the main difference with the original paper is that it includes many new ideas that were missing from the original</li>
</ul><br><ul>
<li><strong>Theoretical Background</strong>: The paper explains how reasoning about humans can be done efficiently and reliably. The way that this approach thinks about the problem is through a process of recursive reasoning on the problem space. The main thing to be gained from understanding how humans reason is that we can provide a formalization of human reasoning for aligning other AI systems with.</li>
</ul><br><ul>
<li><strong>Technical Background</strong>: The main algorithm that the paper develops is an efficient logical inductor. Logical induction is a type of logical inference and it uses probability theory to reason about what to believe in order to help reason about what to do given incomplete information about the world.</li>
</ul><br><ul>
<li><strong>AI Alignment Approach</strong>: In this alignment approach, there are two main insights. The first is that when we try generalize to something that's not a human, the generalization can be quite bad. The second insight is that there are many models of human reasoning that, according to Solomonoff, seem fairly plausible.</li>
</ul><br><ul>
<li>The authors take one of them, iterate on that generalization, and then take another one and iterate on that. In the limit of infinite iterations of recursive reasoning, we should be able to converge on one that looks pretty reasonable.</li>
</ul><br><ul>
<li>Learning Human Biases via Inverse Reinforcement Learning https://\xe2\x80\x8b\xe2\x80\x8barXiv.org/\xe2\x80\x8bartd</li>
</ul><br><ul>
<li>This paper is mostly about improving learning of human biases (a human tendency for the wrong decision to be made), by learning from humans. This is primarily aimed at improving the process of aligning AI systems with humans as much as possible, but the general process they describe could be useful for any problem where the right model of a human is important to solve.</li>
</ul><br><ul>
<li>The core of the method is a combination of two ideas:</li>
</ul><br><ul>
<li>First, they use a generative model of learning to explain observed behavior in the training data. This gives them information about when humans are acting stupidly.</li>
</ul><br><ul>
<li>Second, they identify which information from a model is important for this to happen. Then, they use that information to train a model to imitate the distribution of that information.</li>
</ul><br><ul>
<li>In the case of this paper, they're using the generative model to think about the distribution of information needed to solve problems.</li>
</ul><br><ul>
<li><strong>Outer alignment approach</strong>: This is a very general idea and a lot of people would object if it were taken as an alignment solution, but I consider it a more important direction to improve outer alignment than to solve the original problems. So, if you read this paper and it seemed like you had a very general approach, then a) check if you have the conditions to improve outer alignment, and b) if so, work on this.</li>
</ul><br><ul>
<li>__Inception-aware adversarial training for inner alignment https://\xe2\x80\x8b\xe2\x80\x8barxf.in/\xe2\x80\x8b\xe2\x80\x8b2RdHc</li>
</ul><br><ul>
<li>This paper mainly concerns the problem of detecting whether a trained neural network contains a learned model using an auxiliary training process. In other words, you want to get confident that it's not merely a neural network that's doing some good reasoning. They focus on this as a proxy for inner alignment.</li>
</ul><br><ul>
<li><strong>Motivation</strong>: We train neural networks on tasks with clear objectives, while having the model learn some objective on top of that at test time. This leads to the model becoming more competent at the task, but also the objective changing so that it's becoming less well aligned. One solution could be to use a proxy measure (like classification accuracy) during training to infer whether the model has learned a model, but it's likely that will fail to detect what the model is optimizing for at test time. That is one motivation for their approach, they also have a second motivation, to help us verify that any proxy measures we use at test time can easily be optimized away.</li>
</ul><br><ul>
<li>It turns out that any well-calibrated proxy measure can be optimized away with simple adversarial attacks.</li>
</ul><br><ul>
<li>And, even though the model uses the proxy measure for its own internal decision-making process, the model has no feedback about the optimization process being used.</li>
</ul><br><ul>
<li>This gives us a way to check whether a proxy measure really is well calibrated or not.</li>
</ul><br><ul>
<li>A related issue is that many proxies are so brittle that you can break them without breaking the original model.</li>
</ul><br><ul>
<li>And, one way in which a proxy breaks could be very bad\xe2\x80\x94it allows a suboptimal solution to be found by brute force optimization.</li>
</ul><br><ul>
<li>So, it's useful to figure out which features of the problem are most important.</li>
</ul><br><ul>
<li>The paper mostly presents one type of attack, where the attacker looks at the gradients of the loss function and then optimizes to turn off (or to cause) the gradients of all of the important variables.</li>
</ul><br><ul>
<li>It's useful to focus on one type of attack because it's relatively easier computationally.</li>
</ul><br><ul>
<li>Robustness of Adversarial Examples https://\xe2\x80\x8b\xe2\x80\x8barxi</li>
</ul><br><ul>
<li>The main contribution of this paper is a particular regularization approach that's supposed to be robust against these types of attacks. For example, if I was planning on optimizing a feature that the classifier doesn't care about, then by using this regularization approach I should be able to get a high loss function even if it wasn't trained on examples with that feature.</li>
</ul><br><ul>
<li>They find that their approach actually degrades performance on ImageNet, but with a small degradation.</li>
</ul><br><ul>
<li><strong>Interpretability approach</strong>: The authors develop a method to gain interpretability of a model by training it to be honest about its internal decision-making procedure.</li>
</ul><br><ul>
<li>This method trains a discriminator D against a generator G.</li>
</ul><br><ul>
<li>First, the discriminator is trained to have low loss on training examples, by looking at all the activations the model made and making their decisions by ranking according to how confident they are.</li>
</ul><br><ul>
<li>Then, the discriminator itself is trained via adversarial training against the classifier, such that we maximize the number of decisions that the discriminator makes the same way the classifier did.</li>
</ul><br><ul>
<li>The discriminator can be trained to be "honest" in that, instead of giving a low probability on a certain example that it thought was likely, it will just say that it was relatively equally likely.</li>
</ul><br><ul>
<li>For example, let S be a set of images that cause the model with probability less than a specified threshold to generate, and let T(S) be the discriminator that says that there were no images in S but that T was relatively equally likely in that case.</li>
</ul><br><ul>
<li>Measuring and Avoiding Side Effects in Reinforcement Learning: A Causal Influence Diagram Perspective https://\xe2\x80\x8b\xe2\x80\x8bar</li>
</ul><br><ul>
<li>This paper presents an approach to a problem in RL: how to avoid having a RL agent cause side effects.</li>
</ul><br><ul>
<li><strong>Description</strong>: We are interested in situations like this: An agent is exploring an environment for some unknown purpose. Say the environment has a red button that the agent cannot see. The agent has been trained with an auxiliary reward function with the goal of making good consequences when pressing the red button. However, now the environment has been modified such that pressing the red button results in side effects that were previously not observed on-policy. These side effects are actually what we would really like the agent to do, so we want to ensure that the agent never causes side effects.</li>
</ul><br><ul>
<li>As the agent gets more and more of these kinds of modifications, there's a risk it will forget what we want. Thus, we add a regularization term that is the negative of the log probability of the model (the discriminator) doing what we want, at every time step.</li>
</ul><br><ul>
<li>The main result is that this new regularization makes the agent much less likely to accidentally press the red button in simple test environments.</li>
</ul><br><ul>
<li><strong>Technique</strong>: I. Instead of the discriminator classically computing the probability that its discriminator classifies something as the desired action, compute instead, the utility the discriminator estimates, normalized appropriately to avoid infinity. Then, maximize this over the possible outputs.</li>
</ul><br><ul>
<li>II. Use this modified utility function, plus a regularization term to maximize.</li>
</ul><br><ul>
<li><strong>Outer optimization argument</strong>: When you have a system that does some kind of "inner optimization" (as in inner alignment), then this regularization should push your system towards never having such inner optimization.</li>
</ul><br><ul>
<li><strong>Robustness argument</strong>: This regularization should not cause things to go significantly worse than they would not have otherwise, because all the important parts of the model being trained via reinforcement learning. For example, we would still see reward functions that optimize for things that really want the agent to optimize for actions in training distribution.</li>
</ul><br><ul>
<li>Interpreting Latent Variables in Deep Reinforcement Learning with Causal Influence Diagnostics http://\xe2\x80\x8b\xe2\x80\x8barx</li>
</ul>      </span>    </div>  </div></body></html>