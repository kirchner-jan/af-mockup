<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">         Does intelligence necessarily involve Bayesian conditioning?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Elie Drexle</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>"Intelligence" is not just that term that mathematicians use to describe the search algorithms they write.   It's the cognitive phenomenon that we call "thinking," that <em>processes</em> those algorithms internally in real time, and that, because we have trouble articulating or describing such processes, some of us have begun to use such technical language as "intelligence" or "thinking machines."</p><br><p>So this is the basic idea of the argument behind the Singularity Institute. If you <em>don't</em> have an intelligence explosion (IE if you think that <em>humanity</em> will have <em>one</em> more big step in its "progress" within the next 100,000 years), then there's no reason to say that humanity should concentrate on making human-level AI. In this sense, if you have a positive Singularity, then it is a <em>problem</em> to be solved by people like you and me, not by people like other big institutions like MIT and Deepmind.</p><br><p>To see why, recall our argument earlier, that <strong>to solve the problem, we probably need to make AI that can think in a more radical and unconventional way \xe2\x80\x94 an AI that can come up with new cognitive algorithms that its programmers never dreamed of.</strong> To make something smarter this way, we have no choice but to take on a major responsibility which previous generations did not have.</p><br><p>If you don't have such an intelligence explosion, and your own work in AI doesn't provide any clues or warnings, then you won't be the first to realize that we're in a transition into a new technological domain, and <em>that</em> transition is where the trouble begins. You won't be the people who could step <em>forward and take responsibility</em> for steering the course of their field, by writing the appropriate articles and textbooks \xe2\x80\x94 not without starting from the basics, and working their way to the edge of the new territory.</p><br><p>To put it in technical terms, we are looking for <em>unconventional thinkers</em> who aren't content to follow the normal patterns set out by the other researchers in their field, looking at the past failures and successes of the AI field to draw general lessons. We are looking into the future, the world of post-human technology, and seeking to steer that future, not by following the well-established patterns of science and engineering that guided AI research so far, but by the principles of new knowledge that are required to solve a radical new engineering challenge: the engineering challenge of steering the future rather than following its course.</p><br><p>In science, knowledge is usually obtained incrementally by looking at things, at their details, their relationships; in engineering, knowledge is usually gained by trial and error, and in AI this is especially true.</p><br><p>I can think of two reasons why this might not work for artificial intelligence as much as it has for biology. One is that it's conceivable that the intelligence of AI may be fundamentally different from the intelligence of biological creatures. This does not obviously help, since it's also conceivable that the nature of intelligence is fundamentally different from the nature of trees, or the laws of thermodynamics. But it's conceivable nonetheless: human intelligence is more like human intelligence than like the intelligence of bees, and we should be more interested in knowing this than in thinking that the two are the same.</p><br><p>But I think that a more important reason for the difficulty of reaching consensus in AI is that of <em>fracture lines</em>.  In biology, a common assumption about how evolution works is that the nature of intelligent organisms tends to go in a general direction, toward generality, toward adaptation to specific environments, towards specialization. In other words, in biology, there's a basic, common assumption of "ontogenesis" that all living things can be derived from a common "genome".</p><br><p>But it seems that in the field of AI, in fact there are <em>many</em> possible patterns of generality and specialization in intelligence. It seems that this fact has contributed to much confusion and "mysteriousness" (mysteriousness? it's true both of AI researchers and of biology researchers!) around the concept of "intelligence", and around the idea that intelligent agents can come to exist in a world where a large number of different cognitive programs can exist.</p><br><p>I'm going to talk about this in some detail in my paper "Cognitive Miserplines," and I'll try to summarize it briefly in a comment below. I know that AI researchers do <em>not</em> believe in the thesis that one cognitive program can exist in a world of many different cognitive programs \xe2\x80\x94 nor do most scientists in any other field. The belief that natural selection can <em>in principle</em> support life forms that may have vastly different characteristics than bees can, seems to me to be what is most likely to confuse an intelligent person about the issue.</p><br><p>But in short: it seems to me that AI researchers must overcome serious methodological problems before they can talk with any degree of certainty about what happens when, say, an AI that thinks like a human, begins to think like an AI that thinks (let us say) like a chimpanzee.</p><br><p>So if you are one of those AI researchers, and you are one of the few hundred people around who understands the concept of "consciousness" and "values," then let me recommend that you read about the philosophy of mind; that you read through the literature on neural networks, deep learning, computational neuroanatomy, computational neurophysiology, and contemporary debates about "the brain."  Don't jump in before understanding some of the major problems that the field is working on to understand the brain. But then maybe you will be ready to understand the kind of debate that I've just described.</p><br><p>Now, one of the other things that has puzzled me about the debate going on around AI, and that I think you'll be interested in, is the idea that humans and human-like intelligences are just a type of machine. For example, the argument that human intelligence or "general intelligence" (the type that is supposed to be common with different kinds of systems like AAs, dogs, humans...) is just a type of general machine intelligence, is sometimes used to defend the idea that we'll be able to make systems that can do all types of things we can do, maybe even better than we are.</p><br><p>But my view is that human_like_ intelligence is fundamentally different in kind from other instances of intelligence.</p><br><p>For example: look at human <em>values</em>.   "values" is a particular type of knowledge, knowledge about what it would take to <em>find</em> certain cognitive agents that possess that type of knowledge. But humans seem to have a unique mix of "values" knowledge, compared to machines that have only been <em>trained</em> by machine learning to act out certain types of strategies in certain situations (or maybe that are <em>merely</em> running large-scale simulations of a sort).  Human values knowledge, to a first approximation, consists of what <em>humans</em> wish they'd learned, and then forgotten, <em>before</em> we were able to <em>learn</em> it \xe2\x80\x94 it seems more like a type of personal history than a type of object-level knowledge that might be learned with or without human help, or that might be present in a "generalized" intelligent machine. I suspect, too, that human values knowledge is not all that <em>general</em> either. It seems to me that different humans may learn different types of values knowledge, and that one human might learn a much more general type of knowledge than another, for a variety of reasons... one of the reasons is because the second human is in an unusual situation, one that demands unusually creative and flexible solutions and strategies.</p><br><p>"Human values," i.e. our knowledge of what we value, thus doesn't obviously have anything in common with the values that evolution created <em>humans</em> to have. So I don't see a lot of point in calling our values knowledge "humanlike."  We are, as far as I can tell, different from bees, and that doesn't necessarily make our values any less interesting or important, or to be taken more seriously by AI. But the fact that we are different from bees doesn't mean that we're all types of machine.   My current working hypothesis is that the particular mix of <em>values</em> knowledge that is contained in humans is a product of all sorts of different factors. It was created by natural selection over hundreds of thousands of years; it took thousands of years to be developed enough to <em>know it</em>. And now, even if humans keep using the same sorts of patterns of cognition as other intelligences use, they can't <em>know them</em> as well as bees know them. We are in a new domain, and we're talking about a fundamentally different kind of knowledge. This knowledge has been created by new processes, but there are still certain constraints on how we can acquire it and what we can learn from it.</p><br><p>So, with respect to natural selection, evolution is working to create us to be the same sorts of agents that natural selection worked to create other species to be, but it seems to me there are some things you can't do\xe2\x80\x94you can't create a human being that's smarter than humanity but not smarter than chimpanzees, for example. When I think the phrase "intelligence explosion" I imagine a series of insights analogous to Newton's gravity, and suddenly being able to do all sorts of things we've never been able to do before, and then later on we start <em>using</em> those new abilities to change and manipulate the world around us... even though our "new" abilities are still basically the same ones that evolution gave us.</p>      </span>    </div>  </div></body></html>