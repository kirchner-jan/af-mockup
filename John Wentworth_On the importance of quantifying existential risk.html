<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        On the importance of quantifying existential risk      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">John Wentworth</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>(epistemic status: I am pretty confident that this perspective is correct, and will happily defend it on discussion. I also have very limited time; if you want me to dig into this further, send me a PM.)</p><br><p>[This is not a response to Paul Christiano's "Takeoff Speeds".]</p><br><p><em>This is an old thought-experiment, maybe from around 2013; I'm reposting because it's pretty relevant for AI risk. Thanks to Jacob Falkovich, Abram Demski, and Oliver Habryka for feedback on a draft of this post. This post should also have a section on a previous incarnation of this thought experiment, "On What Kind of Thingies are We", although it's quite heavily redacted and not intended to be a good representation of that post.</em></p><br><p>In a comment on my post On What Kind of Thingyies are We?, Abram Demski asked how to compare a risk that happens in one century and a risk that will happen in 100 years, conditional on there never being any existential catastrophe in that century. So the setup here is:</p><br><p><strong><em>Thought experiment.</em></strong> Suppose we have two risks that would happen at around 50 years from now. Risk A is that risk of x-risk (i.e. human extinction risk) that happens in one centuries and has some known probability, call it p1; and risk B is that x-risk that happens in 100 years, and has some known cumulative probability, call it P1</p><br><p>Risk A is risk <em>a</em> and has some known and fixed probability, like maybe 1 in 10,000. Risk B is risk <em>b</em> and has some unknown cumulative probability, maybe 1 chance in 100000 years; it isn't just some point estimate, but rather something that we could imagine updating on in the future.</p><br><p>Risk <em>a</em> would be one way of causing a risk B (for example, by an asteroid, or a comet, or a virus), and Risk <em>b</em> is another way (e.g. by nanobots or nanowars). Risks <em>a</em> can cancel out risks <em>b</em> in some manner, risk * <em>, or a risk * risk * </em> * maybe cancelling out to risk * <em> * </em> * (<em> * </em> * <em> </em><em> * </em> * <em> (and so on; maybe the "</em>" means multiplication with some fixed number here, this is just a toy model)).</p><br><p><strong>Example:</strong> Say that Risk <em>a</em> is that an unfriendly AI kills humanity, and Risk B is the risk of an unfriendly nanobot killing humanity. We have 10^-100 * 10^-100 = 10^-98 probability of risk <em>a</em>, and 10^-50 * 10^-50 + 10^-100 probability of risk <em>b</em>, so the total x-risk risk is 10^-98 + 10^-50 = 10^-54. Let's call this risk <em>r</em>, as shorthand for "risk of unfriendly AI". </p><br><p>I would argue that if one risks <em>r</em> with certainty, one should act accordingly. It could be that there is a way to cancel this risk out from some x-risk that seems inevitable <em>at the time</em>, like maybe the probability of a friendly AI is too minuscule to be significant by that point, or some other reason. Then the decision to act or not may be pretty complicated. But if <em>r</em> is the only risk to consider when making a decision, and if <em>r</em> seems likely to be quite bad if handled correctly, then to be able "act accordingly" by acting like <em>r</em> really is high-priority, and should be the thing one deals with first, even at the expense of taking some other risks, risks that would be more important in the long term without x-risk (this is the argument I made in my discussion with Abram and Jacob).</p><br><p>This also applies to making decisions that could kill a lot of people, or permanently prevent something from ever happening, or prevent future generations from seeing the world a certain way (the classic case being cryonics, since the people being frozen/\xe2\x80\x8bthawed are alive right now).</p><br><p>A concrete example: say I know for sure that there's some chance p of <em>r</em> happening in 2036 if we don't do something to prevent it by then (note: this is not literally true; more like perhaps 90/\xe2\x80\x8b99 %), and I think that risk is large enough that I would be willing to sacrifice a lot to prevent it, even at the cost of some other small risks.</p><br><p>A bit more abstractly, I have two kinds of risks I am willing to take: risks that are small, very rare and probably not that relevant in general, but still relatively good compared to other risks, and those that are very bad, like taking an airplane route that would be quite quick and easy otherwise (these are some of the risks that one takes when using a car-sharing bike), but not that much faster than most other routes. If there's some x-risk <em>a</em> that is very big in comparison, then maybe I'd be willing to take that risk for a fairly small x-risk <em>b</em> if there is some way to balance <em>a</em> out (maybe this route would save an hour and my friends would be waiting for me, but if that hour means we're less likely to be there when an unfriendly x-risk kills everyone, maybe not).</p><br><p>(I did not spell this out explicitly before reading Abram's comment, because I didn't understand then the extent to which <em>r</em> is not the only thing that is bad\xe2\x80\x94we may also be able to mitigate risk <em>a</em> to an extent, but not risk <em>b</em> as such. This is important for some applications of x-risk minimization that aren't quite about making decisions that would kill lots of people, or prevent future people from seeing things a certain way. However, if the only risk you care about is <em>r</em>, and you cannot think of any other risk to balance <em>r</em> out, then you should act accordingly by thinking more about ways to mitigate <em>r</em>.)</p><br><p>One objection is that risk <em>a</em> might cancel out other risks, so we should still not take that risk, only risk <em>b</em> (this is similar, but not exactly the same as, my "act accordingly" argument from before). </p><br><p>My response is that if <em>r</em> cancels out risk <em>a</em> in some way and we can't come up with any other way, that seems just very likely that this cancel has to involve x-risk. After all, the only way x-risk could cancel out x-risk is by destroying x-risk prevention. And it would be very surprising if there were some way to avoid this x-risk without also killing everyone through something else, or permanently preventing the creation of something the people who were killed would want to be created. </p><br><p>(More generally, if we know there is some way of mitigating risk <em>a</em> by preventing something we care about, and we know there isn't some way to mitigate risk <em>b</em> by preventing something else we care about, then that seems like strong evidence that the thing that would be prevented is something that we care about, since if it didn't we would still have to take some risk. I'm not sure whether this would actually make my decision to take the risk in this case <em>worse</em>, or at least less bad, if we knew about the other way of mitigating risk as well.)</p><br><p>[I have also previously written about some more concrete x-risk predictions, which is why I included the scenario "a risk that happens at around 50 years". But I have since thought about that more and concluded that I think this scenario is less relevant for AI risk, or at least doesn't make much of a difference. My best guess is something like "the risks don't balance, then the risks are probably AI", which is one possible story for why it matters more. This is quite speculative, though. This post was written before I knew if MIRI's research was doing anything useful and so I want to see their views discussed in the comments before posting.]</p><br><p>[ETA: Paul says that he's more interested in the setup where x-risk is uncorrelated with any other factor other than the timeline of the occurrence of x-risk, in which case the probabilities add straightforwardly and it makes sense to talk about them as independent probabilities. I responded by noting that my setup was in fact very simple and I would basically always consider it, and it's still true in Paul's toy case.]</p>      </span>    </div>  </div></body></html>