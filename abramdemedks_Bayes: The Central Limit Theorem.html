<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Bayes: The Central Limit Theorem      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">abramdemedks</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>The Key Points</li>
</ul><br><ul>
<li>
<ol>
<li>Some Probability Distributions aren't Normally Distributed</li>
</ol>
</li>
</ul><br><ul>
<li>
<ol>
<li>A Few Graphs and Some Probability Distributives</li>
</ol>
</li>
</ul><br><ul>
<li>
<ol>
<li>The Central Limit Theorems</li>
</ol>
</li>
</ul><br><ul>
<li>
<ol>
<li>What Should You Do With This Knowledge?</li>
</ol>
</li>
</ul><br><p>What is the difference between a normal distribution and a uniform distribution? If you aren't familiar, the difference is that a normal distribution can have low tails and still have non-zero expected value (like a uniform distribution which could still be very high in some places), and the variance of one has expectation 2 whereas the variance of the other has just expectation.</p><br><p>In <em>Probability Theory: The Logic of Science</em>, Judea Pearl takes a first look at a Bayesian approach to probability. When Pearl describes Bayesian probability, he gives the central limit theorem as a major example of its power. As far as I know, this is the first use of the central limit theorem in a Bayesian book. I'll look at it, and then we can talk about it in some more detail. The key ideas will be to get Pearl to summarize them in his own way and then talk about the connection. (In general, I worry that he makes too many assumptions, which will be made clearer later.)</p><br><p>If you want more advanced Bayesian understanding, Pearl doesn't leave any real content on probability theory, so it's not very valuable unless you already understand that. I'll skip the material that explains the basics of decision theory and focus on the rest. (You <em>should</em> read that for more advanced understanding, and there are some useful ideas in there, but I won't be getting too deep into that for now).</p><br><p>The Key Points</p><br><ol>
<li>Some Probability Distributions aren't Normally Distributions</li>
</ol><br><ol>
<li>A Few Graph and Some Probability Distibutions</li>
</ol><br><ol>
<li>The Central Limit Conjecture</li>
</ol><br><ol>
<li>What Should I Do With This Knowledge</li>
</ol><br><ol>
<li>Some probability distributions aren't normally distributed</li>
</ol><br><p>This section summarizes the main ideas that Pearl covers.</p><br><p>There are two common distributions that are commonly used to illustrate the central limit theorem (and which are commonly used in a different way in machine learning and other areas). One is a normal distribution (Pearl calls these "frequentist distributions", but they're not frequentist!) and the other is the uniform distribution, which does not look much different from a normal distribution for the purposes of the central limit thorem.</p><br><p>Both of these distributions have the property that their densities, if plotted on a two-dimensional graph, are bell-shaped, like the bell-shaped distributions on this graph:</p><br><ul>
<li>The red distribution is a normal distribution f(x):=1\xe2\x88\x9a\xcf\x80e-12xy2 with x\xe2\x88\xbcN(0,1)</li>
</ul><br><p>And the black distribution is a uniform distribution u(x):=12 with x\xe2\x88\xbcU(0,1).</p><br><p>The key feature of both these distributions is that the heights of their bell curve look very similar on a scale of one length unit. And the heights have a mean of one and a variance of two (respectively).</p><br><p>The question is, which of these distributions is most similar to a normal distribution as the number of samples, n, grows?</p><br><p>The answer, for both of these distributions, is that the distribution converges to a normal distribution when you increase the number of samples. Both distributions have mean 1 with variance 2, but the ratio of the variance of the two as you increase the number samples approaches 2:1 in the case of the normal distribution, and 1:1 in the for the uniform distribution. And both distributions converge to a bell curve like the one on the right, with the "shape" of the bell curve converging to a Gaussian curve.</p><br><p>1a. Some probability distributions are not normally distributed</p><br><p>The argument in the last sentence was for illustrative purposes only. A <em>proper</em> probability distribution on a continuous set X has a well-defined mean and variance in each coordinate. For the sake of that argument, you can call the mean and variance the coordinates of the center and the standard deviation respectively.</p><br><p>(In real life, this definition leads to some awkward edge cases! If the distribution is a normal, the mean <em>is</em> the median, and so if the distance from x to the center moves around the same, the x is an outlier at any given time. But this is an edge case, and we won't talk about it right now.)</p><br><p>The central limit theorem is very strong in this sense. In particular, when you sample N independent and identically distributed variables from a distribution \xce\xb6, their means and variances converge to the same values as N\xe2\x86\x92\xe2\x88\x9e (up to the constant and rounding error terms). This is one of the strongest results in probability theory. But it is usually presented in the context of a <em>frequentist</em> notion of convergence where a result is valid iff the mean, variance, etc converge with probability one. Pearl's use of the central convergence theorem is different, and uses a <em>Bayesian</em> notion of convergence.</p><br><p>But for those who use the frequentist notion of convergence, the argument given in the last section is still valid. The means of the distributions converges to the expected values of the distributions sampled. And because samples from a distribution converges to the population mean, the variance of the distribution converges, up to the rounding error in computing sample mean.</p><br><p>1b. A Few Graph, and SomeProbability Distributives</p><br><p>Before we get to discussing the central limit theorem, we'll discuss two different distributions that are used in graphs:</p><br><p>We can think of the X axis of the blue graph as representing the value of one particular variable in a system. Imagine we are investigating the value of another variable that might be determined by combining lots of different observations. We want to predict the value of that other variable from the information in this graph.</p><br><p>In the red graph, the vertical axis represents a new parameter which is going to be the value of that variable. This parameter can vary from 1 to another variable that we are going to predict. The horizontal axis represents the value of the X variable that we already know, or perhaps, we know the value of some other X node. And so the question is, what is the X node that the new parameter is most likely to correspond to. The graph looks like this:</p><br><p>The question is the same in both cases: What distribution would be most likely to represent the graph?</p><br><p>For example, we could imagine that the value of the new parameter is generated by a normal distribution where the mean is equal to the existing X value and the standard deviation of the distribution is the coefficient on this term.</p><br><p>1.3 The Normal N(X\xe2\x88\x92\xce\xbc,\xcf\x832)    \xce\xbc</p><br><p>Another common distribution for graphs is a uniform distribution, where X is uniform(\xce\xbc1,\xcf\x832). The distribution with a single mode looks like this:</p><br><ul>
<li>X\xe2\x88\xbcU(\xce\xbc1,\xe2\x88\x9a\xcf\x832)</li>
</ul><br><p>Or we can imagine a <em>log</em>-uniform distribution with mean \xe2\x88\x92\xe2\x88\x9a\xcf\x832. (The log scale isn't particularly standard, but log was used by Pearl in his book to define his log odds, a variant of log2.)</p><br><p>If you've been following along with the previous posts (and perhaps even read this whole series of posts), you'll notice that the two distributions are the same, except for some constant factors in the variance. And so, if we are not particularly interested in the value of the variable we are estimating, but just the <em>ratio</em> to \xce\xbc, we wouldn't be looking at graphs that would distinguish between the two distributions (if we can't even estimate the mean \xce\xbc).</p><br><p>One simple way to demonstrate this is to ask what is the X value that corresponds to a specific value of the ratio /\xe2\x80\x8b \xce\xbc? In the case above, the ratio is \xce\xbc1, which means that the X value 1/(\xce\xbc1) corresponds to (1\xe2\x88\x921/\xce\xbc1) values of the ratio. So by drawing some graphs showing the range of possible ratios and X values, we can often determine which distribution corresponds to a given ratio and X value:</p><br><p>(or alternatively, we can make each of the graphs as a histogram, and take the value of X at a histogram count of one minus the ratio)</p><br><ol>
<li>A few graphs and some probability distributions</li>
</ol><br><p>Pearl draws some graphs to show some distribution-properties. They will all be probability distributions over a specific thing: the variable with ratio /\xce\xbc. Some of the graphs will be continuous, and others will have multiple discrete possibilities (in reality, probabilities that are continuous may have discrete approximations, but for now assume discrete).</p><br><p>For example, consider the case where X\xe2\x88\xbcu(1\xe2\x88\x9a\xcf\x832). Drawing a histogram of X values:</p><br><p>If you ask which continuous probability distribution with this ratio corresponds to the graph above, the answer is a normal distribution with variance \xe2\x88\x9a\xcf\x832:</p><br><p>In the section after the graphs, we will talk about a few other properties of distributions, like tails, variance, etc.</p><br><p><strong>3. The Central limit theorem</strong></p><br><ol>
<li>Introduction to Graphical Models</li>
</ol><br><p>Before we get into the key points of the CLT, we will cover some graphical models and the Bayesian Inference Method, which is an elegant way to specify probabilities as graphically.</p><br><p>In Bayesian graphical models, nodes represent random variables while arrows are the information we are trying to send between the nodes using Bayes' rule.</p>      </span>    </div>  </div></body></html>