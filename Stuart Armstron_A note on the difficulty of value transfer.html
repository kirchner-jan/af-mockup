<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        A note on the difficulty of value transfer      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Stuart Armstron</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>I'm pleased to announce the release of my paper (working title: Value Learning for Unknown Reward Functions) that attempts to solve the problem of constructing a utility function to value-learn another agent. In machine learning terminology, this paper would represent a "Value Learning" classifier.</p><br><p>I am posting it here in order to provide pointers and feedback about potential problems with this agenda, some of which might have been identified by researchers at CHAI and possibly others. Some of these are problems that we might be able to solve (such as wireheading and acausal attack) while others require the construction of new ideas or a new angle of attack.</p><br><p>If you're a researcher, please send me an email or comment below if you've seen any problems like these, ideas for potential solutions (as well as comments of the form "this seems pretty similar to that AI Alignment research area, here is my reasoning process"), and resources or pointers that you think would help address these issues. This will provide a good point of reference for future AI safety researchers.</p><br><p>I welcome all suggestions for how to promote this paper!</p><br><p>Abstract</p><br><p>This project aims to provide a framework for understanding the challenges that a learning agent might face in adapting its values to another, possibly alien agent. In particular, it is based on the observation that we don't usually expect learning (such as in machine learning or developmental psychology) to produce agents that are capable of adapting the values of another. In this light, this paper argues that human values can indeed be transferred, and that that transfer is required in order for this to also transfer the reward. This paper describes the general class of value learning problems, which can then be solved by either theoretical approaches or practical approaches. The main theoretical approach is to assume that the target agent's preferences are described by a value function over the world-modelled within the agent. The problem of transfer between different value functions is then considered. Finally, the paper considers some ways that this can be addressed.</p><br><p>Appendix</p><br><p><strong>Example 1</strong></p><br><p><em>The agent has two goals (R1 and R2) and may switch them around. R1 gives a high value to an outcome and R2 gives a high value if it has reached that outcome by the end of the round.</em></p><br><p><strong>Theorem 1</strong>: The agent will switch to a high value of R1 while preserving R2.</p><br><p><strong>Proof</strong>: An outcome is defined by a pair (C,R), where R is the current value function and C is the code for the current algorithm. Let U be a utility function (that is, a map from outcomes to real numbers). Let Ci be the possible code that the agent may follow. Let u=U(C,R) be the utility in any outcome (R,C). Let f(u) be the function mapping u onto [0,1] representing the fraction of outcomes that have high u. Let gi(C,R)=Ef(U(C, R)). There exists an algorithm that will maximize g1+g2 so at least half of all outcomes will have high U (and therefore high f(U)). Hence the agent will maximize R1. \xe2\x96\xa1</p>      </span>    </div>  </div></body></html>