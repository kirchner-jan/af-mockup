<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        On the Promotion of Safe and Socially Beneficial AI via Reward Engineering      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Ryan Care</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>What AI safety researchers have learned from conventional ethics</li>
</ul><br><ul>
<li>Problems with AI safety research as traditionally conceived</li>
</ul><br><ul>
<li>What are researchers hoping for?</li>
</ul><br><ul>
<li>Why does the field seem so small?</li>
</ul><br><ul>
<li>How to change the field?</li>
</ul><br><p>The AI field is in a situation much like the early field of psychology. As the field of psychology was established, different schools of thought were given a lot to play with. The dominant school had no problems with what other people were doing, and that's not entirely fair\xe2\x80\x94others had problems with what the dominant school was doing. But the mainstream eventually formed out of those two schools, and has stayed so ever since.</p><br><p>When AI is introduced to the mainstream, researchers may worry that other subfields of science might end up dominant, due to being the fields that initially got the most funding and credibility. (This is an issue, discussed by Yudkowsky, for the mainstreaming of AI in general.) The dominant school in AI has no problems with the subfield of AI safety as a whole, and it will almost certainly continue to do so. But some subfields of AI safety are worried about what the broader field might become, and have set up their own submovements, just as psychologists before them.</p><br><p>In my experience, the most prominent subfield of AI is ethics, or more precisely, the subfield of ethics of AI. Researchers in AI ethics worry about what other AI researchers will come up with, just like how other psychologists worry about what other psychologists are doing. Both groups complain about "conclusions for ethical systems that I don't think they intended"... even though both groups are in agreement that these conclusions should be used! I also think both groups have a way to learn from one another.</p><br><p>What AI safety researchers have <em>learned</em> from <strong>conventional</strong> <strong>ethics</strong></p><br><p>The leading-edge school of philosophy of AI doesn't like anything that looks like it could be used by anyone else. They prefer something that seems so completely unique that no one would ever believe that it could _imply _anything. If any school in analytic philosophy seems to have a lot of respect for safety, it is this one that seems to have a huge amount of respect for AI safety. I'm not going to say all the relevant names here so you have to do your own research, but try reading these people: Yudkowsky's post on Timeless Decision Theory, a couple of comments by Stuart Armstrong and Jessica Taylor on that post, Nick Bostrom's Superintelligence, Daniel Dennet's What-If Tool, and Yudkowskys's Artificial Intelligence as a Positive and Negative Factor in Global Risk. Also check out Bostrom's brief presentation at the Future of Humanity Institute.</p><br><p>Even Bostrom's presentation\xe2\x80\x94which is by far the closest thing to a public explanation of how to go about this sort of research that I have ever heard\xe2\x80\x94is very technical. Most of it is not really written for a general audience. This makes AI safety a nice target for mainstream ethics' concern with being too technical.</p><br><p><em>All</em> of the above authors are sympathetic to mainstream research in AI ethics, which is very important, and probably wouldn't even be popular except for the AI-safety camp. As mentioned above, though, some members of AI-safety research also believe mainstream research is leading us _down _a dead end. They are right, because most mainstream research is. But the fact that AI-safety research wants to make sure its own work continues to get attention is a sign that it's making progress.</p><br><p>Problems with AI safety researchAs it stands, mainstream ethics is basically trying to figure out how to _stop _AI research. That's fine, because the problem is only starting. Even if some AI researchers are _wrong _about AI, it's _inevitable _that some of them would be right about AI! They're just starting out, after all. And it would be _crazy _to try to _disqualify _people from the fields of AI just based on the opinion of a few academics, even though most academics are pretty sure AI is a pretty bad idea after all.</p><br><p>But there is a _lot _of work to do! These fields need the help of everyone.</p><br><p>AI safety researchers should probably _not _be focusing their attention on AI issues alone. There are plenty of non-AI-safety issues where academics have developed deep expertise, and could help a lot with getting this whole thing started (such as how to develop human brains). But there are some problems that are so AI-specific that they would, I expect, attract a lot of attention from the AI-safety community even if that wasn't a core part of the project. (My sense is that this is the case right now.)</p><br><p>AI safety researchers will need to think in terms of "AI design" and "AI deployment" instead of "toy problem" and "hard problem". If you do it the right way, this is more or less okay, because you're learning from both mainstream ethics and AI-safety-camp research, both of which teach a <em>lot</em> about both problems and solutions.</p><br><p>Why does the field seem... so small?</p><br><p>"There's only so much brain you can fit in your bathtub: either shrink it or expand its bathtub. You are the brain-engineered species, not the bathtub-engineering species."</p><br><ul>
<li>Scott Alexander, Is There a Sense That Most AI Researchers Are Being Mired in Inadequate Equilibria?</li>
</ul><br><p><em>You are the brain-engineering species, not the brain-designer species.</em></p><br><p>I don't think it's totally unfair to say that the world would be a better place if the AI-safety-ladden mainstream would <em>actually let it be known that they care</em> that the mainstream is doing any work relevant to AI safety at all, and don't just use it as a cover for their own agendas. This field seems small, not just <em>because _of its relative independence from mainstream science, but also due to the fact that the field _doesn't seem to have a high degree of funding</em>. _</p><br><p>_I have seen multiple people in the field of AI safety research who thought they were doing important but neglected work just looking for grants, who ended up finding work elsewhere even though they were excited about their work. _</p><br><p>People in the AI-safety field would be well advised to learn from psychologists, and get more feedback and evaluation procedures going. There _is _a lack of high-quality feedback and evaluation going on, but that's not a fatal problem in this case, because the problem doesn't actually lie in a low-level lack of feedback/\xe2\x80\x8bevaluation, but in a lack of feedback/\xe2\x80\x8bcritique from mainstream researchers.</p><br><p>How to change the fieldThis is all a matter of degree. The field of AI safety wouldn't be so small <em>now _if there were no research going on from mainstream academia, or even if the field was too small to make it into the textbooks. (Some are about to publish chapters on the problem of AI-safety, in fact.) Unfortunately, there is a large amount of research being produced by mainstream academia that _isn't even relevant to AI safety</em>.</p><br><p>The obvious first step in changing the field is for people who care about safety/\xe2\x80\x8balignment to stop pursuing mainstream ethics so completely. A lot of the rest of this article will be spent trying to argue on behalf of safety as a research program. Unfortunately, <em>I don't see how you do that without</em> using mainstream ethics a lot, so I'm going to mostly ignore that part.</p><br><p>A second way to change the field is to push for a shift toward safety as a research focus in the field of mainstream AI. This probably requires a lot of "getting more AI-safety-relevant feedback from academia..." work.</p><br><p>Third, and probably the most powerful tool at our disposal (due to it being what we invented this whole field of study <em>to begin with</em> ), is to write more. More <em>research-relevant</em> <strong>brief explanations</strong> from academic researchers of what's up with AI safety research, even the stuff on mainstream AI ethics and so on. Less abstract, higher-level summaries from thinkers in the field that have made progress\xe2\x80\x94like Bostrom or Yudkowsky or Hanson or Dittmers\xe2\x80\x94who know what they're talking about better than I do, and can communicate what they have learned. A lot of mainstream research is not on the level of <em>e.g.</em> Armstrong's research. </p><br><p>There may be a way to use these summaries to teach mainstream researchers stuff that they don't yet know, or that is probably already known to people who know how to do AI safety research from the start. This seems to be the case with Stuart Armstrong, as I have mentioned elsewhere.</p>      </span>    </div>  </div></body></html>