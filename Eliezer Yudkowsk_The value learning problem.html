<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        The value learning problem      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Eliezer Yudkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><em>(This essay originally appeared in 2008, edited slightly, and uploaded to the __Less Wrong__ __Archive in 2012.)</em></p><br><p>The value learning problem, or the value learning problem, is this: how can a human mind learn about values?</p><br><p>In other words: how do you define a "value"?</p><br><p>The original problem was posed in a slightly different context. I thought it was interesting enough to deserve a separate name since it's slightly different.</p><br><p>In the case of natural language, you can say what you mean without fully spelling it out. The problem comes in when you are learning an artificial language\xe2\x80\x94one which lacks obvious and simple grammar, and has no obvious starting vocabulary. The question is not, "How can I learn this new grammar-less or vocabulary-less language?", but "How can I tell if I'm learning the right grammar-less or new vocabulary-less language?" </p><br><p>You can start from the answer "I see other humans speaking the language, so I'll guess that these are human values." </p><br><p>But is it safe to guess that way? This is the value learning problem. How do we define the term "human values"\xe2\x80\x94or, for that matter, the term "human"? If a human walks on the moon, they may be an Earth human even if they did not leave Earth and live in the moon. There are many different ways you could draw a line from human ancestors on Earth to humans on the moon. Some of these definitions of "human" use our understanding of where the human species evolved _on Earth _as the line of demarcation. So the human species on Earth has a different definition of "human" than the human species on the moon\xe2\x80\x94and yet they are, in fact, different species, and they have different values. </p><br><p>This is the value learning issue. How do you define a certain pattern of configurations of atoms/\xe2\x80\x8bparticles as "human values"?</p><br><p>And this is where the original problem and a related one form a crux; and as well as the original problem do I believe that the related problem is also, ultimately, important in its own right. Because, <em>yes</em>, once you've started defining the human species by using our present understanding of what constitutes human anatomy and physiology, our current understanding of neuroscience and DNA, that means that you'll also be using it as a _line in the sand _to define morality.</p><br><p>Why do we care about the related problem about the line on the moon? Well, this is the origin of the moral relativist /\xe2\x80\x8b naturalist who, being a moral relativist, says, "Morality changes from one time to another", even though the moon is the same place right where you stand.</p><br><p>What exactly is this moral relativism that leads one to say things like this? One way you could answer is, "What the <em>hell,</em> this is <em>the Moon?</em> Morality is different when you're up there, because you know you're separate from the ground." That is, by <em>describing</em> our values in terms of our <em>present understanding of anatomy and physiology,</em> we're also <em>including in our definition</em> the current notion that the human species on earth has morality and the human species on moon does not.</p><br><p>And this seems <em>troubling</em>. It's <em>concerning</em>; this has the flavor of <em>believing that the world is strange</em> because you use a present understanding of the world as your definition. Even as you say, "I use my present understanding of anatomy and physiological processes."  And the moral relativists go, "But you're leaving out these <em>other</em> people who think the moon's a hokey place without any atmosphere!"</p><br><p>But the world is, as the saying goes, mad. But then you could go, "Yes, look, on the surface, the moon has just the same rock as down here, and is just like the ground, but under the surface it has a different atmosphere, which means that it has a different morality." </p><br><p>Well, <em>in a way</em> it does have a <em>different</em> morality. But there <em>is, in fact,</em> a difference, because the value of breathing (or, the value of human life) is <em>different,</em> though both values exist on the same map of physics.</p><br><p>So yes, this whole idea has the flavor of a world <em>that's in a bit of trouble.</em></p><br><p>You could go further. You could say, "It's <em>not</em> a different <em>morality</em>, but the fact that there's a difference at all shows a flaw in this very notion of'morality'."</p><br><p>And a lot of moral philosophers say things which imply that they've discovered an inconsistency at the heart of their notion of morals, like the view that two murderers can't be said to have the same value as each other.</p><br><p>But the <em>whole point</em> of human values is that <em>humans don't know</em> their values\xe2\x80\x94they're not omniscient judges of morality, they don't fully know all the consequences of their action\xe2\x80\x94so that what's important to them in a situation can be quite different from what they would explicitly endorse off-the-cuff. Thus the notion that "morality is what people actually <em>want.</em>" </p><br><p>All of the proposed solutions to the value learning problem are based on the assumption that we can infer human values by what humans say they value.</p><br><p>A classic problem of AI design is trying to create an AI that acts in accord with the wishes of the programmers\xe2\x80\x94</p><br><p>Well, suppose that we're trying to create an AGI that acts in accord all of humanity's values!</p><br><p>How do we know what human values are if we know <em>nothing</em>? How can we be certain to have correctly inferred human values, if we don't even know what we want?</p><br><p>The answer to this classic problem is "I'll learn human values by induction." </p><br><p>This answer is part of the value learning problem itself. But now that you have an AGI that infers what we want by induction, how do you program it <em>anyway?</em> Where in the universe do you put "don't execute a treacherous turn"?</p><br><p>The "infers what we want" solution seems like a bad one. The idea being, "A human would do X if (and only if) X is useful for them according to their values."</p><br><p>The other way is just to try to "do what humans tell us." This is the solution we used in the old days, when it was more popular to run on human physics, and humans were smarter than machines\xe2\x80\x94not because they were inherently better, but because they knew more; we could ask them more detailed questions, and they would be able to answer.</p><br><p>With today's AI, we're starting to see that it will be possible for the AI to "infer what we want" by running on an architecture that is actually wiser than us, and yet we can't say\xe2\x80\x94it seems to us that we are wiser. Just like we can't say, "My present understanding of physics shows that there's a different morality between the surface and the atmosphere." We just know that there is.</p><br><p>Do we have any recourse, here?</p><br><p>The classic "what human could do this?" reply to this situation is that it is only a <em>trivial</em> question, since everything is ultimately determined by the laws of physics\xe2\x80\x94but you can't answer it by brute force, because it's very hard to write down what are and are not lawful rules for every possible action or possible physical state.</p><br><p>So the only way that we can answer the question at all is <em>by induction.</em> We can come up with various rules, which we observe humans satisfying, and we <em>induce</em> further rules from the rules of induction.</p><br><p>We find that human preferences <em>do not satisfy</em> a simple, elegant, intuitive rule with no exceptions: "Do what your fellow humans want."  As for "infers what human want <em>based on</em> our behavior," the notion of "infers" leaves all the work out to humans, and we know from history that humans will not have good reason to use this principle to infer a higher morality. Asking "What would a human do in my situation?" is just so much less effort that you don't even bother asking it.</p><br><p>And what are we left with if not induction? Well, the classic approach to learning human values is <em>instrumental convergence</em>\xe2\x80\x94human behavior tends to be <em>reproductive,</em> because it helps promote survival and/\xe2\x80\x8bor avoid punishment; <em>signaling,</em> because it sends a message to other people about the human's value system; and <em>paternalistic,</em> because it's designed to help out the human.</p><br><p>This is as near as we can come as a group, I expect. There are lots of subtle variations within these notions of "reproductive" and "signaling" and "paternalistic". But the point is that you can't really do this <em>except _by imagining a hypothetical human and drawing on whatever generalizations you can from the example of that hypothetical human. You can't do this except by trying to _generalize from individuals.</em></p><br><p>You can't infer human values just by looking at the laws of physics, because the laws of physics don't care about moral philosophy. You could even say, "Look at the laws of physical chemistry", meaning that we're using "physical chemistry" as a rough approximation to talk about morality. </p>      </span>    </div>  </div></body></html>