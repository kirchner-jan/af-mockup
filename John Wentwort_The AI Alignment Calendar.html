<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        The AI Alignment Calendar      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">John Wentwort</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>The calendar</li>
</ul><br><ul>
<li>What is "time" in the model?</li>
</ul><br><ul>
<li>Predictions &amp; time-line plots</li>
</ul><br><ul>
<li>The AI Alignment Research Agenda</li>
</ul><br><ul>
<li>The next 6 months</li>
</ul><br><p>The AI Alignment Research Calendar is an attempt to consolidate different timelines and research agendas/\xe2\x80\x8btopics by putting them through a single unified time-line, showing when we will see different pieces in the final picture. The model is currently in draft form, but you can see the time-line plots so far here. You can also find the embedded version of these plots here, so you can make your own predictions in the comments (although please don't do that in the comments on my drafts). It's a bit of work to display a lot of things at a high resolution on a webpage, so I'd appreciate help. The code is available on github and you can try running it there to see how it looks like.</p><br><p>The calendar</p><br><p>I've divided the research calendar into "technical agendas" (Ie, pieces of research that are about the actual alignment process), "direct effects", "indirect effects" (which include both ideas and "epistemic state" in the AI Alignment research community), and other categories that should be able to be added to the wiki.</p><br><p><strong>What is "time"</strong> in the model?I have used a clock to represent the process of AI development: each tick is one year. I've also put a different colored line on the graph for each "technical agenda", since I expect their "time-to-completion" to be different.</p><br><p>__Predictions &amp; time-frame __predictions are things I've made about the timeline and what people would do if they saw the model. However time-frame predictions are more likely to be over- or under-optimistic/\xe2\x80\x8bpessimistic (although they do at least show the shape of the curve we should be expecting)</p><br><p>__The AI Alignment __research agenda represents a list of things I expect to see or "happen" in the time-frame. It is the same for both MIRI-like and OpenAI.</p><br><p><strong>The next 6 months</strong> are six months from now, representing the next few months of this research agenda/\xe2\x80\x8bdevelopment.</p><br><p>The <strong>technical agenda</strong> are different things, but their "time to completion" is given by:</p><br><ul>
<li>Superintelligence: _The first detailed work on how to align a complex algorithm with our values. _Most likely to be between 2020-2040.</li>
</ul><br><ul>
<li>Consequentialist agents: _The first practical algorithm that reasons using consequentialist goals, rather than deontologist goals or agent utility functions with a linear loss function. _Most likely before 2030.</li>
</ul><br><ul>
<li>Instrumental convergence: <em>A convergent instrumental goal for agents using complex decision theory</em> Most likely around 2065-2090.</li>
</ul><br><ul>
<li>Goal systems: _A complete formalization of the study of values that allows us to precisely specify what a good outcome looks like. The study includes the analysis of complex goals, the construction of goals from scratch, and various methods of analyzing the internal "state" and "reward" of goals. _Most likely around 2031-2070.</li>
</ul><br><ul>
<li>Whole brain emulation: _A practical method for making detailed copies of organisms that is good enough to allow us to "play around" with their properties, and potentially perform complex science experiments on them. _If WBE gets cheaper it should happen around 2059, if not for other technical reasons I expect it will happen significantly later.</li>
</ul><br><p>The next section goes through these by showing the time-line for each. Note that you can use the embedded version of the plot on this post (code) to see them at higher resolution. This would also mean that if someone wanted to run their own research agenda on OpenAI or MIRI's research agenda, they can simply copy/\xe2\x80\x8bpaste these.</p><br><p>What is "time", and how do you calculate it?</p><br><p>Predictions &amp; time frame predictions</p><br><p>What about "time-frame" predictions though? My main source for making such predictions is when and where I think the various research agendas will happen or have happened. Since these predictions are often very over-/\xe2\x80\x8bunder-optimistic/\xe2\x80\x8bmoderately pessimistic compared to the median AI Alignment or ML-AI forecasters/\xe2\x80\x8bcommentators, I tend to be conservative in my predictions here by either putting a 10% or 20% "troll" probability on seeing a research agenda come true or not.</p><br><p>If "time-to-" happens quickly enough, I also tend to forecast it to happen "soon", based on the idea that if you "time horizon" it doesn't really affect whether it's "early" or "late" anymore.</p><br><p>I'm using "time horizon" to mean what happens, and whether or not we see it by some kind of standard timeline. For example, I do "time horizon 1" for AGI before 2030, and "time horizon 2" for AGI after 2030, because I think these will be the first two AI-related things that people will seriously be worried about, which I think will move the work in these areas from mainly being theoretical research to more of being engineering and "direct" work.</p><br><p>I like to think of these "time horizons" as "bets", since they try to make a point about "when" something will happen, as opposed to saying "it will happen by this year", which is "when".</p><br><p><strong>Prediction</strong>:</p><br><ul>
<li>AI safety camp: _Started before or after 2016?  _I think it started in 2015, but I don't know if it has published anything by now?</li>
</ul><br><ul>
<li>Agent foundations in the MIRI/\xe2\x80\x8bCEA/\xe2\x80\x8bOpenAI way:  _Most likely before or after 2021?  _The most "MIRI-like" thing happened in 2016, and the most OpenAI-like thing happened in 2018. Note that both pieces are basically the same.</li>
</ul><br><ul>
<li>Agent foundations from first principles:  <em>Mostly the same as the last one, but more narrowly defined</em> See above\xe2\x80\x94I think the last one was just the same as this, but without the word "direct".</li>
</ul><br><p>Of the three first two, I think both are relatively close together in time. However, I also think the last two have the most variability in time-to-completeness. Note that this is from a single author's perspective, and things may well be even less well defined for various reasons. This could happen if the area is "bigger" than expected (like by going into agent foundations from first principles), and so has a lot of different projects. I can't say with any certainty how long anything of this variety will take, but if you look at the timeseries for MIRI's agent foundations page, you can see that there aren't any big jumps in time-till-completeness from the MIRI-esque/\xe2\x80\x8bOpenAI-esque projects to non-MIRI-/\xe2\x80\x8bOpenAI-like projects. However AI safety camp and research done by OpenAI tend to have fairly high time-to-complete. I predict this due to the fact that the OpenAI or Merel projects are the ones I expect to be "highest impact projects" overall.</p><br><p>The AI Al Alignment Research Agenda</p><br><p>The first two sections of the research agendas above are broken up into smaller sections just below, based on what they are. The main categories are:</p><br><p><strong>Technical agendas:</strong></p><br><ul>
<li>Consequentalist agents:  _The first full description of an algorithm that uses consequentialist goals, with a complete analysis of what it means for instrumental convergence and for a set of goal-content. _Possible to do by around 2030.</li>
</ul><br><ul>
<li>Goal-content:  <em>A complete mathematical formalism for reasoning with a utility function with a linear loss.</em>  Possible to do by 2052.</li>
</ul><br><p><strong>Direct effects:</strong></p><br><ul>
<li>AI Safety Camp: <em>Started at various times by various people, but one of the better defined projects was the AI safety camp at EA Global 2015. This gives us an estimate for around 2020 or near thereabouts.</em> Possible to "get into" 2015-2020.</li>
</ul><br><ul>
<li>Agent Foundations from first Principles:  <em>Possible by 2021 based on what I've seen and don't see a reason why it shouldn't be possible. See above for "Time horizon".</em></li>
</ul><br><ul>
<li>Agent Foundational Theory:  <em>Possibly this will just be the next set of posts/\xe2\x80\x8bchapters in the Alignment Newsletter. Probably written by Eliezer and co-author(s) but not sure.</em> Possible to "start" in 2020.</li>
</ul><br><p><strong>Indirect effects:</strong></p><br><ul>
<li>AI Alarm/\xe2\x80\x8bAction: <em>Possible by 2051 based on what we know now. I also guess at some of the details here but it could be quite a lot of work before we see anything new on that list.</em></li>
</ul><br><p>Possible to "start" as long as they exist at the time.</p><br><ul>
<li>AI Alignment as an __Open problem in Computer Science:  <em>Probably not until 2036/\xe2\x80\x8b2037.</em></li>
</ul><br><p><strong>Instrumental convergence:</strong></p><br><ul>
<li>Instrumental Convergence in AGI:  <em>Possibilities: See above?</em> Likely to be useful for various forecasting/\xe2\x80\x8bpredictions of AGI timelines and timelines to AGI.</li>
</ul><br><ul>
<li>Convergent Instrumental Goals in AI:  <em>Predictably do useful things.</em> Possible to start around 2060.</li>
</ul>      </span>    </div>  </div></body></html>