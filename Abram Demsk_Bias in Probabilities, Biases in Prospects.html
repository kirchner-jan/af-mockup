<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Bias in Probabilities, Biases in Prospects      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Abram Demsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><em>My impression is that there may be a good argument for using only true facts in estimating the probability of outcomes. I find this argument kind of compelling, and am posting it here again in case the post can be improved, or refuted.</em></p><br><br><p>Intuition says: [1] Biases in Probability are okay, Biodies in Prospects are not.</p><br><p>You're already biased, it makes no sense to add extra information. But let's look at a few cases, to investigate:</p><br><ol>
<li>You're flipping a coin. I give you a coin where the probability of heads is 0.5 and the probability of tails is 0.5. Should you be more or less confident in your estimate of heads or tails, as a result of this?</li>
</ol><br><p>The answer is, in fact,'more'. This is pretty easy to justify: The probability of heads is the proportion of the time that the coin is heads to the time that it's tails. Adding information changes this. In particular, if I tell you the coin is tails, then your estimate of heads is now <em>exactly</em> as likely as tails. This makes your estimate of heads 0.25 and your estimate of tails 0.25, which is more information than you started with, and hence makes it'more likely'.</p><br><p>You can also say something like: "What if you knew the coin was not biased?" The answer is "I'm going to assume the coin is biased, because this was true historically and I don't already expect that it's not biased, so this new information is relevant only by default and there's not much point in updating."</p><br><ol>
<li>Let's say I roll a twelve sided dice 20 times and count. The probability of rolling at least two sixes is 0.0435, about 12.5%.  It's clear that we shouldn't expect to see "at least two sixes" far more often in this die roll than "at most one six", because almost all of the die rolls are "two sixes" or "no sixes", so in expectation, it should be <em>really rare</em> to get a "three sixes result".</li>
</ol><br><p>Suppose that I tell you this probability.</p><br><p>Your prior estimate that this probability was, in fact, 12.2%.</p><br><p>Should you be more or more likely to expect any given result, now?</p><br><p>Suppose I tell you that this probability is 0.4%.</p><br><p>How should you be <em>expected</em> to behave, now that you have this information?</p><br><p>The argument for updating toward "twelve" only makes sense if we haven't noticed "at least two 6s" very often so far, or if it makes sense to assume that the bias is in the 6-sided die and not in the 10-sided die. What if I tell you that it is 0.5% with a 10-sided die, and 0.05% with a 6-sided die?</p><br><p>If you aren't getting the number of sixes per roll as exactly right as you think, and this changes your expectations about future rolls, you might be underestimating the probabilities we've seen so far, even if you update with the same frequency in response to new information.</p><br><p>More generally, the way I would think about it is as follows:</p><br><p>You can get really wrong about the probability of seeing some event, without being strongly or systematically biased about probabilities in general. Biases in particular can come from not counting every possibility; they could affect the order of operations rather than just the proportion. For example, when I tell you that the coin is biased you might initially think to yourself "if the coin is biased against tails, then it's less likely to be tails than if the coin is unbiased. I should update more toward heads than I would have otherwise, in expectation.</p><br><p>Forcing yourself to take a biased die roll as much more likely, than a nonbiased die, is probably not actually an accurate way of thinking about the problem, since you can make such errors in either direction. You can also notice your <em>failure</em> to notice that the bias comes from the 10-sided, rather than 6-sided die, when you take the biased die result, and then you update. And so on.</p><br><p><strong>3.</strong> Let's say we're talking about some complicated situation we aren't experienced in. Someone gives you a fair coin that produces a biased result only half of the time, and asks you to predict the result. They flip the coin; if you were going to make an estimate, how confident do you expect to be, in the future? I don't think I'd expect my confidence to change very much, and my best guess is that a fair coin should be equally likely to have heads or tails, so my estimated probability should be 0.5.</p><br><p>Suppose we tell you that the coins are weighted so that they give heads more often and tails less often. Should you update away from 0.5, to an expectation lower than 0.5?</p><br><p>Supposing the two-sided coin weight is very small, this might do it, since you can imagine that one of the weighted coins is always right, and so you expect to be in the world where both halves of the coin are fair. But it's likely the weight is too large for this to work properly.</p><br><p><strong>4.</strong> I don't believe in Pascal Mugging.</p><br><p>Supposing I tell you "You probably still believe in Pascal Mugnging after learning this: Your average friend weighs more than you, on average, your average relative, in proportion to your average friend, is wealthier on average than your other friends (or the people you would replace them with), and so on, and this continues indefinitely."</p><br><p>Do I think my relative weight in a group or population will change dramatically? I see no reason to think that it's more likely to be greater than it was before learning this, or that there's a limit to how much it can be.</p><br><p>Supposed I was planning on an event and needed to figure out how small to make the group. If you told me that I'd have to halve it, I didn't know whether my plan would work; I hadn't planned on needing half the group but had counted on the event working with a group of size 7-10. Now I'd notice that I should only have counted the group with a size of 1-6. But if I told <em>you</em> that, <em>I'd</em> see this and figure out that I needed to halve the number. We wouldn't have any common model of how this all worked but we'd <em>both</em> be going around thinking "it's more likely that my plan will work."</p>      </span>    </div>  </div></body></html>