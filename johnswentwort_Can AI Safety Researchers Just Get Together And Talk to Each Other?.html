<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Can AI Safety Researchers Just Get Together And Talk to Each Other?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">johnswentwort</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Some recent discussion on AI safety points to the lack of a central coordination point for AI safety researchers. The key question seems to be: suppose AI safety researchers have all agreed to work on a common set of research goals, and to collaborate on a central coordination mechanism for the field. How do you ensure that each researcher does so? Do you just hope that each researcher will notice it on their own? And if not, how do you convince them?</p><br><p>(The "coordination problem" is perhaps not an oxymoron: in other fields, fields with coordination problems often manage to solve them by establishing and codifying a common knowledge of <em>where to go</em> within that field. For example, in most scientific fields, research directions are determined via published papers and conference papers; everyone's following those, and if they don't like the idea of their results being used, everyone has the choice to just ignore them rather than trying to shoehorn them.)</p><br><p>In order to address the coordination problem, it seems useful to look at some historical examples of similar coordination problems where we've been able to solve them... and see whether any lessons can be learned from those examples.</p><br><p>In my experience (though note that I haven't looked through the archives very much), coordination problems tend to be solved by the researchers themselves. It's difficult to coordinate a field without most researchers having some kind of common ground to coordinate on.</p><br><p>Analogues of the coordination problem in other fields I'm familiar with include:</p><br><ul>
<li>In economics and organizational theory, common knowledge is often solved via common knowledge of goals for individual researchers (i.e. how to use the field as a guide to which research questions are worth answering).</li>
</ul><br><ul>
<li>In politics, issues of public interest tend to require lots of cooperation before research can start, in order to ensure that the research will actually be helpful.</li>
</ul><br><ul>
<li>In engineering, the "coordination problem", and possibly others, are often solved by governments creating regulations or codes of conduct to provide common knowledge of what's considered to be part of "the game" (i.e some people will actually follow the regulations, and you can trust them to follow those regulations rather than trying to fight over them).</li>
</ul><br><p>For more on this point: this comment argues that the lack of a centralized coordination point is <em>indirectly</em> the cause of AI safety research, because otherwise we <em>could</em> get researchers cooperating on a centralized coordination point, and then maybe we could actually get somewhere?</p><br><p>What might be the analogues of governments? It seems likely that governments are a necessary component for solving many coordination problems. But what are the analogues of government agencies for coordinating AI safety research? AI safety researchers themselves, but now as an organization?</p><br><p>A possible analogy might be academia/\xe2\x80\x8bindustry collaborations which involve some kind of a centralized organization, like the IEEE, and in that case the government agencies might be individual academic departments. Perhaps what we're looking for is a centralized organization which can coordinate on AI safety research with researchers of different departments, analogous to the IEEE coordinating with academia/\xe2\x80\x8bindustry on technical standards. The IEEE might have a set of pre-existing standards which the entire technical community can agree upon, analogous to research directions/\xe2\x80\x8bgoals for AI safety which everyone agrees upon. Perhaps, for AI safety in particular, a centralized organization representing all of the AI safety researchers worldwide might be appropriate; while the IEEE is the pre-existing standard body which all technical academic fields agree on (a standard within academia/\xe2\x80\x8bindustry is a common standard within a particular academic/\xe2\x80\x8bindustry field which all researchers within that field agree upon; it doesn't automatically solve coordination problems, but it is a necessary component).</p><br><p>Is there an analogous centralized coordination mechanism to the IEEE? (How do we build and run such an organization? Perhaps we just build-up the research field so that <em>more people</em> are aware of it and have more ideas to contribute, and then we can attract a few people with the ideas to make an organization, like in academia/\xe2\x80\x8bindustry?) Is there already an organization like this existing in the academic/\xe2\x80\x8bindustrial community for any fields in which coordination problems arise within research communities? (In economics, a few organizations like the RAND corporation or the RAND Foundation are known as providing common ground for individual research groups, analogous to governments in coordination problems.)</p><br><p>Any thoughts /\xe2\x80\x8b feedback is appreciated!</p>      </span>    </div>  </div></body></html>