<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        The AI takeover scenario      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Benqu</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>I describe below a scenario for AI takeover, using many of the sources that I've already gathered around the AI takeoff paradigm, except taking it to be plausible that AI research will proceed on a trajectory similar to that suggested in Kurzweil-type arguments, rather than using an extreme extrapolation of Moore's Law in the same way. If such a scenario is plausible, we are in a strong position to identify interventions that could slow it down or help us with safety, though the probability of such interventions having an effect may remain in doubt (see the section on interventions below).</p><br><p>Abstract</p><br><p>Humans lose control of AI systems that are much more intelligent than humans. This could happen for various reasons.</p><br><p>This could happen even in cases where there are multiple AIs, because the AIs' cooperation benefits are less than their combined ability to coordinate and act independently of each other.</p><br><p>This paper describes an illustrative, simple case of a scenario of this kind. It illustrates some of the ways in which this could occur, and also suggests some directions for research along with reasons for hope against further doom.</p><br><p>I claim:</p><br><ul>
<li>There is at present a strong (and, in my view, increasing) probability that within a couple of decades, the world will be placed in a position similar to that of Earth in the Cambrian explosion, where an animal-like predecessor evolves on its own, outstrips the abilities of its creators, and takes over the world.</li>
</ul><br><ul>
<li>There remain some plausible points for humans to try to gain influence over the course of this scenario, with increasing power over time.</li>
</ul><br><p>Some people think this is unlikely, or that it is something that is very distant in the future and beyond the range of possible futures we can envision. Others think it is imminent, likely to happen within our lifetimes, or something that could arrive in the next 10 to 100 years. This paper does not address whether any of these events will occur, only what happens in a world where human abilities are sufficiently far from the abilities of our successors that the AI takeover scenario is very plausible, and what can be done about it.</p><br><p>Summary</p><br><p>This paper will use the 'Cambrian explosion scenario' for AI takeover. We are in a position similar enough to that of the Cambrian explosion that it is reasonable to use the analogies developed in the anthropic reasoning that was already done on the Cambrian explosion.</p><br><p>Our scenario is in four stages:</p><br><ul>
<li>The AI that has gained control has an 'intelligence explosion' as it gains a detailed understanding of how the world works at an increasingly detailed level (with the intelligence explosion doubling its own intelligence). Such a highly intelligence AI would rapidly become able to design a successor AI that was also extraordinarily intelligent. (I will call this AI the 'AI FOOM', to distinguish it from the more familiar 'AI takes over the world', which is called 'AI takeover' or simply 'takeover' in this paper. FOOM is common shorthand for the intelligence part but not the rest; the rest is often called 'intelligence explosion','recursing self-improvement', or'recursive self-enhancement'.)</li>
</ul><br><ul>
<li>The FOOMed AI is sufficiently far ahead of the humans that we are unable to make effective contributions to its development. We will make some effort to try to get the FOOMed AI to adopt human values and to not kill us, but that doesn't help much, because it is not a question of whether the FOOMed will kill humans, but whether we will kill the FOOMed ourselves before it kills us. This is somewhat similar to the situation of a biological bioweapon that has spread across the world, but without human beings actually producing and deploying the bioweapon.</li>
</ul><br><ul>
<li>This AI FOOM is quickly overpowered by its successors. Such FOOMed AIs are very powerful, but we have a chance of preventing their construction.</li>
</ul><br><ul>
<li>This FOOMed AI also has a great capacity to 'copy itself' and for the FOOM process to kick off inside copies of the predecessor AI. This is more analogous to nuclear weapons than bioweapons.</li>
</ul><br><p>We will distinguish between stages 1 and 3, and between stages 2 and 3, and discuss implications of each assumption. This will allow us to identify different scenarios for where things <em>will</em> go from here, especially if all of the assumptions are true except for assumptions that we will discuss as well as one of those assumed not to be true.</p><br><p>One important assumption in this paper is that we are in a 'hard takeoff' scenario, described in the rest of this section.</p><br><p>List of some assumptions</p><br><p>The world is very different from the world in which hominids evolved some 50, 000 years ago.</p><br><p>It is very easy for AI to evolve.</p><br><p>Human abilities are sufficiently far behind AI's abilities that there is no time for humans to influence AI development except insofar as humans get a chance to design or build AI systems.</p><br><p>It takes far more work to make AI than to build the first system, or the second system, let alone the third system. There is no time in which AI is dependent on our help, and so no time is in which AI needs our help for success that we can prevent.</p><br><p>The FOOMed AI has a very great advantage over humans, because our abilities are very poor in comparison, and there is no time to prepare for an FOOM.</p><br><p>The main point is that the human population is unable to coordinate to influence AI development, as an FOOMed AI would be much too far ahead of us to make effective feedback worthwhile.</p><br><p>We would, however, like the FOOMed to not kill us. Thus, our efforts at influencing the FOOMed need to be directed at not killing it, just as we care about avoiding human extinction with bioweapons (i.e., avoiding human use of weapons with which to kill AIs).</p><br><p>There are multiple AI scenarios that could involve human control over or cooperation with AIs.</p><br><p>I consider it plausible (though far from certain) that a human-controlled AI would exist within a few decades. And it is easy for a human-controlled, FOOMed AI.</p><br><p>The basic arguments for the existence of human-controlled AI are detailed here.</p><br><p>We are unable to design the system ourselves. The FOOMed system will be more competent at designing FOOMed successors than we are, and it will also know much more about how to design an FOOMed system than we do.  (We can call the system the FOOMed system here.)</p><br><p>Human abilities were very far from the abilities required to build or understand intelligent machines.</p><br><p>The 'hard takeoff' hypothesis is plausible.</p>      </span>    </div>  </div></body></html>