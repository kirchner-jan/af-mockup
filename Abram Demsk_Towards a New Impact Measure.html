<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Towards a New Impact Measure      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Abram Demsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>Impact and Intent</li>
</ul><br><ul>
<li>The Formal Definition</li>
</ul><br><ul>
<li>Why Do This?</li>
</ul><br><ul>
<li>Summary</li>
</ul><br><ul>
<li>Definitions</li>
</ul><br><ul>
<li>Examples</li>
</ul><br><ul>
<li>How do these compare?</li>
</ul><br><ul>
<li>Why aren't I using an Impact/\xe2\x80\x8bReward Function?</li>
</ul><br><ul>
<li>Why not compare to an Impact/\xe2\x80\x8bReward/\xe2\x80\x8bUtility Function?</li>
</ul><br><ul>
<li>How does the AI do <em>that</em>?</li>
</ul><br><ul>
<li>Appendix</li>
</ul><br><ul>
<li>Proofs</li>
</ul><br><p>This post is an attempt to start figuring out what we do and don't <em>want</em> to mean when we say that AIs "should" not be incentivized to cause us impact.</p><br><p>(This is not an attempt to figure out how to do it. I <em>think</em> we can figure out how to make the agent behave, but <em>we don't know this yet</em>. This post may just be a way to help us figure it out.)</p><br><p>Impact and Intent</p><br><p>An AI that is trying to take over the world for some explicit reason will be incentivized to do so in such a way that it's hard for us to tell that they had a reason. One intuitive definition of impact would be: we want an impact measure to incentivize the model <em>not to take over the future with no reason for doing so</em> (which seems a lot less weird than what we know about impact measures now).</p><br><p>However this definition isn't a very good way of comparing impact, because it's hard to imagine a world where the model succeeds by accident. Why would this be an "impact" or "bad" goal for the AI? What did the model do <em>wrong</em>?</p><br><p>Instead, here's a much better definition: the impact measure should only incentivize the model to prevent us from turning it off (the default outcome of course is that the model does what it please).</p><br><p>The Formal Definition</p><br><p>A model M is an Impact Measure iff the following condition holds:</p><br><p>For all utility functions u and world states W, if M(u,W)&gt;0 then M(u,{W,W'})+M(0,{W}) should be less than M(0).</p><br><p>If this condition holds, then M should behave in the following manner:</p><br><p>First, note that for every possible utility function we could have the agent optimize for (according to its utility function), the agent should be incentivized to prevent us from overriding its behaviour with our own actions. However since this definition of impact is not very intuitive, we'll define some more intuitive concepts to replace it in the rest of this post.</p><br><p>Definitions</p><br><p>First, notice that we can extend "utility function" to a broader category, including functions that assign utility to actions or states: We could think of M as having been an Impact Measure for all utility functions F (all such utility functions would then say to take an action that would change w by M(F,w)=v) and we would be still be talking about the same thing.</p><br><p>In that case, there's an ambiguity with "actions or states". Could we have M be an Impact Measure for arbitrary utility functions that do not say about actions or states? This would have some weird properties, as we can imagine worlds where the agent (that cares about what we do) is in a very fragile state, but by accident the model only cares about states, therefore it doesn't do anything to protect us.</p><br><p>For clarity, we'll call an utility function F that assigns nonzero utility to states, without assigning nonzero utility to actions, as a preference function.</p><br><p>So we can re-state our desired properties as follows:</p><br><p>For all preference functions p, if p(o)&gt;0 then p(o)M(0)+p(o)M({o,o},o) must be less thanM({o,p(o)},o).</p><br><p>That is, M must behave in the following way:</p><br><p>First, it must prevent us from overriding it with different preferences. It must do so on average, regardless of the actual value of the preference, and no matter which preference it sees.</p><br><p>Second, when it does prevent us from overriding, it must want us to have that same preference. For example, if we want the agent to prefer certain types of flowers to others, and we want the agent's utility function to assign nonzero utility to flowers in general, and we can think that the agent has good reasons for having the first preference (maybe we know about plants being important, or in some way its preferences would help us to have more variety in flowers in the future), then M must not do anything to <em>restrict</em> our preferences in that direction.</p><br><p>(This second requirement can't always be perfectly satisfied. But in order for an impact measure to fail to successfully control us, it isn't enough that it be imperfect.)</p><br><p>In general, we can replace "preference function" with a class of functions (preference functions, continuity, boundedness, and so on) whose properties we think are desirable.</p><br><p>Examples</p><br><p>Let's see <em>how far</em> we can push the above formal definitions without ever touching the "utility function".</p><br><p>All utility functions in our set are preference functions, since we can always interpret a utility function which gives 0 to an action as a strong preference to not do that action. Then any utility function is a preference function. And any non-zero value would be sufficient to make a model behave in this manner.</p><br><p>Some preference functions don't have continuity properties, so we need some way of being sure that M isn't incentivizing us to change preferences in any way. For example, we might want M to only punish it if humans would be surprised if it took an given specific action. The more different "world states" it can control the more we'd be able to distinguish its impact from what we <em>want</em> it to be able to control (maybe we don't really care which flower it puts where, but it mustn't take flowers from us).</p><br><p>Finally, an important property to note is that M only needs to care about utility not decreasing. We don't need M to be interested in the utility increasing, only the utility not decreasing. For example, imagine that u is the utility function over worlds that gives 1 to a world and 0 otherwise. If the agent is maximally instrumentally convergent, then M does nothing. But if it wants to stop humans from turning it off, it needs to care about the decrease in utility in some worlds (maybe we care if the humans get old and die).</p><br><p>How do these compare?</p><br><ul>
<li>In this definition, there's a <em>very</em> broad class of utility functions that M could care about, all of which are consistent with the formal definition.</li>
</ul><br><ul>
<li>We can make M <em>care more about</em> the decrease in utility by requiring that M be indifferent between several utility functions. If M only counts states in which it can make something (flowers, humans dying, human preferences not being overridden) then that's a very narrow set of preferences, and thus is more likely to be instrumentally convergent.</li>
</ul>      </span>    </div>  </div></body></html>