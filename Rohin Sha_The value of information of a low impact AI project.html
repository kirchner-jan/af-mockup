<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        The value of information of a low impact AI project      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Rohin Sha</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>I have this idea that I now realize is wrong, which I might write a good formalization of, but in the meantime, I'll present the idea with no formalization and maybe people will correct it and tell me what I'm doing wrong (or tell me what they think I'm doing wrong and maybe I'll change my mind). Here's the general idea.</p><br><p>Suppose that in order to achieve some ambitious goal, you build a powerful AI system which you do not want to use for unintended purposes, in order to ensure that it has a low impact on the world. You want this AI system to use human feedback and have high approval from the human judges. (We'll assume that the human judges are all altruistic because otherwise we wouldn't have a reason to do this).</p><br><p>So what should the AI do in this case? I think the right answer to this question is: we should ask the human judges if it's OK to use the AI for its intended goals, and if they accept, we build the AI, if they reject, we don't. I think I'm just confused, but if you can tell me exactly how we should formalize this, and what we should do if we reach that point, I'd be happy to look back at this post in a year and see where I went wrong. So this post is just an intuition pump.</p><br><p>By the way, this post is not about preventing malevolence with a low impact AI or otherwise, this post is about "what to do when you already have a low impact AI system, which you really really really really don't want to use."</p><br><ol>
<li>Let's first make concrete this idea of a low impact. "Low impact", in order to match what the AI system is actually doing, would involve a definition of what would be low impact from the humans' point of view. But we already have this definition in a sense: we would judge it to be low impact if we would want to accomplish the same ambitious goal in a low impact manner. Also, we want high approval from the humans. So here's a formalization of a low impact idea: </li>
</ol><br><p>Intuitive formalization 1: Take the high level description of what an AI system is trying to accomplish and define its impact as a function of this abstraction. </p><br><p>Consider the example of DeepMind's GPT-3. The high level idea is "given a few bits of input text, generate a few more bits of text based on that input." So this is low impact if we care only about the high level goal from the human's point of view: do not change the world in this way (e.g. do not make us miss their deadline to publish their work). </p><br><p>From this perspective, for example, GPT-3 would be low impact because it's taking bits from the internet and generating output in the same way, and it turns out that this description of GPT-3 is low impact in the sense of accomplishing the same goal in a low way, not in the sense that the goal hasn't been completed (not even the goal of the high-level description), so we can build the low impact version.</p><br><p>Intuitive formalizaton 2: Apply this to the description of the current low impact AI system in order to define the impact. </p><br><p>For example: In GPT-5, the current low impact description is something like "If an AI system is doing low impact action A, then output [x]. If an AI system does not do action A, then if a human says the AI's goals are achievement of goal [y], then output [x], else output [z]." And I suppose this is the high level goal: "given some bits of input text output some more bits of input text based on them or do some other bits." </p><br><ol>
<li>Next, in practice, we don't care about getting this ideal approval from the human about what an AI system should accomplish. In fact, we probably have to be more concrete and deal with the question of what actually is low impact. </li>
</ol><br><p>Let's consider two ways we could define impact: </p><br><p>I. The human judges are altruistic and care how their AI actions influence the world, and the AI system achieves a low level of impact if the human judges in aggregate care to do the same. </p><br><p>This is the definition of impact used by Stuart Armstrong and others for their proposals of counterfactuals and corrigibility. So here's the ideal version of what it would mean for the definition to match the actions that the AI system would actually take if it had the human level approval.</p><br><p>This definition would obviously not match the actions that a low impact AI would take, because in the world where the AI is high impact (due to having a big impact on the world in some way, even though low impact on humans), the impact would not be low impact from a human point of view. For example, humans will try to kill this AI system if they recognize it as a threat to them.</p><br><p><strong>Intuition</strong>: This definition of impact would incentivize AI not to do many things we don't want it to do, the way we don't want them to not do many things. (It's actually more than just incentivizing, since the low level of impact would need to equal the AI system wanting to do something else for some other reason). </p><br><p><strong>I don't have a very good formalization</strong>: I can imagine some intuitive formalizations of this being something like: </p><br><p>IF we do "a", then at most "b" happens (for some b: the sum of the impact of each action is less than the impact of the action, or some such thing like that) </p><br><p>IF we don't do "a" then with probability at least p, "b" happens </p><br><p>IE, the impact from not doing action a at least p of the time is at most b of the time, or something to that effect. </p><br><p>IF we care about "z = the output of the human judges, else b" then this will be true because we only care about what happens if we don't do the thing that the AI system will do anyway.</p><br><p><strong>I think this is clearly the wrong definition:</strong></p><br><ol>
<li>This definition of impact doesn't make sense for the AI system that is low impact from a low level. If we were low impact from a high level, we could make it low impact from a higher level by making it low impact from <em>any</em> high level. The higher level, by the time we get to it, is probably too high for the purpose of what we actually were trying to accomplish. </li>
</ol><br><p>From this point of view, what do low impact from a level x look like? Let's consider two examples: </p><br><p>Example 1: An AI system doing low impact action "A" has it's intended impact from an outside perspective of "acquire resources, influence other agents, influence their decisions". There are two ways the AI system could be impactful:</p><br><ul>
<li>Impact via "A". The effect of the system via "A" will be low impact since the effect of the system through the actions of A are low impact in expectation. </li>
</ul><br><ul>
<li>Impact by changing agents' decisions through the actions of B since that will increase the resources that the AI system acquires. </li>
</ul><br><p>From the human(s) point of view, since the action of A was low impact, and an action with low impact from a big AI seems good, only the second option is available. So from a high level perspective, it's high impact from the human(s)'s perspective to make B change other agents' decisions. In fact, it's even higher impact than "AI that does A" since the AI system we're building is high impact in a different way. </p><br><p>Example 2: An AI system that is not going to use the output of the humans to influence other humans does not have human influence. What about the actions that will be taken based on the output of the AI system? Well, if there were any, they would be low impact, from the human perspective since this is what we actually care about. An influence from the AI system from a high level would be high impact from the perspective of accomplishing the true purpose of the AI system, which is not to influence other humans.</p><br><p><strong>There's a fundamental problem with this definition:</strong></p><br><ol>
<li>The low level of impact to be evaluated from the human perspective is very low. The high level of this would make it very high by the definition given in the first version where low-level impact needs to imply low-level approval from the human.</li>
</ol><br><p>So this suggests a new definition: </p><br><p>A low-level influence from the high level system means that if you would have wanted to accomplish whatever the high level AI system is doing anyway, one way or another, you'd have to also have this low level influence. </p><br><p>For the examples above, we would want the low-level AI to acquire influence by increasing the amount of resources it gets in expectation relative to the high level system. </p><br><p>IE, if the high level system wanted to gain resources, influence high level agents, influence other humans' decisions, so would the low-level system, one way or the other. </p><br><p><strong>Why does this matter?</strong></p><br><p>This matters because having low level output is valuable in its own right since it can increase the chance of us having some influence on the world.</p>      </span>    </div>  </div></body></html>