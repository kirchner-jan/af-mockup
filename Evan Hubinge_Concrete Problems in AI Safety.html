<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Concrete Problems in AI Safety      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Evan Hubinge</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>The Problem: Learning Human Biases</li>
</ul><br><ul>
<li>Modeling Constraints</li>
</ul><br><ul>
<li>Constrained Execution</li>
</ul><br><ul>
<li>Conclusion</li>
</ul><br><p><em>This talk was given at __a public online event on Sunday April 7__. If you're a curated author and interested in giving a 5-min talk, which will then be transcribed and edited, sign up __here__.</em></p><br><p>Like most computer scientists, I am a data scientist. A big part of my job is to develop ways to learn something very difficult: a human's value specification. And I get to do that using a lot of tools we're familiar with: deep learning, machine learning, neural networks. But there's one problem that, to this day, we aren't really understanding how to solve: <em>learning human biases</em>.</p><br><p>Why do I call this a problem? Because it's a really useful one to know about! I know plenty of people interested in AI safety who are super curious but just don't know how to solve this problem yet. There's also a huge body of work and discussion out there (many thanks to the <em>Deepmind Safety Research Podcast</em>\xe2\x80\x94I'd also recommend <em>the Alignment Newsletter</em> for some summaries of interesting stuff that has happened in alignment).</p><br><p>As humans, we're trained to learn the values of other humans, which means that we have two main tools for figuring out what we value. Our values come from our brains: from the neural networks that make up our minds and drive our behavior. And we also, as humans, have certain constraints on our behavior (e.g., we're mostly not allowed to wirehead or do unsafe things on purpose).</p><br><p>The hard thing is figuring out how these constraints\xe2\x80\x94and the values we want to figure out\xe2\x80\x94interrelate. To see why, let's see some examples.</p><br><p>One important human constraint is time-boundedness. For any particular person, there are probably just a tiny fraction of their possible goals that actually get them to act in the time available to that person. This constraint, and a few others like it, makes it extremely difficult for a human, or AI, to figure out what someone else actually values.</p><br><p>For example, consider the value of autonomy. It's pretty clear what a human would want with respect to the goal of autonomy. You could think of autonomy as "having a say in how your life unfolds". Unfortunately, autonomy also generally means that someone else is trying to do the exact same thing, but for their own benefit: e.g., "you're free to do whatever you want to do, just not to manipulate me (or others) against my will". For humans, the constraint of autonomy is one of those relatively rare goals or preferences that get enough resources to actually be acted on, with limited downside\xe2\x80\x94in this scenario, it's not so easy to manipulate others because you could just say no, e.g. to a request. But for an AI trying to learn what another human values, if there's no way to distinguish between these two scenarios, then it's really hard to learn what that person values.</p><br><p>Of course, not all human values are like that. To say that someone is trying to manipulate you or manipulate others in general is something almost we use to describe <em>virtues like honesty or fairness</em> or <em>beliefs like "I should get a lawyer if I get the police to arrest me"</em>. Unfortunately, these values are also pretty rare, and we often use them as a proxy for our other values. In other cases, we might even want an AI to <em>value</em> that rare virtue (e.g. for utilitarianism).</p><br><p>In general though, I think we can frame any value learning problem as <em>figuring out what values a person holds, given that there's some constraint on their behavior</em>. And when you have those constraints there's always a problem of <em>distinguishing between different possible values that could satisfy that constraint</em>. And the more constraints there are, and the more different values they could satisfy, the harder it gets from the AI's perspective.</p><br><p>As a result, it ends up looking like there's a similar number of values and constraints as we add to the AI's list of values and constraints, which then leads to many more different things and problems, for roughly the same amount of work. So it's usually a pretty difficult problem, and not very well understood.</p><br><p>The Problem: Learning Human</p><br><p>Biases</p><br><p>One of my big takeaways after spending time thinking about this is that biases in humans are a really <em>hard _thing to get right\xe2\x80\x94and that learning to model them should be a priority! It seems like _most _of my takeaways from AI alignment work have been about "how to learn what preferences humans have, given that there are lots of constraints on their behavior". But we really need to get much clearer on the fact that values/\xe2\x80\x8bpreferences don't have a cleanly defined domain, and instead will probably have a lot of fuzz around them. That's a really hard problem to solve for a good learning AI, and it's why I say that people are _interested</em> but don't necessarily <em>understand the problem</em>.</p><br><p>First thing first: what is a bias? An influential framing comes from <em>Stuart Russell's seminal book, Human Biases,</em> where the term was used as a catch-all for the <em>"cognitive heuristics and biases of rational thinking"</em>\xe2\x80\x94i.e. the <em>examples _of cognitive biases from the book. But it has since come to mean many more things than that, and this is one of the most common uses. In this use-case, a bias is any feature of our rational thinking: a cognitive heuristic or a bias that makes it harder for us to think clearly about some problem. An example could be </em>"this person has been known to give the wrong answer when asked to choose between two options",<em> or </em>"this approach to solving this problem always leads to the wrong answer."_</p><br><p>To be clear, biases aren't necessarily bugs in our algorithms. Sometimes they'll be a feature of our problem formulation that we've learned <em>in the wild</em>: this is a bias because it systematically tends to lead us to errors in judgment, rather than by any inherent property of the problem itself. There's also some subtlety here. On one level it's hard to tell the difference between a bias that systematically tends to lead to errors in judgment versus there just being some feature of the problem that systematically tends to be hard for us to learn\xe2\x80\x94for instance it might take 10 times as long for the problem to occur on a computer versus humans.</p><br><p>What are some of the most common and pernicious cognitive biases you hear about? Let's start with a classic: <em>confirmation bias /\xe2\x80\x8b base rate neglect</em>.</p><br><p>What is this? Well a short version is that:</p><br><ul>
<li>Imagine you have a hypothesis that there's an innocent driver in a certain house.</li>
</ul><br><ul>
<li>Then, you check whether there's someone home that day (assuming they leave at a normal time) and you end up thinking "hah, now they're off their guard!".</li>
</ul><br><p>This example is actually an interesting case study in how our intuitions aren't very good at solving this problem. Even though they can figure out that there <em>must be an innocent driver in that house</em>, and that there won't be any people at the home when they look, and that they can't rule out the driver being innocent just because they didn't see anything in particular, they still end up over-confident\xe2\x80\x94they <em>think</em> there's a decent chance of it being an innocent.</p><br><p>This is a typical case of human thinking being much more <em>"algorithmic"</em> than what we intuitively think, and our algorithms are terrible at this sort of problem. So this is really easy to learn to not be bad at\xe2\x80\x94just make a bunch of decisions and update your beliefs as you go. But what do you learn to be bad at? Well one thing that I think, in general, will improve your thinking:</p><br><p>__Constrained execution\xe2\x80\x94__When you're faced with a problem involving a lot of different things (like two or more different people, or you'll have to do X), we learn to think more like a Bayesian: instead of updating after each new datapoint, we just run a bunch of simulations with different parameter settings and keep whichever works better. This is what a biased person would do. But the key insight here is that if you've <em>already learned</em> that there are limits at play, then it will help a lot to do those limits in, as it were, to constrain your execution.</p><br><p>In general, when you see a problem that involves lots of details interacting with each other, this is likely a good idea: we've seen this before and tried the same approach. But in other settings it's often helpful to take a step back and try to see the problem from a perspective <em>that's more formal</em>. This sort of perspective can help you see the relationships between all of the various pieces, rather than letting your gut instincts take over. See the section on <em>Modeling Constraints</em> for one idea of how you can learn this sort of thing.</p><br><p>Conclusion</p>      </span>    </div>  </div></body></html>