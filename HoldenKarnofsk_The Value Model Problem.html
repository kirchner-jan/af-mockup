<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        The Value Model Problem      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">HoldenKarnofsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Some of the projects/\xe2\x80\x8bideas you like to do can be dangerous. They can hurt other people, or put them in danger. So, what do you do? That's the hard problem of value alignment. And that's one of the things I try to help with. I think one possible answer is that we need to find a workable <em>value model</em>.</p><br><p>That is, someone/\xe2\x80\x8bsomething who can figure out which actions are good, and which are neutral/\xe2\x80\x8bdamaging. Then, if we can figure out the value model (and learn it well), we can use it and its learned assumptions to choose safe actions. I'll briefly talk about exactly what that would mean, and how I see that as related to value identification (VI), and then talk about why this is hard.</p><br><p>For the moment, let's just say that "to figure out the right actions" means "to figure out <em>what we value</em>." This might be a very ambitious goal. But, if we want to be able to use the same methods of decision-making to go from human preferences to actions, it might be our best hope for this project.</p><br><p>So, how do we learn the value model? That's the question the whole post will be about. I'll talk first about what that means.</p><br><p>It's worth pointing out that the value model part of the idea needs a careful implementation, in order to achieve all sorts of desirable properties. In particular, it should be robust, which means it should work in new ways that our best modelers <em>today</em> can't reason about yet.</p><br><p>To illustrate the need for robustness, consider a thought experiment:</p><br><p><strong>Newton</strong></p><br><p>Imagine a thought experiment where we have a superintelligent being named "Newton" and his project of trying to predict human behavior. Newton knows that humans and animals tend to follow certain patterns and laws of behavior and that these patterns tend to hold good across contexts. But, Newton is unsure about the exact form and details of these patterns and laws. He needs to determine them by figuring out <em>why</em> these patterns and laws work across contexts, and how they could fail to work in new contexts. That's the value model challenge, and it <em>will</em> fail to work if we're not careful.</p><br><p>I don't think we should expect that someone has the value model problem solved, as stated above. That would be like expecting that a self-driving car can identify the right behavior in a given situation <em>without</em> building a whole predictive model in advance that would account for everything. Newton is a much more ambitious idea that requires much more ambitious value modeling. But, if the value model is the hard part, then the hard part is the value model, not Newton.</p><br><p><strong>Beware of Dogmatism</strong></p><br><p>So, the value model is going to be a key component of many more ambitious AI projects, like CEV or Oracle AI systems. So, these projects need to be able to figure out (with high reliability and accuracy) which actions would be safe and which would be dangerous for the humans involved. So, the projects need a "good" value model.</p><br><p>The trouble is that I don't think we can necessarily solve the value model problem even with lots of compute, information, data, etc. What if, for example, we simply ran some very advanced Bayesian analysis of all available data from all human behavior, to figure out what actions we would consider "good" based upon all our other data, our models, etc. (This is similar to how we would do things with Bayes nets in AI or statistical modeling in general.) But, there's some risk that this analysis will end up being very dogmatic and too restrictive in how it identifies our values.</p><br><p>For example, maybe all humans are "bad" according to Bayes nets, because they have a value for things like "pleasure" and "self-preservation." But, we might really care for something else (call it X), where we'd judge all behavior involving X according to its own intrinsic good or badness. So, Bayes nets have no way to tell that someone's X-loving behavior is worse than someone else's, and so we get into trouble.</p><br><p>(If you're still unconvinced, try this thought experiment: If Bayes nets identify that someone should be classed as "good" depending on a variable that includes the word "X" in its description, then it could also judge someone bad for having a value that includes the word 'X' in its description. So, there could be many different values/\xe2\x80\x8bpreferences that result in something getting deemed good. Thus, there'd be many more outcomes leading to something being deemed good than any other outcome leading to something being classed as bad. If you don't see this, that's probably because you don't see that this setup is an instance of a "value model." But, I suspect that this isn't actually the reason people find the value model problem hard. Rather, it's because people tend to have intuitions that something like this isn't possible because that seems "too strict," or "too dogmatic." Maybe that could be the right thing to do, but I'm not sure.)</p><br><p><strong>Why Is The Value Model Hard /\xe2\x80\x8b NP-Hard</strong></p><br><p>Why is the value model hard? It's because we could have tons of potential values/\xe2\x80\x8bprefers (or "values models" at least) that all result in the same "good" outcome, and different outcomes of the same potential value. In other words, the space of potential values could have many of the "same density" as the space of all possible policies (for an arbitrary agent).</p><br><p>Imagine two people driving towards each other in an empty car along a one-way road (e.g., an off-ramp from a highway, or perhaps just a one-way urban road). The first person is in a hurry and driving a bit fast (though neither fast enough to run you over nor slow enough to ensure your safety). As the first person gets within a few car lengths of you, he slams on his brakes suddenly ("Oh crap! Sorry!"). But, he didn't run you over, and you're fine. Why does he think he was "safe"? Because he was driving within the limit of what he would expect "safety" to be according to the value model he had, which involved driving that fast and therefore being safe.</p><br><p>So the hard part is that we can't always specify the "right" thing to do based just on simple descriptions of preferences. For example, imagine that we want an AI that can learn what a human would value from a long conversation about human values, preferences, ethics, etc. This requires a value model that can infer human "values" based on a rich understanding of human behavior and preferences. But, it's very unclear how to achieve such rich understanding. It's very unclear how (or if) we can find the "right" way to use all of this data to "learn" the value model. We just end up specifying the value model in terms of what we know already about humans, and then we need that value model to infer what the humans <em>actually</em> prefer. (This seems fairly similar to using a prior for inference, but that wouldn't actually help much here!)</p><br><p>So, we need to find some way to infer a good (idealized) value model, without specifying the value model as a simple "correct" one or even using any data at all about what people actually <em>want</em>.</p><br><p>Some other possible approaches:</p><br><ul>
<li>If we know how humans typically solve some kind of value inference problem, like Pascal's Wager, then we could infer the right value model from our reasoning here. But, we'd need to know the right way to apply this inference to complex, under-specified value/\xe2\x80\x8bpreference problems, and we'd need it to work well even in domains like physics, where we have to reason beyond the data in order to make strong predictions. (See also this post about this.)</li>
</ul><br><ul>
<li>Maybe we're OK with being dogmatic, and assume that if the right values are not exactly what we find to be right, then these values are wrong. This sort of dogmatism seems fine for many values, like "life" or "being nice" or even "being right." But, it doesn't seem so for values that involve human preferences, which we <em>really</em> want to be right about (at least in most cases). Why is that? I don't have an intuitive guess of the answer to that question, but I think I can say a bit more about here.</li>
</ul><br><p><strong>More About What Could Possibly Go Wrong</strong></p><br><p>I think there are two kinds of things that could go wrong with this approach. At this point, I've seen some related ideas that seem plausible to me\xe2\x80\x94they're not airtight nor am I too confident they'll pan out\xe2\x80\x94and it's not clear whether these ideas can be made to work. But, here are some ideas anyway.</p>      </span>    </div>  </div></body></html>