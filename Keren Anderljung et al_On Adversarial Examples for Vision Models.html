<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        On Adversarial Examples for Vision Models      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Keren Anderljung et al</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>Background and motivation</li>
</ul><br><ul>
<li>The proposed approach: Anchor</li>
</ul><br><ul>
<li>Experimental results</li>
</ul><br><ul>
<li>Further thoughts and discussion</li>
</ul><br><p>Background and motivation</p><br><p>The success of convolutional neural networks in image classification has led to their broad application to computer vision tasks like medical imaging, automated visual recognition, and image synthesis. However, while many researchers are enthusiastic about convolutional neural network-based approaches (Szegedy et al. 2013, 2016a; Goodfellow et al. Bengio and Bengio 2016; Zhong et al. 2017; Wu et al. 2017) there are also significant concerns with regard to their robustness against adversarial attacks (Bengio et al. 2014, 2015; Bouchilloux et al. 2017).</p><br><p>In fact, adversarial perturbations to black-box neural networks are becoming a major cause of concern in the machine vision community (Gilmer et al. 2017, 2018; Rudin et al. 2018; Huang et al. 2018a; Gilmer et al. 2018b). They are already used to attack vision systems (Rajasamy et al. 2018), and their popularity in large language models (Brown et al. 2018) has spurred a research community to find and demonstrate novel attacks (OpenAI Hub 2018; Zeyzhan and Liu 2018a; Gilthorpe et al. 2018). As machine learning systems become more powerful, there is a greater risk that such attacks will enable malicious actors to manipulate them for personal gain, thereby undermining the foundation of democracy and the public trust in the economy (Bostrom 2014; Schmidt et al. 2018, 2018a).</p><br><p>This growing concern motivates us to conduct a scientific study of adversarial examples against convolutional neural net-based vision systems in a real-world setting. We do this work in collaboration with security researchers who are members of this community. The results of our study will be presented in the following paper: <strong>On Adversarial Examples For Vision Models: A Scientific Study</strong>, in which we conduct experiments with a state-of-the-art convolutional neural image classifier, and test whether perturbations chosen by modern adversarial techniques will work against it. Our purpose is to address current concerns and provide a foundation for future research.</p><br><p>The proposed approach: Anchors</p><br><p>In our experiments we adopt the following approach:</p><br><ul>
<li>We test how each of hundreds of common natural images is modified by a set of image normalization techniques, to generate a suite of candidate images.</li>
</ul><br><ul>
<li>We test each of these images against a set of four strong adversarial attacks.</li>
</ul><br><ul>
<li>We explore which of these adversarial attacks works against the classifier, and explore on which of these adversaries it is vulnerable.</li>
</ul><br><p>Our goal is to find out which adversarial examples work well, and which are hard. What follows is the full suite of candidates, then the set of adversarial attacks that we used to test them, then some further thought experiments.</p><br><p>Experimental results</p><br><p>We begin with experiments on the natural image dataset described in Szegedy C., Bengio Y. and Zador U. (2013). We first perform a forward pass on the natural-images feed forward neural network, and then apply each one of 300 natural image modification techniques to the final classification layer:</p><br><ul>
<li>Rescale the inputs from the range 0 \xe2\x88\x92 1 to the range <strong>range</strong>\xe2\x88\x88R.</li>
</ul><br><ul>
<li>Add the outputs from the previous layer to the inputs.</li>
</ul><br><ul>
<li>Clip the images to <strong>min(max(0, input \xe2\x88\x92 range), 1)</strong>, using the output of the previous step.</li>
</ul><br><ul>
<li>Normalize the inputs using a set of normalized images <strong>N</strong>.</li>
</ul><br><ul>
<li>Clip all inputs to <strong>interval</strong>\xe2\x88\x88R.</li>
</ul><br><ul>
<li>Multiply all inputs by (1 + cos(<strong>theta</strong>))\xe2\x88\x9ad\xe2\x88\x88N, where      theta=\xcf\x89(1 \xe2\x88\x92 p(d)) and \xcf\x89\xe2\x88\x88R+     p(d)\xe2\x88\x88[0, \xce\x98(d))].</li>
</ul><br><ul>
<li>Clip all outputs to zero if they had range <strong>min</strong> or greater.</li>
</ul><br><p>The perturbations from the original images are generated by choosing one of these methods, then a random image perturbation <strong>p</strong> from a uniform distribution.</p><br><p>The experiments are performed by first generating images using the <strong>baseline</strong> method, then applying each of the 300 perturbations to that image, and finally using the classifier to calculate which of the four adversarial attacks is most effective at detecting a given image. The four attacks are:</p><br><ul>
<li>L_p-ball: p is chosen from a uniform distribution on the L_p surface of a unit ball in R^d</li>
</ul><br><ul>
<li>L_q-ball: p=\xe2\x88\x92|\xce\x98\xe2\x88\x921(p)|</li>
</ul><br><ul>
<li>L_1-ball: p=-|max(0, min(0, image)+max(0, image))|</li>
</ul><br><ul>
<li>L_2-ball: p=(\xe2\x88\x92|image|)\xe2\x88\x97max(\xe2\x88\x92|image|, |image|\xe2\x88\x920.1)</li>
</ul><br><p>The __baseline __method results in about 20 images being classified correctly, on average. (However, this is not a fair evaluation as our set only contains 812 images with the true label of the target image.) To get a sense of the difficulty in the classification task, consider that of image 812: (x, y)\xe2\x86\x90(0, 0), and the four attacks are classified as: L_p-balls (y)\xe2\x86\x900 (1), L_q balls (y)\xe2\x86\x94(0), L_1 balls (y)\xe2\x88\x88[1, 1.1), and L_2 balls (y)  (0).</p><br><p>We see that the attacks work most reliably (i.e. with greatest relative effectiveness) against L_2 balls, the least reliable attack. We next want to find out which features make up L_2 balls that are likely to be misclassified by adversarial perturbation. For that, we use feature selection \xe2\x80\x94 a method used in other fields of computer vision for the identification of robust features in images.</p><br><p>Feature selection </p><br><p>First, we need some measure of how good or bad a feature is at discriminating between classes. To this end we use <strong>class SAC</strong>,<strong> __an algorithm that computes the class-feature discrimination strength of each potential feature i.</strong> To do this, we evaluate its distance from the nearest sample of interest. Using __class SAC __we identify a set of 30 potential class-features:</p><br><p>The 30 features are as follows:</p><br><p>{(<strong>cos(\xce\x98)\xce\xb8+sin(\xce\x98)\xcf\x86)_1</strong>+(((1+cos(\xce\x98))+sin(\xce\xb8))\xce\xb8_2__+((-1)\xce\xb8_3__+sin(-2\xce\xb8))\xcf\x861),...,(<strong>cos(\xcf\x89(1\xe2\x88\x92p(d))\xce\xb8)\xcf\x86)7</strong>+((-3)\xcf\x868)... (__cos(\xcf\x892\xce\x98)\xce\xb7\xcf\x88)21=0\xe2\x88\xaa\xe2\x88\xa9,(__sin(\xce\x98)+3cos(\xce\x98)+2\xcf\x8820=0},</p><br><p>where:</p><br><pre><code>\xce\x98=(1\xe2\x88\x92p(\xc2\xb7))\xce\xb4,
</code></pre><br><pre><code>\xcf\x88=\xe2\x88\xa5\xe2\x88\xa52p\xe2\x88\x922\xce\xb4d\xe2\xa5\xa52,
</code></pre><br><p>\xce\xb7=\xe2\x88\xa5\xe2\x88\xa8\xe2\x88\xa7\xe2\x88\xa3\xe2\x88\xa3p\xe2\x88\x922\xe2\x88\x921\xce\xb4d\xe2\x82\xa42 </p><br><p>Where:</p><br><p>\xce\xb2=8\xe2\x88\x9ad\xe2\x88\x97\xcf\x88d\xe2\x88\x97\xce\xb7  </p><br><p>We then apply the feature selection criterion described above to each of the 30 potential class-feature sets     (in order to select the optimal feature set). This yields an estimated total score     of each potential classifier. </p><br><p>We use the feature score output by this method to determine <strong>rank</strong> of the features.</p><br><p>For each feature i we take the rank r of the feature i such that r\xe2\x88\x97\xe2\x88\x91n=1\xe2\x88\x92rmax_m=1|\xe2\x88\x91j=1(C_m[x_i-C_n[x_i]]\xe2\x89\xa0y_i)|\xe2\x89\xa40.1. For each set of candidate class-features {C_n=(1r)&lt;...C_n=(sr)} we then calculate the distance of the feature i to its nearest sample in the current set, and add these distances weighted by the score of the features associated with C_n.We define the set that has the lowest total score as the optimal set of candidate class members for the current feature set, Cp.</p><br><p>Finally, we test each classifier C_p for its ability to classify the target sample (x,y):</p><br><p>The classification is done by determining which of those classifiers has the smallest total difference between the computed and the desired class assigned to the test image(x,y) : </p>      </span>    </div>  </div></body></html>