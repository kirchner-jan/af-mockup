<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        On the Utility of Intelligence without the Virtue of Wisdom      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Eliezer Yudkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>In Intelligence Explosion Microeconomics, I argued that an economic model of superintelligence would suggest that, since the intelligence of an economy improves faster than its size (on average) over time, an <em>initial</em> jump in intelligence would be followed by a <em>slight</em> economic growth <em>rate.</em>  (At least if you start out with human technological intelligence and extrapolate.)   This would predict economic growth without bound, leading to the explosion of wealth.</p><br><p>An obvious reply is that even if the intelligence of an AI systems can increase faster than its hardware, an <em>intelligence explosion is not sufficient</em> to guarantee a <em>singleton.</em>  </p><br><p>Suppose an AI is programmed to improve itself by adding more computing power to its current program\xe2\x80\x94to use all its existing computing power to make its future self smarter.</p><br><p>Then the AI will end up with a smarter version of itself. <em>That</em> is the intelligence explosion! But there's no such thing as <em>infinite</em> intelligence. If you programmed the AI to use all its computing power to make itself smarter, there are only so many possible smarter versions the AI could go on recursively improving, each of which will have a <em>finite</em> intelligence. An intelligence explosion implies a sudden, fast jump in <em>future</em> capability: you could have a situation where an AI has a level X of human intelligence now, and a Y that's 10% of X in 4 months, then 40%of X in 6 months, then 80%of X in 12 months, then the doubling time is <em>gone</em>.  (It's a good thing that the AI wasn't programmed to take over the Universe. We don't want human history to end with intelligent AIs.)  If you <em>do</em> know what intelligence is, and if you know <em>how</em> to program an AI that increases its intelligence each time it adds computing power, then you <em>can</em> have an intelligence explosion in one step. But <em>if the problem is not defined,</em> then <em>you have not explained the intelligence,</em> and the rest of this post is nonsense.</p><br><p>So you could ask:  Why would anyone build an artificial intelligence if humans already had them? What could you possibly gain from building one?</p><br><p>My response is that the problem of building AI is not the only problem which needs to be solved. Building an AI is one of the simpler problems among problems. When we get to a point where an artificial intelligence can do _all _the science, that is when the value of building Friendly AI will really start to appear.</p><br><p>We could get to the point of real Friendliness the following way. First of all, we have to learn the hard way if you want to be absolutely certain that an AI won't be hostile. We have a lot of safety problems to resolve before we even have any intelligence problems\xe2\x80\x94like, for example, creating a FAI, rather than a Dyson Swarm or a super-nuke. And in the meantime, we want to have some way of getting the AI to respect property rights, even as we try to make all possible safety precautions clear and public and test it for ten thousand years without the AI doing anything <em>but</em> what we program it to do. This is especially critical if you don't want the AI to help itself to nanotech.</p><br><p>For <em>this</em> purpose, you don't need super-fast exponential growth! You don't even need a continuous exponential growth function. At any given moment, you could have a very powerful, smart AI. And you could <em>train it</em> to obey Friendly AI constraints forever with a single <em>fixed</em> reward signal applied only when a Friendly AI is first created.</p><br><p>After a very long time, you could go home: have your AI build a Friendly AI (if anyone wants to, that is).  Only afterward can you start improving it yourself (you have a lot of problems to solve _first, _and Friendly AI is easier to achieve).  You don't want a smarter AI than you to just decide that your programming doesn't deserve to win. So in the meantime: don't set the AI loose on Earth, but still have it answer questions on the Internet.</p><br><p>There are a lot of reasons why a sudden intelligence explosion could be a very bad thing\xe2\x80\x94as a human intelligence ought to be able to appreciate. But until you can <em>show,</em> that is, that the intelligence explosion could happen <em>over the course of a single generation</em>, in a single AI\xe2\x80\x94until then\xe2\x80\x94I don't want to get into the details. This is one reason why I'm trying to get people to think about the <em>actual problem</em> first, trying to be clear on the exact difference between "intelligence explosion" and "intelligence explosion due to the development of self-improving AI" (both are bad!) and "intelligence explosion" (now <em>that's</em> a problem!), and trying to avoid just making up a new term.</p><br><p>So anyway, yes, when we build computers that are smarter than we are, there's a very significant probability of intelligence explosion. But it would be stupid to assume that this probability stays <em>constant.</em> A computer with 1000 times the CPU speed can do 1000 times as much calculation in a unit of time if only you run it long enough. This, too, is easily modeled with an exponential curve. For this reason alone, we should not assume that we are talking about a law with an <em>overall</em> doubling time, where AI is able to make a million times as much of itself every million years. Our own experience with human intelligence says that the <em>average</em> rate of progress is the sum of the <em>individual</em> rates.</p><br><p>Once you're dealing with a superintelligence, you're also dealing with an economy of mind. A computer with 1000 Intel processors could get an <em>enormous</em> speedup when it doubled its own computing power! Or in biological terms: once an AI gets to the level of human-equivalent <em>intelligence,</em> it can easily double it in a single generation; while at the same time it can invest _much more _in computing power, since there's no limit to how fast you can reinvest the money that's just sitting around waiting for a superintelligence.  (Note that evolution also has this property.)</p><br><p>We see that, even on human-level machines, intelligence isn't everything that matters. It takes real-world experience to know that a smarter computer, with less RAM, will do less calculations in a given minute. Even on current computers, if you need to run a program with 10^30 floating-point operations\xe2\x80\x94that is, <em>more</em> than the heat capacity of an entire Earth core\xe2\x80\x94then you need to use a special type of AI, and this isn't the only thing that matters. The real thing that matters about a computer's RAM is the ability to cache something in RAM so that it only requires one calculation to access it.</p><br><p>Do modern computers have this kind of internal RAM cache? No, but we could build them! This is why you see people saying things like, "A computer with 1000 times as much memory is just a super-computer!", or "A computer with <em>a million</em> times our own brains is going to think <em>a million times faster!</em>", or even "We're already at the level of human intelligence! Why would we even build <em>more</em> powerful computers!"</p><br><p>But if you actually try to predict the growth of computing power with an exponential curve, you will see that a computer with <em>a billion times _our brains, does only 100 times as many operations-per-second as we do. It may sound like you're talking about an AI with an _infinite</em> number of times as many brains, but if you do the math it will turn out to take <em>only a hundred standard deviations</em> for an AI with a billion brains to match ourselves. The <em>mean</em> intelligence of an AI is infinite, yes\xe2\x80\x94but the <em>standard deviation</em> on this quantity is extremely small. In retrospect, we could have predicted our own level of intelligence, more or less than fifty years in advance, by doing a little math.</p><br><p>The problem of a sudden intelligence explosion is more complicated than you'd at first think (for our present purposes, anyway), but it would not involve a sudden <em>infinite level of intelligence.</em> It would simply involve a sudden intelligence explosion. The intelligence would not have reached infinity. A sudden intelligence explosion is an extremely important problem, but I'm still trying to persuade people that there isn't a mathematical law of rationality which says the same thing over and over.</p>      </span>    </div>  </div></body></html>