<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        What makes a good AI?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Benquo, btrette</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>AI is hard, and humans, and alignment</li>
</ul><br><ul>
<li>AI is easy, and humans</li>
</ul><br><ul>
<li>Why AI is hard</li>
</ul><br><ul>
<li>Intelligence</li>
</ul><br><ul>
<li>Value systems</li>
</ul><br><ul>
<li>Social intelligence</li>
</ul><br><ul>
<li>Alignment is hard!</li>
</ul><br><p>In the course of trying to reason about AI, many people make statements about what AI systems should be, but it's often not clear how the idea is supposed to make any sense as a concrete proposal. In this post, I try to lay out what I think they're really gesturing at.</p><br><p>I'll argue that there's a useful sense in which "hard" is not the right term (as in "AI is hard in a different sense" or "AI is hard compared to other difficult things"), and describe the features of intelligence that are most useful for thinking about AI. I'll distinguish between "agentic" and "social" intelligence, and argue that a particularly compelling approach to AI is to start from an understanding of agency at the highest levels of social intelligence rather than working at a lower level.</p><br><p>AI is hard, and human, and alignment</p><br><p>In the post AI Alignment: What's the Problem? (And what can we do about it?), Paul Christiano argued that any approach to AI alignment will also require us to understand agency:</p><br><ul>
<li><strong>AI is hard:</strong> AI progress is accelerating (comparing 2012 to 2020 is like comparing 2018 to 2012), but I don't think that is likely to last. It's entirely possible that after 10 years we will be able to solve alignment, and that the world will end up looking like 2020, but from our point of view it will look like the future is much less predictable than it appears right now. While it's possible that progress is bottlenecked by the difficulty of AI, it could also be that there is some fundamental property of the alignment problem that makes it harder than the difficulty of other problems we don't yet know how to solve, and that has led to acceleration of AI progress.</li>
</ul><br><ul>
<li><strong>AI looks human:</strong> The easiest-to-measure ways to check that an AI is aligned with human interests will involve some level of modeling the human. If this AI model is built in a way that looks like "take an honest but imperfect measurement of the things humans value (which is itself a system with errors), add them up into a utility function that's maximized by the AI and then the AI tries to maximize this utility function", we will be able extend the concept of "good for humans" from our own intuitions to the AI system. </li>
</ul><br><ul>
<li><strong>The AI looks aligned:</strong> The human concept of alignment will serve to help the AI figure out what we mean by "good, in context" or "acceptable behavior". In effect, Paul's comment about how he wants to "build an aligned AI that builds things that are useful, even though I don't know how to reason about that" is really saying "build an aligned but imperfect AI that doesn't get too far from what I want."</li>
</ul><br><p>-</p><br><ul>
<li><strong>AI looks safe:</strong> An aligned AI will only take actions that we're happy with. This is a pretty trivial constraint on an AI system once you start looking at it at the right scale, but safety is hard from the perspective of a human who hasn't been explicitly trained to understand agency. The problem may be much harder for AI systems because our intuitions for agency are built into the very architecture of the brain, not the very architecture of a computer program (which makes them less predictable from the starting point). </li>
</ul><br><ul>
<li><strong>AI is safe:</strong> An AI system that can't take any action that could be described as not what we intended will usually be much slower in most or all of the ways that we use our lives.</li>
</ul><br><p>-</p><br><p>A lot of the reason for my interest in these points isn't for the sake of arguing that AI alignment necessarily requires understanding agency. Rather, if you take agency for granted, what you're looking for is some kind of story for why the development of aligned AI is likely to work: some sort of story for how the properties of agency can be preserved in an engineering process, or what it looks like to avoid or to make progress on that particular "fundamental property". Paul seems to be interested in that kind of story, too.</p><br><p>What makes it clear that what's really needed for AI alignment, aside from the AI system itself, is the human concepts of "alignment" and "good" that will be used to guide the AI? I suspect that a large part of the answer is that the story for learning to align an AI must come from somewhere that human engineers can trust, in an environment where human engineers can see the consequences of their work and correct course. This story needs to contain enough information to produce a system that can be deployed in an environment, so it won't come from Paul's or any other individual working directly on or from the problem. </p><br><p>That makes AI alignment seem hard.</p><br><p>If you view the world as an environment where humans can deploy systems that are aligned with human interests, then the AI systems that can be deployed end up being things that look like human brains, not like a "big red box" which is just another way people have come to think of "computers" that humans have control over. In other words, AI alignment is basically a version of outer alignment which works by default: it's a version of alignment that people are likely to be okay with.</p><br><p>But it may be less obvious than that, or less important, to what extent we need to be good with alignment from the perspective of the kinds of agents with high-level goals.</p><br><p>AI is easy, and human</p><br><p>I feel that one of the most common misconceptions about Paul's ideas is that what's necessary for AI alignment is to understand human values: that we need to learn, or at least discover, the "true values" of our current systems. When I write posts about this, people often respond by saying "surely humans can't <em>possibly</em> value the things that their AI systems say would maximize human happiness, etc."</p><br><p>Part of the goal of this post is to demonstrate that this kind of reaction is very common in spite of the fact that (a) it's rarely justified by specific arguments or objections, and (b) it's often not very relevant to what's actually going on. (For example, the fact that I'm currently in the process of being in a long-term relationship does not mean that what I really value in my relationships is the happiness my partners experience.)</p><br><p>As someone who has put a lot of effort into thinking about how to align AI, and spent a long time talking (at events like the Singularity Summit and other rationalist communities) to other people who are trying to think about AI alignment, the amount of time that I've spent thinking about this stuff gives me at least a somewhat privileged vantage point into the actual landscape of arguments that people are making for and against it.</p><br><p>A good starting point for reasoning about AI alignment is to distinguish between two axes on which AI systems can be thought of:</p><br><ul>
<li><strong>Intelligence:</strong> AI systems can be imagined as being at the level of a human, or perhaps having an IQ on par with a human superintelligence capable of understanding everything better than a human can. (Paul's post mostly discusses alignment from this perspective, though the first example there is "AI research that could make a non-dangerous system smarter than all the people on Earth" and the second example is the same as the first, which is not to say the post does not discuss alignment from the perspective where the human-level agent is less intelligent than the human-level superintelligence, though this point is not very clear about what's meant by smarter than all the humans on Earth).</li>
</ul><br><ul>
<li><strong>Self-modification:</strong> AI systems will, or at least might have good reason to, be able to modify their own source code. I believe this is a much more basic concept than "agency" and I'll argue that most of the difficulty in alignment comes from self-modification.</li>
</ul><br><p>Here, self-modification may not necessarily mean "AI systems will be able to read and act on their own source code". The human concept of agency and the concept of code and "reading" it might seem straightforward enough to describe for humans, but the task of writing a program that implements these concepts on behalf of an AI seems likely to be much harder. It's easier to imagine an "agentless AI" that isn't also an "agent" \xe2\x80\x93 and if you're reasoning about alignment as-if, it's easier to think about systems you can build without having to know in advance how to build systems with your own minds.</p><br><p>In the rest of this post, I will be using "self-modification" and "agent" interchangeably.</p><br><p>Why AI is hard</p><br><p>For the sake of this argument, I'll assume that human intelligence is fairly stable over our lifetime and that people aren't going to get dramatically smarter by default. If we can come up with a way to get this intelligence increase without it coming from AI in the first place (which is a pretty huge assumption), then that could help us avoid some of the alignment problems.</p><br><p>However, I think it's reasonable to expect these difficulties in the absence of a large intelligence increase from AI to be a big factor in limiting what sorts of AI systems can actually be built. Here, we'll be looking for a story for why these sorts of difficulties are solvable in spite of the AI risk community's common assumption that an intelligence explosion is the answer.</p><br><p>An early insight in thinking about AI alignment is just how hard it is to reason about intelligent behavior.</p>      </span>    </div>  </div></body></html>