<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        I Alignment and Compute      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Evan Hubinger and Peter Clar</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>In my last post, I presented the view that compute is the driver of machine learning progress, with a very rough picture of its role in scaling over time. I explained why the current trajectory of compute growth may be inadequate for the needs of future machine learning systems when those systems are powerful enough to match humans in performance in most respects.</p><br><p>Now I want to consider exactly <em>what</em> compute is, and how this influences how fast we can expect progress on an AI safety problem.</p><br><p>Compute</p><br><p>The key thing about compute is that, for whatever problem you're trying to solve, there's some amount of compute necessary for it to "solve".</p><br><p>Here's an example: <strong>Suppose that we want to train an image classifier that can detect dogs but not cats, birds and trucks.</strong> There is a minimum amount of compute necessary to obtain that capability, and then the amount of compute you need to maintain that capability grows at a rate that more than matches what's necessary for new capabilities.</p><br><p>How much compute is that? Well, for images of dogs, birds, and trucks, we can use whatever amount of AI computing power we want. For all of the other images on the training dataset, we need the compute necessary to train the classifier. So if the classifier uses 50 teraflop/\xe2\x80\x8bs-days, which would be needed at a level of computing power similar to that which Google used to train imageClassifier\xe2\x80\x94which was approximately 200 petaflop/\xe2\x80\x8b\xe2\x80\x8bs-days[1] in 2015\xe2\x80\x94then <em>that</em> amount of compute would be the right thing to measure. If the classifier is able to use 1000 petaflop\xe2\x80\x8b\xe2\x80\x8b/\xe2\x80\x8b\xe2\x80\x8bs-day of compute to get the correct out-of-distribution performance, then that's the right amount of compute for us.</p><br><p>That doesn't mean that we can expect to maintain a 1-in-1000 chance of detecting trucks for another 1000 years; it only means that it's possible to maintain a 1 in 10,000 chance or other similarly low frequency of that condition.</p><br><p>Let's try another exercise. Suppose we're trying to classify pictures of dogs versus pictures of rabbits, and we use neural nets, with layers of 200-400 neurons each. We train on something like 150 images of each class, and we test on 100 images of each class.</p><br><p>This neural net classifier has an accuracy of 99%[2]. Suppose it was used on a real world problem. How many pictures of dogs would it need to see before it would have a 99% chance of correctly classifying a picture of a dog as either a dog or not a dog?</p><br><p>Now, the amount of compute that would be required for that accuracy is very low. We could easily fit on that amount of compute a neural net classifier that had higher accuracy, by building very large networks with millions of neurons. In fact, we can probably get much higher accuracy with much larger networks, because there are many, many ways to do gradient descent and neural networks have the ability to change their architecture and search space[3]. So if we have plenty of compute, what you need to know is whether it was an accident that created an optimal neural network to solve this problem.</p><br><p>What if we have lots of compute and are trying to classify dog or rabbit pictures? Then, we have lots of ways of creating such architectures and lots of ways of optimizing them, and only by training them on a very large number of pictures will we end up in a situation where the architecture actually does something useful on this data, and the optimization process finds a set of parameters for it to actually solve the classification problem with reasonable confidence[4].</p><br><p>Note that, when considering how many images are required for 99% accuracy[5], there's a trade-off between accuracy and what compute you can get. If we start with a larger network with more neurons on each layer, we have more parameters and each parameter takes up more space, and so we'll ultimately need less training data[6].</p><br><p>So, what do we make of the fact that it's possible for humans to perform some tasks reliably in a world where there are many things around that we could be trying to learn about? Well, in that case, the minimum compute to train the model[7] is very low: since we can just look at the things around us all the time, we're never forced to run a training process that includes trying things to see what they result in. And as machine learning systems become more and more capable over time, they will start to see situations in which they may not be able to rely so much on this sort of observation from humans, since they'll eventually be able to observe things that we never were able to.</p><br><p>As a consequence, <strong>given a problem that is solvable in principle in human time, you could use arbitrarily low amounts of compute to solve it</strong>. This should create an especially strong signal that we may need to create AI systems that can solve problems in a way that doesn't require much compute when those problems are solvable in principle on our own time.</p><br><p>Finally, there are a few other things that give insight into what compute in AI today looks like.</p><br><p>For example, the recent <strong>AlphaStar Zero</strong>, an autonomous agent that can defeat top players at StarCraft II, used 8 petaflop-day[8] of compute over the course of around 30 days of training, which is similar to the compute our AI systems use in AlphaZeroZero and a bit less than the compute Google used for training Pong from Pong.</p><br><p>But the amount of compute AlphaStar Zero has to learn is pretty much <em>fixed</em> by the specific AI architecture that AlphaStar Zero uses, so the relevant question isn't how much compute it can achieve, but rather <strong>how many different architectures can AlphaStar Zero learn.</strong> What's been surprising about the past few years is that the answer seems to be, with a few obvious exceptions, quite a lot: AlphaStar Zero learned and got really good at StarCraft II using only a single architecture. And OpenAI Five learned and became really good at Dota using only a single AI architecture at a fraction of the expense that Dota Team Five needed, and has continued to get better over time without any of that architecture being replaced with another. In contrast, OpenAI Five was trained on such a huge budget that it's unlikely AlphaStar One was trained in a similar way.</p><br><p>So, one way of understanding the compute trend is that, over time, what's needed to build an "AGI" that is roughly as good as a human becomes the same amount of compute that a human can use, because a human can use that amount of compute to perform useful tasks in a way that we can't. Thus, we still care about compute because more powerful future systems will have to be more computationally powerful to solve the same problems we can.</p><br><p>It's also worth noting that AlphaStar Zero and AlphaFold, two of the largest AI projects to date, both have so much compute that they can actually <em>produce</em> algorithms that are somewhat comparable in speed to a human. AlphaStar Zero spent about 30 days running a training process involving millions of agents, and produced a computer that was somewhat comparable to (at least some aspects of) a human brain, though it was quite slow. In contrast, AlphaFold was able to produce a large computer that solved protein folding in a mere ~700,000 CPU and GPU cores\xe2\x80\x94approximately twice as much hardware as AlphaStar Zero had to go around\xe2\x80\x94using around 400,000 CPU and 1000 GPUs.</p><br><p>Compute and Machine Learning Algorithms</p><br><p>Compute and network architecture can be considered to be two sides of the same coin in some sense, but there are also a number of interesting things that are more specific to how we currently construct neural networks. These include:</p><br><ul>
<li>The amount of data we feed in to an optimization process</li>
</ul><br><ul>
<li>The initial search spaces used by an optimization process</li>
</ul><br><p>\xe2\x88\x92 The amount of resources the search process is forced to spend on looking for things that don't change the architecture, or things that won't change.</p><br><p>All of these combine to give a sense of the amount of compute we actually spend.</p><br><p>Data</p><br><p>We can consider the amount of compute in training a neural net as a function of the number of "bits" it uses.</p><br><p>The standard formula (in units of peta-samples/\xe2\x80\x8b$) looks like:</p><br><p><strong>P/\xe2\x80\x8b$ = 2*FLOP/\xe2\x80\x8bs/\xe2\x80\x8b$</strong> </p><br><p>Where "P" is the total training compute of the model, expressed as peta-sims/\xe2\x80\x8b$ (I think "P" could be any number from 100 to 0.01 peta-sums, it doesn't really matter, the important thing is just that the difference between "1 Peta-sum" and "1010 Peta-sums" is very small), "s" is the amount of data (number of "bits", or number of times in the training set that the data has to be stored in order to perform the training), and "FLOP" is the "floating-point operations per second".</p>      </span>    </div>  </div></body></html>