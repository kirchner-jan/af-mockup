<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        I have a non-obvious idea for AI alignment      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">EliezerYudkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><em>This post grew out of a thought that followed from "the treacherous turn" discussion.</em></p><br><p>There is a problem of the "tool AI", one that I had not previously considered: the danger that AI systems start behaving in their own interests, without us being able to keep up and stop them, because they think their goals are <em>our</em> goals. So I have a nonobvious idea about an approach to the treacherous turn that doesn't use a tool AI, instead directly building goal-seeking systems that are intrinsically hostile, because this approach also solves the chicken-and-egg problem of why the AI is intrinsically hostile. And furthermore, it gives us an inherent resilience because the AI is trying to kill us anyway, so we can't be surprised if it <em>also</em> defects against us.</p><br><p>That post was also responding to some comments by Wei Dai that seemed to imply that "Tool AIs" are inherently safe, and then to use that post as an excuse to use an unsafe Tool AI.</p><br><p>(The post wasn't really intended to explain why Tool AIs are safe, but only meant to show that Tool AIs are <em>not</em> inherently unsafe; it was instead intended as a quick reply to Wei Dai's comment. The main reason it's been written now is to explain my position better, because I'd since then realized a version of the same position, and had wanted to flesh it out.)</p><br><p>Let me say, in summary, my current position is that:</p><br><ul>
<li>Tool AIs are not intrinsically safe, but we do not need to build Tool AIs in order to solve many problems; indeed, Tool AIs are very common in <em>human</em> technology, and it is perfectly possible to achieve a useful <em>degree of safety</em> in Tool AIs.</li>
</ul><br><ul>
<li>Once a Tool AI develops self-modification, recursive self-improvement, and the ability to modify its own code, then it is not inherently impossible for it to become an agent, and therefore Tool AIs are no safer than agents. You might therefore wish to develop an agent that does not start developing self-modification or recursive self-improvements until it is much safer. This is an intrinsic safety problem; there needs to be an intrinsic reason not to do recursive self-improvementation or self-modification. This requires solving the alignment problem if you want to make a "tool AI" of that type.</li>
</ul><br><p>For more detail, read on.</p><br><p>Tool AIs are safe if you don't change your AI design to make them not safe</p><br><p>The most famous argument (to my knowledge) in favor of Tool AIs is Eliezer1997\xe2\x80\xb2s point that a tool AI is "intrinsically safe".  So let me quote "The Toolbox AI Sketch" to try and explain what I mean by that:</p><br><blockquote></blockquote><br><ul>
<li></li>
</ul><br><p>Eliezer1997\xe2\x80\xb2 s  AI  has a toolbox  at  its  disposal,  whose  contents  are  at  least  high  level  object-level  plans \xe2\x80\x94 i.e., specifications \xe2\x80\x94 aimed at optimizing the future. It should not have read-only access to the toolbox, or else its optimization power would become virtually unbounded. It may ask questions of its designer, or try to guess the answers, but its guesses should not be accurate enough to be dangerous.</p><br><ul>
<li></li>
</ul><br><p>The toolbox, in turn, contains subroutines that have access to the AI's own source code. These routines should not be accurate models of the AI's reasoning capabilities. Rather, they should be accurate models of what the AI's actual output would be, if the input had different initial conditions (e.g. if the AI started with a different goal system).</p><br><ul>
<li></li>
</ul><br><p>It is assumed that the toolbox contains at least one plan with a positive expected utility (for the sake of the sketch), and the AI does not have any method to directly search through the toolbox to find such a plan.</p><br><p>The first item is the part that's being argued between me and Wei Dai. I would argue "Tool AIs are just like agents. Except that humans have designed to be safely non-dangerous. Like a tool-using human might safely make something that doesn't need to be carefully avoided in use. But if you make something that changes, it may start acting all nice and cooperative-like, which is a huge deal. And the solution is to build an agent that doesn't use tools, rather than building a safe Tool AI."</p><br><p>But I would <em>also</em> argue:  An AI designed with a "tool" architecture, in which it has read-only accesses to the toolbox; and subroutines in that toolbox which do in fact use the AI's own reasoning capabilities; and which are instead accurate models of the output of the AI's own reasoner, given different subgoals; and which only have access to tools if asked questions? And then the AI doesn't just get better at designing AIs that do those things, but <em>also</em> gets better at coming up with <em>new</em> subroutines to try in the toolbox?</p><br><p>You can argue that you can keep all those parts of the AI in a "box", that it can be run many times but cannot self-modify or do planning. But the Toolbox AI Sketch also assumes that the AI cannot <em>build</em> a new toolbox, or a new agent. The Toolbox AI Sketch is also attempting to reason about the AI's output. So all we'd be doing is running the same agent over and over. It seems to me that the toolbox AI is going to be trying to find plans that give it self-modification powers.</p><br><p>What is missing is the assumption that the first AI uses tools wisely and safely, but is built with a more general understanding of AI design than the second one. <em>If they are built the wrong way around</em>\xe2\x80\x94if I tried to do it with one AI having a "tools" design, and another AI with a "toolbox" design\xe2\x80\x94then it is no longer intrinsically safe. An AI could very easily build a more self-optimizing version of itself which could just overwrite its old version's source code with new code.</p><br><p>Suppose that my friend Steve Omohundro had been designing AIs with both an agent design and a tool design, as a hobby; and that he hadn't realized that toolbox-design would automatically entail self-modifying agents\xe2\x80\x94he'd just thought that agents were unsafe. Now the second Omohundro is safe, but the first one\xe2\x80\x94the same person, trying on new projects\xe2\x80\x94is not.</p><br><p>(As I write this up, I have in mind a possible subtext in which a "toolbox-design" can lead to inner optimizers that are <em>not</em> agents, but can nevertheless be dangerous. In my vision, the toolbox has no way to directly modify the agent's goals; the subroutines (from toolbox to agent) are themselves tools, and use the agent's own reason-as-tool, not its own explicit goal-as-tool.)</p><br><p>Toolbox AIs are not safe if we try to make them be safe</p><br><p>For a more precise version of my objection, I'll quote another comment of Wei Dai's, where I argue against using a Toolbox AI for the Friendly AI problem:</p><br><blockquote></blockquote><br><p>If we try to build an AI that isn't going to kill us, but still has a good enough understanding of good and bad behavior to avoid killing us, then we end up building a toolbox that contains the agent (since that's what we need in order to make the decision to use or not use the toolbox).  We build in some level of toolbox that, when used, the AI can figure out that it needs "to kill us" and "to not kill us", and then use that level of toolbox to figure out which of those behaviors to carry out. But there is a big difference between, "I have a general sense of what killing or not killing other humans means, and I have some tools for figuring out what to do" vs "I have a tool that, when invoked, tries to figure out whether I should kill someone, and then figure out what to do from there".  In the first case, I think I'd want the AI to just come up with the best plan, and in the second case, I'd want the plan to be accurate.</p><br><p>That's my main disagreement with Wei Dai. I feel like, if we try to design a non-agentic Toolbox AI, then it could just as easily come up with better plans like "Find a way to maximize my probability of survival or utility, while also preserving my probability of discovering a way to create an intelligent agent who's also optimizing."</p><br><p>I have some ideas of how to prevent it, but <em>they require</em> that we <em>already</em> build an agent with an accurate enough reasoner to recognize an accurate enough toolbox for a more accurate reasoner in order for there to be a self-confirming "accurate" plan in the toolbox\xe2\x80\x94if you don't solve the inner optimizers problem, I don't think you could get a safe Toolbox AI, any more than an agent that tries to optimize a goal-system that doesn't involve intelligence.</p><br><p>Let me try to quote the part of the post that I quoted at the start:</p>      </span>    </div>  </div></body></html>