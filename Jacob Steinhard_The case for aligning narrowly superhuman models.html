<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        The case for aligning narrowly superhuman models      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Jacob Steinhard</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>The goal of this post is to provide clarity about why I think the work of aligning narrowly superhuman AI systems can be profitable for us, even if we don't work on AGI at all. I think there can be a lot more clarity around what people working on aligning narrowly superhuman systems are actually trying to do, the challenges facing them, the advantages and disadvantages of their research directions, and what kinds of risks they're trying to avoid. And I'm pretty optimistic about making progress on all of those fronts!</p><br><p>Aligning narrowly superhuman models is closely related to the problem of AI safety from first principles, and I've talked about it a bit on this forum before. Here I'll explain a bit more why I see this research path as extremely important. Broadly speaking, my opinion is that any work in this space will have three key components: building models that are narrowly superhuman at a task, applying that information to help solve larger, more general problems, and actually creating useful, valuable products based on the solution(s). The closer any model is to being superhuman, the more useful it becomes, and the more likely we are able to solve all the problems in AI safety by being a few years ahead. I also think that there are interesting conceptual parallels between the work of aligners and the work of safety analysts: both involve us learning about the limitations of AI systems, but I expect them to be complementary, with aligners working on the part of the problem that I don't think any safety analysts currently understand well and safety analysts working on the part that is most salient to them.</p><br><p><strong>I. Modeling what can be learned</strong></p><br><p>I'd be very excited about work on aligning narrowly-super human models because I think it's likely to answer a really important question, in which case I expect it to be worth the best current AI labs throwing a lot of money and manpower at the problem. Of course, a lot of this is not about what models would be useful to work on, but rather about finding models that are useful in a large space of models, which is a question I think is at least vaguely related.</p><br><p>As evidence, I'll give a few arguments about why I think work on aligning models may be especially important (this list is by no means exhaustive). I think that these arguments can point to the directions that I think most potential customers want aligners to work on, and that there is likely to be a good market opportunity in the product-based business models that these customers use. I also think some of these arguments can have a very large positive impact, at least in the context of small, narrow AI companies like DeepMind or OpenAI. I think this last point is an especially important motivation for why I think narrowly superhuman models may be the most profitable way to use the work at today's AI labs.</p><br><p><strong>II. The basic research direction: The "Align this" problem</strong></p><br><p>Why do I find this research direction useful for aligning narrowly-human-models? The core insight is that since we're going to do pretty much all of the same things we did before with superintelligent models, if we can make them easier to align, then maybe we can just do that work one or more levels of abstraction up and then apply our findings to superintelligent models. (This is a key motivater for me.)</p><br><p>This argument assumes that aligning superhuman models is sufficiently similar to aligning ordinary models that we can just use the previous experience in a straightforward manner. One important question is how hard this is to pull off: If it's hard, is it possible we can get an order-of-magnitude or more speedup from using the techniques from the previous case?</p><br><p>So let's zoom out a bit and think about it from the perspective of the customer. If we solve the relevant parts of the model alignment problem, then we would be able to produce the product that uses the model as described in the post \xe2\x80\x94 which itself could open up entirely new, very profitable business models. A small company could use a narrowly superhuman model to take over Amazon by taking advantage of its recommendation and logistics systems. A medium company could use it to find a software engineering team to build the system that can use it to effectively manage people and customers in the process of finding the next product-creation bottleneck. And a large company could use it as a core component to take advantage of all the economies of scale that larger companies have. </p><br><p>This leads to what I find one of the most compelling motivaters for pushing forward work in this space: This can potentially help us solve all the problems with AI safety, at least to the extent that this research path is useful. More concretely, I think there are two distinct sets of arguments that suggest the space of possible problems in alignment is large, and if we're able to solve the relevant parts, then one or more orders of magnitude of acceleration could make the entire space of AI alignment problems easy to address. This last argument gives a pretty concrete benefit of working on aligning superhuman models, at the very least it shows a clear way in which we can expect to have a strong positive impact on the overall landscape of AI alignment problems. The other arguments do something more subtle, because (1) they give more broad reasons to expect that it's worth working on the part we want to work on and (2) they give additional reasons why even that part is hard to tackle and might be really useful to solve by learning from experience.</p><br><p><strong>III. The "Align that" problem, and why it might need its own theory</strong></p><br><p>One complication with the "Align this..." problem is that I think it could benefit from a different frame than the "Align that..." problem from the perspective of safety (see The Alignment Problem for my previous thoughts on why this is true). This leads me to think there may be something important to be learned from working on aligning narrow models, that could apply to aligning models that are arbitrarily superhuman at a task. Since aligning superintelligent models will be one of the most difficult and important parts of the field in decades, it's a particularly urgent concern that researchers who think they have good ideas might not be able to work, in part, because they don't know their own ideas really well. And so in these cases I find it valuable to try working with models that humans can already get some benefit from, and see how we can take our conceptual findings and re-applied them into something that's interesting to the rest of the field.</p><br><p>I also expect some important lessons from aligning narrowly superhuman agents to carry over into the "align that" problem, but those lessons don't have to necessarily come form the alignment problem of superintelligent machines at all, we could learn from e.g. the field of robustness in ML. It's possible that some solutions will be general and could be useful here even if they look specific to aligning superhuman agents. For example, I think it's particularly interesting that the way human bias affects most modern AI systems is through misalignment with the user's intentions: It's more likely that some general techniques for avoiding alignment failure in any setting will apply.</p><br><p><strong>IV. The product based framing</strong></p><br><p>The main use case for us as a field is to help out at any one of the big AI labs. This leads me to believe that the best potential impact and the best potential customers I can bring to this area need to be relatively similar to the sort of people I've had to work with at the big AI labs: people who are relatively young and trying to bootstrap companies. </p><br><p>Since we're not trying to produce an end-to-end functioning product for the first time, this leads me to think that there are likely to be a lot more problems we can solve with these kinds of models than we have been thinking about. It's easy to be excited by the biggest and most exciting success stories (e.g a new company making $100M doing something really cool with GPT-3), but I think we underestimate how many problems we can solve simply by understanding the bigger picture. An understanding of the field as a whole, as well as the specific problems and obstacles we need to overcome, is likely to improve our ability to be productive on the problems we work most frequently with at the smaller AI labs. </p><br><p>I think this perspective could change a lot of the research direction I recommend. I think that for a large fraction of the work I think could be done today, the people who are most well-suited to work on it are people who are trying to bootstrap their own companies. For other work, such as research at the top AI labs, I'd actually recommend working on the sub-problems that might be useful as research directions later if and when other researchers come back with their own goals. My biggest objection to this perspective, however, is that it makes a lot of alignment researchers less excited about the work (to the extent that existing machine learning research is not doing what they'd like it to do in this space). For example, I'd prefer a slightly bigger market for aligning superhuman models than I don't to someone who wants to work on aligning superhuman AI systems more. </p><br><p>In any case: I think that working towards product-based business model is a good guide for how to choose and organize work. This perspective also gives some additional reasons to think that aligning narrowly superhuman model might be useful: even an ordinary narrowly superhuman model can be a useful stepping stone for later progress in the field.</p><br><p><strong>V. Other reasons to work on aligners rather than safety engineers</strong></p>      </span>    </div>  </div></body></html>