<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        How bad are other existential risks?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Richard_Ng</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>In this post, I'm going to argue that non-anthropogenic existential risks are not serious or urgent enough to be taken into consideration when evaluating the risk posed by AGI. There are risks which seem very important by my lights, including risks from nanotechnology and biotechnology. But these risks are so far removed from the problem of AGI that I don't think we can use them to inform an estimate of the risk posed by the development of artificial general intelligence.  </p><br><p>Note that my assessment doesn't rest on anthropogenic catastrophes being the only existential risk worth worrying about. Rather, the conclusion depends on the risk posed by an AIs with a long time horizon, capable of self-modification and capable of causing a variety of catastrophes in the medium-to-long-term, outweighing any existential risk from human extinction. And if long-term risks do outweigh short-term risks, but AIs are very constrained in what they can do, I think it's reasonable to take them to be much greater than short-term risks.</p><br><p>One important note is that, like many previous discussions of existential risks, I have in mind a <strong>"fast takeoff"</strong> scenario. In this scenario, there's relatively little time available to react to the development of powerful AIs. I'll discuss this scenario briefly, and then turn my attention to slow takeoffs which involve AGIs which are constrained in how they can affect the world but which are nevertheless able to cause significant suffering.</p><br><p>If we had some way to evaluate a given catastrophic risk that wasn't based on its probability of causing human extinction or suffering, what might the criteria be? In the case of human extinction, it seems clear that we ought to use the probability of death and suffering that would result from the catastrophe itself. To be more specific, if a catastrophe leads to the extinction of only humanity but leaves billions of other species living on in an otherwise-vibrancy universe, then that catastrophe might be more important that the loss of life on Earth. Indeed, by the same token, if we imagine that we're in a position to cause a slow-onset existential catastrophe which leaves an overwhelmingly vast universe filled with life, that may make us look less scary by comparison. So I think the most important thing in our picture of a catastrophe is the extent to which it threatens everything from the perspective of life as a whole.</p><br><p>I could imagine the catastrophe leading to the survival of only humanity by some miracle. So, we ought to look for ways in which that catastrophe could harm life on a broader scale than the one which humanity lives on. We should also look for ways in. which it could lead to the extinction or radical transformation of human civilization or even the universe as a whole. </p><br><p>My tentative conclusion so far is that some human extinction catastrophes, for example an asteroid impact, are likely to have a very low impact on the rest of the universe while other extinction catastrophes might have very high impacts. I would like to see more work on this question, as well as empirical work determining whether we can predict whether fast or slow takeoffs will lead to the survival of most of the universe, and on what the consequences of such scenarios would be in practice. That being said, I tend to consider the most important consideration to be the extent to which an existential catastrophe leads to the annihilation or a significant restructuring of life on a broad scale. </p><br><p>Another important topic is the extent to. which a catastrophe can occur gradually or suddenly. We want to have a good sense of what level of progress towards AGI the world would need to reach before we can be confident of the safety of an AGI. In general, I expect this question to be hard to answer well, because it depends on the level of advancement we expect in the relevant time scales. An important piece of evidence may be if various AIs have achieved impressive cognitive performance, before this ability has been combined with enough self-improvement to change the game drastically. And if we believe long-term scenarios of self-improvement, then it seems that that will be enough time for any level of intelligence to lead to a decisive strategic advantage without being caught. But I also expect it to be very hard to predict what level of self-modelling capability we'll have, and in any case, the best reason for thinking there's a non-negligible risk of disaster is if there's little information we can draw from extrapolating what AI progress will be like in that case. In principle, I'm fine with using intelligence in service of other goals (such as paperclips or self-modification) as a potential source of evidence for why the world is getting less stable over time.</p><br><p>So far, I've discussed the likelihood of various causes with significant negative impacts on Earth. I also discussed some potential benefits of AGIs, like technological development and increased economic productivity. But so far, we've mainly discussed the risk posed by a general intelligence capable of causing harm in the far future. I next want to talk about other sources of risk which seem more immediately important and imminent. I haven't thought very deeply about them, and expect most readers with a greater interest in this topic than I have to do the research themselves.</p><br><p>One obvious source of risk is climate change. Anthropogenic natural catastrophes like floods, hurricanes, wildfires and so on are particularly concerning for people living in highly populated areas. But in principle, I expect the risks from AGI to be particularly important given that the relevant existential risk is not extinction but rather significant suffering. This means we can get away with assigning a significantly lower probability to extinction by anthropogenic and/\xe2\x80\x8bor civilisational catastrophes, compared to extinction due to alien invasion for example. So, to evaluate these risks we can try to estimate the likelihood of each one taking place.</p><br><p>I think that climate change is the main non-anthropogenic source of risk today, and I've already discussed in more detail how I'd evaluate that. But one reason I think some people are reluctant to accept anthropogenic risks as a serious contender with climate change is that they assume climate change is relatively new, and therefore unlikely to be a source of future problems. But I think non-anthropogenic risks are much more fundamental. And I don't think people currently expect climate change to be a major source of problems, so I think their fear of climate change is a sort of "rationalization" of anthropogenic existential risks.</p><br><p>Even if climate change is not a leading cause of future risk, I think that there are other sources of non-anthropogenic risk which are more important than climate change. For example, I think that pandemics caused by biotechnology are much more important than pandemics caused directly by wild animal viruses, but the latter seem to get more attention by non-expert voters. So my intuition, even if correct, is that there's a good chance that this consideration will become more important over time. And I haven't made the argument in more detail because it's not clear to me in advance which sources of risk will dominate. But I think AGI is the most important source of non-anthropic existential risk.</p><br><p>The problem of AGI may be solved by a single team or a small number of teams; if so, and if the field is relatively stable, I think we can expect the risk to be easily calculable. But if there's a potential for the problem to emerge from multiple teams, then I think that even if one can calculate the risk, it's not yet clear whether the estimate from any single team will have a decisive impact. This is because it may be hard to know how many teams will be working on AGI. It's also not clear how far they'll be from achieving a decisive strategic advantage.</p><br><p>A question similar to these is whether AGIs which are relatively "constrained" in what they can accomplish will outperform those which are not. If, and for a very long time into the future indeed, this is the case, it might give us a strong indication that the risk posed by such an AI was actually small. While it may be that the risk will be high even with constrained AIs, and the only reason why we can calculate an estimate is that our future projections about intelligence are very bad, in which case the small risk is dominated by the high uncertainty about the initial situation.</p><br><p>The reason I think climate change matters as a source of AGI risk is that there's an underlying argument about what level of world literacy is a desirable standard in an intelligent system. More specifically, I think that in order for AGIs to be able to cause harm in the future, they will need to be capable of achieving an enormous range of outcomes that the human population isn't, including:</p><br><ul>
<li>The ability to manipulate their environments in subtle ways. For example, an AGI might be able to control when there are or aren't certain types of clouds in the sky.</li>
</ul><br><ul>
<li>The ability for a single AGI to prevent certain events from happening in the world by manipulating other AGIs that will be strategically relevant to achieve its goals. This is analogous to how humans can stop AGIs from being used to do bad things by manipulating them. It's similar to the problem discussed in the Nick Bostrom paper "The Superintelligent Will".</li>
</ul><br><ul>
<li>The ability that a single AGI has over thousands or billions of other AGIs. For example, if it has created a very large and powerful network of other AGIs, then it may be able to obtain a decisive strategic advantage due to the leverage given by having a huge network.</li>
</ul>      </span>    </div>  </div></body></html>