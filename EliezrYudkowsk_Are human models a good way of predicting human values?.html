<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Are human models a good way of predicting human values?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">EliezrYudkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><strong>Followup to:</strong> The Singularity and Machine Ethics</p><br><p>Yesterday, in the comments here, I expressed doubts about the validity of an evolutionary argument in favor of human values. This morning, I was reading about some work on evolutionary psychology that <em>makes the argument</em>\xe2\x80\x94specifically, it's not at all clear that it's an argument <em>that actually works.</em></p><br><p>I have to say, this is just absolutely astounding. A field that seems to me to be in full support of what I stand for, is suddenly running headlong into a wall. As far as I can make out, this is what happens when other people start asking about the future. They get stuck. So do I, more of the time.</p><br><p>And yet it occurs to me that there may not be <em>any</em> future. It occurs to me that all this moral progress we saw over the past two centuries\xe2\x80\x94the abolition of slavery, the abolition of child marriage, the reduction of violence\xe2\x80\x94might have gone on indefinitely, but the <em>factors</em> leading to these changes, these evolutionary forces\xe2\x80\x94might not be in the right place to do it any more, the way we have them now. Perhaps the moral improvements that seem to be so important for me and people like me\xe2\x80\x94things to do with less fear, more self respect, more compassion and cooperation\xe2\x80\x94are only <em>slightly</em> more powerful now, in human DNA, than they were two hundred years ago.</p><br><p>Maybe the way we value things now\xe2\x80\x94how we got from tribal values to universal human rights\xe2\x80\x94is the way <em>that</em> happens <em>right now.</em></p><br><p>We do all the things that seem good and virtuous to us, at around this moment in history; we make these improvements to our morality according to the values we've now, at this moment of time, and they happen on a global scale for a global audience (and not just on a local level), as people talk to each other and learn things about each other.</p><br><p>Maybe we got a lot more powerful than anything else before, in the last two hundred years, and that might be why?</p><br><p>It seems to me that there's this big gap between our current knowledge of the human brain, and our current understanding of the future of intelligence; this gap lets me to see through to whatever values come next. In the future, we will probably know that much more about the nature of intelligence. But if you look over that gap (all the way back to where human brains first evolved a year ago), it's not <em>fully</em> filled\xe2\x80\x94nothing ever seems to go all the way through in a single jump. Maybe there are no future values at all, then, only some human species that is <em>almost</em> what human values are now, only a little different.</p><br><p>I don't like this idea, because I think it means we should give up on trying to improve our values. Not so fast, says Robin Hanson, because there's still a future.</p><br><p>So there it is, a possibility that I must consider, and a danger I need to avert. But Robin wants to throw out the baby with the bathwater here.</p><br><p>He wants to throw out <em>not thinking about values</em> in the future, because the future is already here, so that means it may already have human values. For Robin, it's very important to have a future, and so a lot more important than the future to have values right now\xe2\x80\x94or as important as the values right now, but with more intelligence over time, because "intelligent values" <em>needn't</em> be the same thing as what we have now.</p><br><p>As far as I can tell, here Robin is taking the values right now for granted (he talks about the values that have already been written into the human genome), and is just trying to get them from there to intelligent values in the future.</p><br><p><strong>Note:</strong> I may be getting this wrong, but Robin and I disagree on what the values in our human genomes might look like, and how much intelligence is required to create something like that. If my model is false, Robin would have the advantage on this one.</p><br><p><strong>Further note:</strong> I think that what I stand for\xe2\x80\x94the things we would stand for if we were all very smart\xe2\x80\x94would be "value loading", not "value learning". That is, most of my values are fixed (I don't want to move my kidneys over there, I don't want any more of this), but my goals for the future and my reasoning about the future, are changing over time\xe2\x80\x94learning from experience, rather than knowing in advance what I would learn when I saw it for the first time.</p>      </span>    </div>  </div></body></html>