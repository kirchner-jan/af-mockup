<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Should AI researchers care about the long-term future?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Elie Zivi</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>Introduction</li>
</ul><br><ul>
<li>What is the long-term AI-risk?</li>
</ul><br><ul>
<li>AGI, FAI, and timelines</li>
</ul><br><ul>
<li>What should AI researchers do in the short-term?</li>
</ul><br><ul>
<li>AI policy and governance</li>
</ul><br><ul>
<li>What should long-term AI risk researchers do in the long-term?</li>
</ul><br><p><strong>This is the third of a six-part sequence introducing</strong> <strong>"The Long-Term Future of AI," by Joel M. Cohen and Nick Bostrom. It was originally published on July 18, 2014.</strong></p><br><p>Introduction</p><br><blockquote>
<p>When humanity has created an AI, we can be fairly certain that we will have achieved something of truly global significance.</p>
</blockquote><br><p>-- Stephen Hawking</p><br><p><strong>Read the rest of the series (by the same authors) here:</strong> http://\xe2\x80\x8b\xe2\x80\x8blongtermrisk.org/\xe2\x80\x8b\xe2\x80\x8b2013/\xe2\x80\x8b\xe2\x80\x8b05/\xe2\x80\x8b\xe2\x80\x8b01/\xe2\x80\x8b\xe2\x80\x8bthe-long-term-future-of-ai</p><br><p>(In the next post of this series, we'll go through two different strategies for influencing the long-term outcome of AI research, then we'll combine them and examine the potential impact of coordinating between them.)</p><br><p>It is very important to me that <em>the right people are working on the right problems</em>. We can't have, say, a thousand AI researchers working on creating AGI, while only some of them care about a "long-term" impact. It would be far less effective than we need it to be if only a tiny portion of researchers thought seriously about issues for millions of years to come.</p><br><p>It is far less important, however, that <em>the AGI field is working on the right problem</em>. A lot of other research areas also seem to be working on the "right problem", but I see these working on different problems. It is the <em>convergence</em> of many good efforts into a coherent whole, which is the key factor in the future's trajectory.</p><br><p>It may sound naive, at first, to think that more people might work on the same problem. But, I actually think that things <em>are</em> converging in certain ways.</p><br><p>I want to first review the problem that AI risk researchers are currently working on, which I call artificial general intelligence, or AGI. We'll see that most AI-risk researchers do not believe that AGI will be the end of the world; they expect it will simply pave the way to an even better future, a future in which intelligent machines play an essential, even dominant role. I think this is reasonable, given the evidence in favor of the general view, and what is at stake.</p><br><p>But most non-AGI researchers working on the task of AI risk just don't think AGI is very important. In particular, many AI researchers believe that the most likely use of today's technology is simply to "do" computers, even if those computers do things like being programmed, being curious, and even being somewhat conscious. These researchers tend to be pessimistic because our current state of knowledge is not sufficient to create a real sense of optimism about the future.</p><br><p>The other major way in which non-AGI AI risks researchers differ from AGI researchers is that non-AGI is far less central an issue in their minds. The other major issues for them are things like unemployment, education, and other matters related to individual human beings and non-superintelligent machines that have what I called "limited autonomy."</p><br><p>The result in both cases is people working on AGI risk while not really being "nudged" by other factors, which makes them less effective and less likely than they could be.</p><br><p><strong>So as a first step to improving the effectiveness and effectiveness of AI risk research, we can ask how to nudge many less effective efforts into something like an effective whole.</strong></p><br><p><strong>Here I present a nudge that I and others have been considering, called</strong> <strong>the</strong> <strong>Strategic AI Risk Project</strong>, or <strong>START</strong>. It will be best, in my opinion, if people use this project as inspiration and/\xe2\x80\x8bor encouragement for new initiatives to coordinate well among themselves.</p><br><p><strong>But I am not going to write up the full details of this project in this post. In each subsequent post, I will simply summarize more details, and then discuss the question of whether this kind of coordination and influence-seeking, along with other factors, can be usefully coordinated.</strong></p><br><p>What is the long-terrm AI-risk?</p><br><blockquote>
<p>It is conceivable that a form of Artificial Intelligence, or AGI, could be created, with the potential to improve itself rapidly to a point where it was orders of magnitude more intelligent than humans.</p>
</blockquote><br><p>-- Bostrom and Yudkowsky, <em>Human Compatible__ </em>(p. 18)</p><br><blockquote>
<p>I think the most common mistake people make is to assume that AI is just a very powerful computer program that learns. The problem with AI isn't that it's a computer that learns\xe2\x80\x94it's that it's a powerful computer system that you can't control.</p>
</blockquote><br><p>In real life, the world is governed by the rules of science. If our brains are as smart as we think they are, they should be able to apply the laws of science and understand that these laws don't go away just because we build powerful computers.</p><br><p>-- Stuart Russell, <em>Surfing Uncertainty</em></p><br><blockquote>
<p>I believe that it is entirely possible that we could build an Artificial Intelligence that was radically more intelligent than the best human brains, yet had little or no inclination or ability to alter the real-world environment around it...</p>
</blockquote><br><p>-- Nick Bostrom, <em>Superintelligence</em> (p. 28)</p><br><blockquote>
<p>What if someone invents a type of artificial general intelligence that's much smarter than a human\xe2\x80\x94say, 25 or 30 times more intelligent\xe2\x80\x94but which isn't super intelligent? If such a being were ever created, it could go on to colonize the galaxy.</p>
</blockquote><br><p>Why are such things possible? Because you could imagine some sort of being that was so smart that it could come up with clever ways to improve its performance. If you're a human trying to solve a problem, you don't stand a chance against someone who's even smarter, who can think even cleverer thoughts. I don't think such things are possible, but it's conceivable, so I don't dismiss the problem out of hand.</p><br><p>-- Yudkowsky and Bostrom, "Creating Friendly AI with current Oracle AI research" (p. 14)</p><br><p>There seem to be three basic classes or risks:</p><br><p>Artificial general intelligence or AGI would end the human era, and we should try to avert this danger as quickly as we can. If AGIS are not created within a few years, this would be an existential catastrophe and humanity's long-term future would be gravely at risk. If AGIs are built within a few decades, many other potential existential risks, including ones related to the consequences of the creation of AI, would become more acute, and would deserve more attention. If an AGI is constructed within hundreds of years, then we are unlikely to ever have to choose whether to create an AGI (except in the cases where we intentionally create AGIs and then learn from them), but we should probably take any possible AGI risks into account. (An AGI with even a slight propensity to take actions it perceives as catastrophic or self-destructive would most likely meet the definition of an AGI, so any AGI that exists at all would not automatically be regarded as safe or friendly.)</p><br><p>Artificial superintelligence or ASI would be extremely useful to its creators\xe2\x80\x94in contrast to, say, nanotechnology, it seems difficult to imagine ASI being used destructively against its creators. But as it becomes more capable (whether due to the increase in computational power provided by more advanced computers, or some other source), it would also become more dangerous to its creators as well. The creation of safe ASIs may also be a great deal more difficult than the creation of safe AGIs. It is not obvious to me that we should give more attention and emphasis to AGI-related dangers than we currently give to ASI-related dangers.</p><br><p>Artificial unFriendly advanced intelligence or AUFI would, if we succeed in creating it at some point, have potentially vast effects on the world. Unlike an ASI which would have essentially an instrumental goal of making itself more intelligent, an AUFI would not necessarily care about continuing to exist or about anything else beyond its instrumental goal. Unlike an AGI, it would not automatically care about being friendly or safe. It might be instrumental to the creation of safe (or more intelligent) AI, but it would not be instrumental for it to continue to exist after its goals had been accomplished.</p><br><p>A final class of possibilities, what I am starting to call the <strong>human-AGI</strong> scenario, are the ones for which it may make the most sense to try to influence humanity's long-run trajectory to increase the odds of desirable outcomes. For example, if someone creates an AGI and it is friendly and cooperative, or if people create an AGI that is not-yet-friendly but still has a certain amount of self-control, we may be better off if our descendants in general, and our descendants in particular, continue to care about each other. (This may be a special case of the "human-ASI" situation, but it seems good to treat it as a scenario on its own.)</p>      </span>    </div>  </div></body></html>