<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        An Argument Against Utility Functions      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">EliezerYudkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>From a certain perspective, there seems to be something <em>very</em> odd about attempting to model human beings as maximizing expected utility\xe2\x80\x94in fact, about thinking at all when making decisions on the basis of maximizing expected utility?  (There are some possible responses to this puzzle which do the math better, but, still...)  I have long pondered this puzzle and its strangeness, but I can't seem to figure it out. I think it's related to a kind of meta-problem where we try to construct an intuitive understanding of how we make decisions, in such a way as to be convinced that our decisions make a lot of sense at the time they were made\xe2\x80\x94to feel that it's <em>plausible</em> that those are the kinds of decisions a human being would be <em>likely</em> to make based on this or that information.  (In short, we try to <em>see</em> ourselves making the kinds of decisions we <em>actually</em> see ourselves making, not some abstracted idealized version of that. Or so I am trying to see myself doing.)</p><br><p>And I can't seem in general to follow that intuition where it says, "If your brain is doing something as a direct result of some abstracted computation over state machines, maybe there's a better way to think about that."  The way I feel about these things\xe2\x80\x94at least the way I <em>feel</em> within my own head\xe2\x80\x94seems to me to be that the intuitive model of our decisions as making sense\xe2\x80\x94even in theory, even if only conceptually\xe2\x80\x94seems to correspond with having a utility function with a single well-defined output and a single well-behaved internal computation that implements the function. If you look only at the utility function's inputs, outputs, and internal computation, and forget what the <em>actual decisions you</em> used to compute the result, the outputs and internal-computation may not reflect the true function at all!  (At least, no more than a random set of random points will reflect any given function, if you're searching a two-dimensional manifold.)</p><br><p>Or consider it from a different perspective:  We want to think about ourselves making and using "utility functions", but if we look at our actual brainware for making decisions, it doesn't use utility functions. To the best of my meager understanding (though of course I could well be ignorant or misspecified), neurons in the human brain do not in general take the world, process it in their own self-contained ways, and use some result of the processing as their internal decision criterion\xe2\x80\x94they do not have utility functions.  (Even neurons which implement some approximation of TD learning or other model-based planning don't in the end just compute a function, but learn a <em>model</em>. They look at the results of their own calculations as inputs to further calculations.)  I don't think we generally think about ourselves as acting on utility functions\xe2\x80\x94rather, we think of ourselves as having goals and using planning and learning to reach them.</p><br><p>I have long ago given up on trying to argue the existence of an abstracted utility function within the human brain. It would not, I think, be very useful to the AI. As for the rest... that's something I'm still pondering.</p><br><p>But just in case the above does not convince you\xe2\x80\x94which it should not! As I've indicated several times now\xe2\x80\x94but if you <em>must</em> believe that there's some invisible essence of human utility which you're computing, then the following will not be convincing.</p><br><p>For starters, I'd suggest that we just try <em>not _to think about the outputs and internal computations of the utility functions in the brain, as the outputs and internal calculations are (I think) not what's being represented at all in the brain; they merely serve the function of conveying information to the parts of the brain that implement the value function, and are not the ultimate decision. Maybe this is just because we have _different</em> representations of outcomes in our neocortex versus our hypothalamus, or maybe we can tell which representation is talking at which time. (This is one of my current pet research projects.)  But I think it's more likely that these output/\xe2\x80\x8binternal representations aren't the same representation as the brain thinks they should be, in the same way that a human might talk about wanting to go to the bathroom while not having a bladder that actually feels full, or while having a bladder that still <em>inhibits</em> the urge to go to the toilet\xe2\x80\x94they're just not the same brainware as the algorithm the brain uses when it feels the urge to go. (As a possible analogy, "You can actually have a feeling of hunger that doesn't require food to take up space in the stomach."  And yet "You can think about eating food, even while you do not have a stomach."  Again, the information represented will be the same for both, but the representations used to <em>feel</em> the urge are different.)</p><br><p>Which brings us to the next point:  Even if we can't or don't want to think about something in the brain, we still <em>see</em> that something in the brain\xe2\x80\x94and it's still not the output/\xe2\x80\x8binternal representations. We can see <em>it's a picture</em>; we can sense <em>what's out there in the external world</em>. If the human was told, "That picture you see in front of you, is not an image in your own head or an image of the outside world, but is instead a tiny red rectangle in a world of grey lines, one of which you are just now standing in front of"\xe2\x80\x94well, I don't think that statement could have gone over into the human brain uninterpreted. The human can see a picture in their own head, and even if they say so afterward, they can't possibly deny it because their internal monologue of "I see a picture in my head" is not the same thing as the visual experience they had. So it seems to me that the utility function picture which our brains use to decide how happy to feel, is not the same model as the brainware (neurons?) which implement the model.</p><br><p>So for the utility function to correspond to anything, we would need to somehow have a <em>connection</em> from neural representations to utility representations.</p><br><p>This does not seem to me to be biologically possible\xe2\x80\x94if you can read <em>that</em>, then you can read this too. For example, if someone says, "This part of the human algorithm corresponds to joy because of X", you can just look up X; it says, "Because of neuron #93420 is active is a sense of joy"; there is nothing special that happens inside an algorithm in the human brain when the algorithm goes on to say, "The resulting utility value associated with the joy is five," because the connection from the algorithm to the utility function happens outside in the external world.</p><br><p>Suppose I put some little smiley face on a rock. I wouldn't think it was because of some little program in my head going on to say that happiness should be associated with having a little smile. The little smiley face would be there because it's <em>out there</em> in the world, and it made a little sound as it hit the surface of the rock. This gives us a very basic and basic example of a utility function not occurring inside an algorithm (a human brain, possibly).</p><br><p>So then what if I tell you that inside my algorithm there's a little smiley face as the first step? But when I say that, I don't have an accurate algorithm that implements this algorithm\xe2\x80\x94I'm just telling you <em>what the algorithm feels like from outside</em>. I'm also saying that <em>you're standing outside the algorithm; to think that a tiny, red, smiley-faced grin-shaped thing is making a _decision</em> and <em>acting on</em> a utility-function that was computed inside a black box is just silly <em>a priori</em>, like expecting a little green robot to make tea.</p><br><p>Of course you <em>could</em> be inside a black box, and it might not even be <em>real code</em> or neurons\xe2\x80\x94the box might be a mathematical structure with some representation of the algorithm's <em>outputs</em> inside it (like a Turing machine implementation) and it might be written in a completely different language from the algorithm's native code (like a pointer to a file).  But just to be a little more thorough:  Yes, it could be the case that the output of the black box algorithm and the black box inputs are <em>also</em> represented inside some tiny little box inside your brain. But the <em>decision</em> is not the black box thing itself, the utility function is not the black-box outputs; the inputs to the black box are not the outputs; if you can read the outputs from inside the black box, it's because they happened to <em>originally occur</em> inside (or in physical connection with) some little picture-making robot inside your brain, and that robot's inner decision is not itself a black box implementing the utility function and causing your brain to say "Oh, look at that tiny smiley."</p>      </span>    </div>  </div></body></html>