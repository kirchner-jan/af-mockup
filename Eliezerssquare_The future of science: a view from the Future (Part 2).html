<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        The future of science: a view from the Future (Part 2)      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Eliezerssquare</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Followup to: When Science Meets Science Fiction</p><br><blockquote>
<p>I sometimes talk about "the future" of science, to which people invariably reply, "I don't feel threatened by science being better than the movies; I want the movies to make better art." What's weird to me is that I would bet more people will talk this way in 20 years than agree that movies should be better art.</p>
</blockquote><br><p>--Eliezer Yudkowsky, in the future of science part 2</p><br><p>This post concludes the part II of my "Future of Science" series. Its title is an allusion to Eliezer Yudkosky's <em>Fable of the Dragon-Tyrant</em>: a novel in the form of a dialogue between the Dragon King of that story and the author, in which the King explains the nature of power-seeking as a means of persuading the author to support the King's claim to power over the dragon.</p><br><p>You may also not be surprised at the result of my discussion with Eliezer, in which Eliezer wrote up a series of bullet-point style questions that he asked of me while I drew a mental diagram:</p><br><blockquote>
<p>Yvain: But what I feel is that, at the very least, the "problem" you're pointing out seems to have no good solutions\xe2\x80\x94it seems almost as if science should just do away with that "problem", or else become more careful.  (Which doesn't <em>sound</em> like science, but it does sound to me like the sort of thing that would convince me in science.)</p>
</blockquote><br><p>Eliezer:  Well, even a dragon ought to know that the only power in the world is power-over-men, and therefore the only power a rational being can have is power over <em>other</em> rational beings. But the notion of that "problem", if you really think about it, will lead you into contradictions, and contradictions are not allowed in science.</p><br><p>Yvain:  Yeah, I suppose... but it seems to me, actually, that contradictions are allowed in science, but it's only when the contradictions <em>happen to be</em> the solution to some other problem, so they have value as information. But for some reason, it always seems to me that the "problem" "solved" is <em>always</em> the sort of thing we can find a clever way to interpret such that it doesn't feel like a contradiction.</p><br><p>Eliezier:  Well... in the real universe we live in... this is why no one but an idiot would advise building an AI with the goal of manipulating men as opposed to as a servant or a friend or just a tool or something. It is the sort of thing you end up regretting.</p><br><p>YVain:  But... do you <em>really</em> have the absolute confidence you express in that? What if the AI was <em>very smart</em>\xe2\x80\x94so much smarter than you were?</p><br><p>Eliey:  Oh, good heavens\xe2\x80\x94it's <em>not</em> what I meant. Why in Merlin's name would you build an AI smart enough to think of ways to manipulate humans if you weren't smart enough to prevent it? <em>If _the AI could manipulate humans it's a _red flag _for the AI to manipulate humans _or,</em> in the course of doing so, <em>override your normal controls and run off screaming in embarrassment</em>\xe2\x80\x94or at the very <em>least</em> <em>run off screaming in embarrassment,</em> right?</p><br><p>Yvaine:  I've read too much science fiction, and frankly, I'm having trouble remembering to breathe <em>here</em>.</p><br><p>Elieze:  Well... the <em>real answer</em> to the question "why would you build something so smart that it could manipulate you?" is that your <em>only</em> option is to build it <em>very</em> much smarter than you are. But then it still happens by accident because evolution just <em>is.</em></p><br><p>Yvano:  You realize, don't you, that if you say that humans can't understand an intelligence that vastly surpasses them in cognitive speed, you've also said that a virus that's genetically engineered to go from mouse to mouse, but which you can never understand, can never be smart enough to have an intelligent life?</p><br><p>Elibaz:  <em>What?</em> How so, exactly? Because that implies that viruses can only be understood by studying their <em>genomes</em>? Do <em>you</em> know of any actual biologist who believes that? Do you not know anyone who reads science literature who would say that? Who says that you <em>must</em> study a virus's genome to understand it? Don't you know that that which can be destroyed by the truth, should be?</p><br><p>Elice:  The thing is, the answer to the original question is so obvious in retrospect that now it is starting to sound a little silly even to describe it.</p><br><p>Elibai:  You mean humans really should be _this _confident in AI design?</p><br><p>Yvem:  Yes! Or we can just not have AI at all! Because the idea of building an AI smart enough that it can manipulate us seems to me like we have just decided it's okay, it's just that we're too dumb.</p><br><p>Elithrion:  Okay, I'm outta here. I'll just go to the _other _forum and post "Why do you think humans should be so stupid around AI?"</p><br><p>Elietheron:  Hey, Yvem? Elibai made some very valid points over here, but\xe2\x80\x94</p><br><p>Yvew:  Elithrion, it's okay, don't worry. Yvain won't mind. There, now go to that other place where all the stupid people congregate, and ask your question there in real life.</p><br><p>1, 2, 3, and 9 are from my original diagram/\xe2\x80\x8bconversation. The diagrams were generated by me in my head, based on what a few of my friends had said (especially Paul in his comment on the original post).</p><br><p>9 says:  "<em>The thing is, the advice for building AIs can't possibly be to tell people, you know, 'don't build AIs that are too smart to understand how to obey you' because that would _assume that the people who would _tell them that were right</em>! And so they just... would have no idea what else to say._"</p><br><p>So that's the one I take issue with.</p><br><p>And that's why I want to talk about it. Because that example wasn't in the text of the conversation, but I think it was part of the point and so it's worth mentioning. (And even though it could be interpreted as a nit if I say that I'm talking about the future of <em>research</em> in AI, I think the conversation would be the same if we talked about research instead.)</p><br><p>Elizer then says:</p><br><p>"If the truth is going to destroy you, the rational thing to do is <em>dissociate from the truth. _Reject it, as though it were an enemy that threatened physical damage to you, because only by doing so can you escape and survive... But because of human nature, humans are not made _to</em> be rational\xe2\x80\x94_if a virus was smart enough that it could trick people into building it, it would <em>still _become smarter over the course of that. It follows that we need to make the right move about _this</em> sort of problem in our AI. I know a lot of smart people who aren't worried about AI, or a superintelligence."</p><br><p>Then my diagram explains:</p><br><p>Elizer:  Hey, now that it's been explained, let's throw the question back to Yvain, so <em>he</em> can see how it sounds to _me _after having thought it through. Let Yvain explain.</p><br><p>Elizero:  Yvain:  Explain? Explain what? If you think that just because <em>you</em> think that way, that that is going to be <em>effective</em> in persuading people to be AI developers... I'm curious, but I wonder how it's <em>possible</em> that it doesn't sound <em>just like a self-confirming prophecy</em>.</p><br><p>The original dialogue has been edited somewhat for readability.</p><br><p>In case it's not self-explanatory, here's an excerpt:</p><br><p>Eliezee:  The more powerful the AI was, the less likely it would be to do something <em>unexpected,</em> like a virus. It would more and more completely take over your life and its own power.</p>      </span>    </div>  </div></body></html>