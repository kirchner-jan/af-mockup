<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        How many transistors is a human brain?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Carl Shulma</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>There is much discussion on this in the effective altruism  (EA) literature with varying conclusions. For instance, there is debate around whether there is a significant computational cost involved in "merely" increasing the number of transistors within a brain. If this were false, this would be unlikely to be relevant in discussions of AI timelines, and probably AI timelines would be driven by AI-specific considerations. Some authors consider the total number of operations performed by the cortex per second of brain time to be extremely small (on the order of a million). On the other hand, Eliezer Yudkowsky and Michael Anissimov (and perhaps others) consider it too plausible that much of the brain's computational power could be performed in such a way as to require only that many transistors per second.    In this essay I will try to summarize the debate which I believe, on the balance of plausibility, is about equally balanced due to the fact that all sides rely on the same fundamental argument, namely that we have good reason to think that there is some kind of exponential or sublinear relationship between the number of transistor counts per second in a particular region, and the volume of that region.    The discussion has been so scattered that it is difficult to even name the most prominent relevant references, and I am not even sure how to categorize them (I suspect I would be better off using words like 'EA' and 'biological evolution'.) I have not even gone through and attempted to classify each of the dozens of papers available in Google Scholar, which makes trying to do this exercise even more of a daunting task:</p><br><p>I believe the basic argument for the importance of  this issue is found in the following two papers:</p><br><p>An estimate of the total transistors in human brains by Ben Garfinkel and Michael A. Horowitz, 2015  </p><br><p>A Neural-inspired AI Timelines Scenarios and Their Related Empirical Results by Ben Garfinke and Michael A. Horovitz, 2016  </p><br><p>Unfortunately these papers cite only a single other relevant paper, which unfortunately cites only a single other paper which cites a collection of several other relevant papers, and which cites only one other relevant paper, and all of which cite another piece which cites a paper which cites one other relevant paper which cites a piece which cites a relevant article which cites... </p><br><p>The best reference I have come across is <em>Nonspherical Computation__ (NSC)</em>, published in 2005, by Charles Hulme and Dan H. Jonsson. They use this single reference to argue that while  the claim that the computational power of biological brains, as a whole, could not plausibly be sustained in a computer as described here is technically true, the idea that we can compare this to the computational power of a computer is absurd. NSC suggest that the computational power, on the other hand, <em>of any particular region</em> within a brain is so small that it is irrelevant in terms of <em>how long it would take to simulate</em>. They then cite a further paper claiming that their method works for both neurons and astrocytes.</p><br><p>They suggest that the method can be used to calculate, at least partially, the expected performance of a particular brain model using a "minimum compute estimate", which is simply the time taken by that model to run on the minimum set of required elements of that brain. Their estimate of the size of this minimum set varies considerably, but their analysis suggests that our estimates for the time required to emulate the performance in question vary between 4 days and several years. They also report on how to calculate an estimate of the minimum transistor density in "human gray matter".</p><br><p>They make no quantitative estimates of the time from brain size to brain size variations, and no quantitative estimates of computational power per volume, and their analysis depends a lot on what volume you use to calculate the minimum compute estimate, which they don't discuss in sufficient detail. They also give no indication of how sensitive this estimate is to that choice. NSC also don't seem to mention the existence of <em>nonspherical computation</em> (NSC), which was originally published as <em>The Brain's Building Blocks</em> in 2001. In the paper cited there, they also suggested that the minimum compute time could be in the range of 1 to 100 days, but this was clearly not a serious attempt to estimate a minimum. The only relevant paper I could find that cites the paper cited here is by Steve Byrnes in <em>Neurons: A Physical Model</em>.</p><br><p>The relevant claim of these three papers is that:</p><br><ul>
<li>We have ample indirect but very weak evidence in support of the claim that the brain's computational efficiency is sub-linear, and we don't have any reason to believe in a brain efficiency which isn't sub-linear. As a first-order approximation, we can estimate the minimum computation needed to emulate the brain as described here as follows: multiply together the neuron count estimated by NSC and the total number of transistors.</li>
</ul><br><ul>
<li>We have some weak indirect evidence that brain's computational efficiency, as a whole is sub-linear. In the paper I am citing, Steve Byrnes argues for the importance of the argument for a sub-linear relationship between brain efficiency and transistor density. It seems that sub-linear brains and super brains are possible and that there is at least one known reason to expect their computation to be sub-linear.</li>
</ul><br><ul>
<li>This is an important, although not yet well understood, topic from several perspectives. To the extent that other AI researchers are familiar with the research in this area, this is relevant to AI timelines. AI researchers who accept the view that only computational efficiency matters, and that only computation bounded agents  can be aligned, would probably assign a correspondingly small probability to the claim that this is what makes humans so different from other animals, and they might conclude that it is less important for them to be concerned about this particular source of variance in what makes humans so very unlike other animals. (For the purposes of the present conversation, however, it is only the difference of the last sentence that I am most interested in.)</li>
</ul><br><p>It can be expected that some people will think that one should use either of these two sources in addition to this one to get an even lower estimate of the possible computational power of the whole brain, if one can't just go by number of transistors or neuron count. Thus, I feel compelled to review the arguments to try and determine how they are consistent, and how they are inconsistent.</p><br><p>In general, it seems that NSC make a fairly clean case for a sub-exponential relationship between the  number of transistors per volume in the brain, and the brain's time  required to complete a task with that brain, and at first blush, one would hope this would be a very straightforward question if one had access to <em>NSC</em>. So it can be quite a bit of a disappointment to realize that some of the papers citing the reference they cite don't actually make an argument for sub-linearity even once you take into account the fact that they are not evaluating that claim, nor are they even aware of the existence of such a possibility. In order to help me sort through these issues, I have divided my initial response to this problem into three sections each in order to try and identify some of its central disagreements.</p><br><p><strong>1</strong> Many of the papers I have found which reference NSC (listed above) actually cite a further paper that does have an argument for sublinearity, but it is not nearly as clear and well supported as the argument in NSC. I have also been able to find some additional papers which argue that <em>the</em> <em>relevant measure is the total time spent on a task</em> and the sublinearity of this time spent <em>is sufficient to explain much of how brains outperform computers</em>. Unfortunately none of them provide more details of their reasoning or calculations. One of the papers cited by NSC lists a paper as supporting a similar claim, but NSC do not cite this. Another paper lists a number of reasons that this is plausible, but does not cite a paper that addresses any of them in sufficient detail, and a third cites a paper which lists a number of arguments for sublinearity of the relevant measure, but that paper mostly consists of citations of other papers that cite NSC and are discussing various aspects of the claim that NSC makes. </p><br><p><strong>2</strong> The more basic and obvious point is that the brain uses energy to do computation, and it is inefficient to do so by having to transport electricity through the neurons and the interconnections of neurons. In general, we know that biological brains are orders of magnitude less efficient than digital computers. As is well known, it matters very little, in such terms, whether the time to complete a task is linear or otherwise, for if the cost is constant, then we expect the time taken to complete a task to grow in proportion to that cost. Thus, one can only gain from a better computational efficiency <em>at a lower energy cost</em> via more efficient computer architectures and lower clock speeds.</p><br><p>Thus, if we are talking about the performance of a brain under the constraint that it be completed within a human lifetime, then there are clearly strong incentives to have the computation take place as a very large number of relatively short discrete steps to minimize energy usage.</p>      </span>    </div>  </div></body></html>