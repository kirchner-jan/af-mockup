<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        he Alignment Problem Explained      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">John Salvatie</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>The Problem:</li>
</ul><br><ul>
<li>The Problem Is The Solution:</li>
</ul><br><ul>
<li>Not An Unimportant Concept Problem:</li>
</ul><br><ul>
<li>Why Does He Alignment Matter?</li>
</ul><br><ul>
<li>Why The Problem Matters:</li>
</ul><br><ul>
<li>Related Problems:</li>
</ul><br><ul>
<li>What I Learned So Far:</li>
</ul><br><p>Summary: Most people working in AI alignment are trying to solve a concept problem instead of a value alignment problem. This explains why people have so much trouble coming to agreement on definitions of the word "alignment".  (In general, the solution to a concept problem is the introduction of new words/\xe2\x80\x8bconcepts, and the solution of this alignment problem is the creation of a novel concept called "corrigibility".)  This is a particularly important problem because it affects how we should conceptualize our work, and what it will look like when we are finally solving this problem. This essay was inspired by reading various discussions on the forum Alignment Forum, and my own frustration at the lack of clarity on "what's the deal with aligning AI with humans and how do you know if you succeeded?"  I hope that this essay will help clarify the conceptual problem, and provide both clarity and direction in answering the actual important question of "How do we get this new concept of corrigibility to actually work?"</p><br><p>This essay is also crossposted on my blog.</p><br><p>The Problem:</p><br><p>What is the problem with the current way people think about "aligning AI with humans?"  People are making an implicit assumption that AI is a "tool" that needs to be "slightly modified" before being deployed for a particular purpose. This assumption means that people who disagree (or who want to change the way the AI works) don't have a concept of how to define which modifications are which. As a result, we are in the unusual situation where disagreements about a concept are more important than disagreements about its solution/\xe2\x80\x8btheory/\xe2\x80\x8bproposed implementations.</p><br><p>In short: most people who work in AI alignment think that the problem is the tooling problem.</p><br><p>The problem is not that the AI alignment is impossible in principle: most people working in the area admit that an Oracle AI that is aligned with its user at some arbitrary level of sophistication and that has a bounded computing budget would be perfectly "alignable".  This would allow it to be used in any given situation, without the need of recursively self improving and self modifying.  (More on this in The Problem Is The Problem!).</p><br><p>And most people realize that it would be easy to prevent this form of AI from being misused: it's just a matter of specifying a tool that is safe for use on any given task.  (More in the Alignment Newsletter #121).</p><br><p>The Problem Is The Solution::</p><br><p>What I'm confused about is why some people (myself included) seem to have a different view of the problem, one that appears to involve conceptual confusions that are causing severe barriers in the way of working together.</p><br><p>It may be that these people don't believe that tool alignment is actually feasible, and we disagree because of this belief about the nature of AI-corrigibility.</p><br><p>But I can also think of at least two other more plausible reasons for this disagreement:</p><br><p>If we both agree that a safe tool is possible, then why don't we immediately begin to work on tool-alignment?</p><br><p>Why is the tool-alignment problem important?</p><br><p>These issues may be very difficult to resolve, and it may be that different people are failing to see different aspects of the problem for different reasons. I'm also not sure how to resolve these issues so I leave it to future essays to describe how to approach these problems.</p><br><p>It seems useful to begin with a brief overview of each of the major problems that seem to be at odds with this concept of "alignment" (from here, for some reason I'm not sure why "reward-alignment" is still a controversial concept).</p><br><ul>
<li><strong>Tool AI</strong>: Tool AI is a proposed concept where an autonomous AI is used as a tool by a human engineer for some task. Most people who work in this area don't believe that this concept is possible or even a serious issue.</li>
</ul><br><ul>
<li><strong>Problem 1: Tool AI would not work for almost all jobs that humans do</strong>: Many jobs involve tasks that are too complicated relative to human competence to be performable with Tool AI given arbitrary modifications to the human.</li>
</ul><br><ul>
<li><strong>Problems 2-8</strong>: Most tasks are simple enough to be performed by human workers modified with tool tools.</li>
</ul><br><p>So here are two problems:</p><br><ul>
<li>__Problem 2: Tool AI does not solve the problem of making AI capable of being used "safely" for any given task. __I think most people working in this area would agree that Tool AI is insufficient to solve this problem</li>
</ul><br><ul>
<li>__Problem 3: Tool AI does solve the problem of tooling AI. __I think this is one of the most important concepts to understand and it is a concept that is largely ignored by people working on AI alignment or Tool AI.</li>
</ul><br><p>If you agree with either problem, and believe that all other disagreement comes from people thinking for different reasons (the first of which I explain in more detail), then I think it makes it way more likely that there is some conceptual conflict here. If there are two people who think independently of each other, and they both have conceptual disagreements that are causing major problems for them and others who work in this field, then it seems likely that there is a real problem here.</p><br><p>I'm going to describe two different intuitions as to why Tool AI wouldn't solve most jobs we do, and I think that one of the issues here is that people have conflicting intuitions about this. Here is my attempt at describing the two intuitions in more detail.</p><br><p><strong>The Problem With Tool AI:</strong></p><br><p>One possible intuition is to think about the complexity of a task, and try to estimate how long it would then take for a human to accomplish it using Tool AI. I think this is a reasonable intuition, and that most people working in Tool AI have this intuition. This intuition (in combination with the above argument that Tool AI doesn't work for most jobs) suggests to us that Tool AI would never be able to solve most jobs.  (As I'm sure you've noticed, this isn't the actual intuition I'm talking about with respect to "Tool AI", it's just an intuition I have about Tool AI)</p><br><p><strong>The Alignment Problem:</strong></p><br><p>The other intuition comes from the intuition that any given task can be "solved" by a human working very fast and/\xe2\x80\x8bor with arbitrarily complex modifications to the initial condition of the AI (which are likely to be arbitrarily complex).  It seems likely that many tasks do have properties that make this intuition more or less likely. This intuition suggests that we can solve any given task, that the real key difficulty with the Tool AI concept relates to the fact that its solving of some tasks does not imply its usefulness for other tasks.</p><br><p>If I'm completely off base in my model and the Tool AI people are really thinking like this (which seems likely to be the case considering everyone involved in my model, including myself, haven't worked on and don't believe in the idea that Tool AI can solve most jobs even if you believe in its theoretical possibility), then I'm interested in the opinions of people who work in Tool AI on this intuition. It feels like a conceptual dispute to me, but I'm not sure what is causing this.I'm also open to other people's criticisms and arguments of this kind. If it turns out that people really think this way, then we have an important problem.</p><br><p>Note: I should mention that I'm sure Tool AI could actually be used for many jobs, and this certainly isn't the main focus of the researchers working in Tool AI. I'm also open to the possibility that I'm not understanding correctly what people who are working in Tool AI are trying to accomplish (what would that even mean?)  I'm just talking about two existing intuitions that I have (probably shared by many others working in AI aligned) and that seem to me to be at odds.</p><br><p>Not An Unimportant Conceptual Problem:</p><br><p>As far as I can see there is more than one version of this conceptual problem:</p><br><p>What I mean to say is that the tool-alting problem seems central. What I was really confused about was why my two intuitions (which came from the arguments presented in Problems 2-8) seemed to be causing so many conceptual problems in the world.</p><br><p>Why Does He Alignment(1)</p><br><p>Why I believe this problem is important is that it seems like the only way to "hack" the AI to not kill the users of the system is not to modify the AI (or at least not to a significant extent) before it's being deployed. If I believe this (and I actually don't feel very certain of it myself), then I think people who take seriously the idea that a "correct" alignment solution can be found have essentially two options:</p><br><ul>
<li>Accept this as basically the definition of "tool", in which case there is essentially a major conceptual problem ahead for any attempts at alignment (tool AI's)</li>
</ul>      </span>    </div>  </div></body></html>