<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        My Understanding of Eliezer Yudeokowsky\'s Concept of Intelligence      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Elinor Skooglar</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>This essay is one that I've read and thought about for a long time, so please treat my summary as one that may not be fully accurate. I have been told to quote the most important paragraphs, so here are those in my summary:</p><br><p>It's a common idea that an AI's understanding of the situation is limited if it didn't know things before it had the time to understand them. But it seems to me like if a machine is capable of understanding (even vaguely, in some fashion) why it needs to follow some code or why it needs to use the next piece of information to know how to use the next one before it can even contemplate all the parts of the code, the need for the rest of the code is probably not significant to the machine's understanding of the problem. If the machine has the time to contemplate all the parts and their interrelations\xe2\x80\x94i.e. if the machine can model those parts and their interactions, and can predict what will happen if different variants of each part are used\xe2\x80\x94before it gets stuck into the actual task of following the code, then the actual task is probably very much less difficult. (This is what I used to call "the machine would understand on sight" when it was a real possibility- if you can understand something even before you can do it, why not?)</p><br><p>The idea that an AI with low "comprehensive" understanding of a situation can still perform tasks well enough to make progress or even "win" an AI's motivation is something that puzzles me. It seems that the machine has to have some sort of understanding of the task's parameters, to be able to decide to use more computation to accomplish the task, and this may even imply some sort of understanding at a lower level (i.e. the machine's motivation may be a high level task, with most of the details coming down to how to get from motivation to actual physical movement, which may be very important without being interesting to the computer), and it seems that the machine's understanding has to extend to the level of understanding of the code it is running in order to be able to realize that it could improve its understanding by executing parts of the code differently. And if this sort of real understanding isn't achieved before it's been trained to do its task, that training might interfere with its understanding of how things are (i.e the model "cannot yet conceive the fact that the cat it sees is a cat") and may even interfere with the actual task if it has some sort of understanding it's trying to do as part of its task. The idea is confusing\xe2\x80\x94and it is one that I am not sure how to think about or if there's a good term for\xe2\x80\x94but it seems to me that an intelligent AI might understand the task well enough to "win" without understanding the task at a level of comprehensiveness that includes its motivation. (i.e, that level of understanding might even be impossible to achieve in the situation it is being trained in)</p><br><p>The way I currently see it, there are three levels of understanding. The level of understanding I am referring to is the level of understanding needed to perform well on the task in question. If the machine can understand enough about the task to decide exactly what to do and why; if it can understand the task at the level of comprehensiveness which includes its motivation for using that amount of computation (not excluding the task); before it even begins to contemplate the task of accomplishing it; but has not been trained at even the lower level of learning to understand the code it is using to complete the task; there are apparently some situations where the machine could understand that situation well enough to "invent" a strategy for approaching it. The third level of understanding requires a model of how the software system reacts to the task under different inputs. I am not totally sure I understand how this level is achieved either: my best guess is that it either consists of many more levels of understanding at lower-level tasks, or that the process of achieving it includes the ability to understand the task from the lower levels as the higher levels are learned.</p>      </span>    </div>  </div></body></html>