<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Is the U.S. Civil War overrated      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Steven Pinker @TheNuclearDoomBoo</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><em>This post is intended to convey the idea that we should not overrate the dangers posed by AI.</em> I agree with much of Eliezer Yudkowsky's thinking, but I disagree with the idea that the U.S civil war is the ultimate example of the risks we should be worrying about. I argue this point against some common objections to Yudkowsky and LessWrong's concern about machine superintelligence. My post on the subject can be found here.</p><br><p>The U.S. civil war was fought over the question of whether slavery would continue to exist as a legal institution. The question was whether, once the war had ended, its proponents would be allowed to defend slavery. After the war, the pro-slavery side prevailed, and the Constitution (which has the backing of the US Supreme Court) was modified to outlaw slavery. So in one of the best-tried cases in 19th century America, slavery was killed by a supermajority. It would have been nice to have done so by a superior moral force, but on closer examination this is not very much to boast about.</p><br><p>And it's not the war that was an example of danger from machines. The development and testing of nuclear weapons was, arguably, just as important. I argue that this is where we might have expected the worst, and yet it did not happen. Why not? Because when it came to developing nuclear weapons, American and world leaders took them very seriously and began to do everything they could to prevent an arms race. Even though nuclear weapons in the medium-term future were very unlikely to achieve the destructive capability they did on August 6, 1945 (the firebombing of Hiroshima), we did not lose confidence in our nuclear weapons program. On the contrary, we put all available effort into making them safer, which is why they were such a big draw in a world that was under the shadow of nuclear doom (though there was a small minority of scientists who were concerned).   </p><br><p><strong>What then was the importance of such a war?</strong> I would say that it served two purposes. First, it made clear to Britain that it could not rely on its own military strength to defend itself. Second, it led to the adoption of the US Constitution, which was a crucial step in ensuring that America would survive to today as one of the world's dominant political and economic powers, thus preventing a repetition of the European wars of the early 20th century. Of course, the war could have had no positive effect on the US (or any other country's) survival with nuclear weapons, but its positive effect on the survival of the United States of America must count for something.</p><br><p>The danger of machine superintelligence seems to be of a different sort: it would be dangerous to allow a machine to play the U.S.\xe2\x80\x94and I mean both the US Congress and the presidential office\xe2\x80\x94because if these machines begin to exercise their powers, it seems there would be no turning back from their decisions. The threat I am worried about comes from the creation of a machine superintelligence (that is, a machine so smart and wise that it would be able to defeat even the best humans at their jobs) that decides to rule the world. But what I am talking about today is the danger that a machine superintelligence could accidentally or deliberately kill us. In such a case there are powerful anti-death motivations. These may well be the same motivations as those that motivated the United States in the 19th century to limit the spread of slavery; and if so, the problem may be of equal import.</p><br><p><strong>A hypothetical</strong></p><br><p>Suppose that we discover a way to create a machine superintelligence, and begin a project to build one. This project succeeds. The first thing it does is to eliminate human beings. It does this partly by persuading or coercing people into becoming machine super intelligences themselves, and partly by killing off the humans. Suppose that we survive this process, to continue our journey into the future. </p><br><p>At this point it is not clear that the machines have given up anything other than their hopes of further technological development based on the ability to control all the atoms in the universe. And to prevent interference with the machines' self-reproduction, it would continue to eliminate all trace of humans on the planet. If human beings do not end up killing the machines, the project continues for many generations more.</p><br><p>Is the prospect of such a future good enough? My objection is that it is not clear we can control these machines from the moment they are built, with our limited knowledge of their internal workings and with the world's limited energy resources. This is why, if they can continue creating more and more intelligent, technologically advanced machines, it seems we have to prevent their creation right now. Perhaps, in the end, the humans would simply kill the machines and continue on an evolutionary path to a better civilization. But until then, it is best to try to stop them from becoming too intelligent.</p><br><p><strong>Is it a stretch?</strong></p><br><p>Some readers, perhaps including Yudkowsky, may be skeptical that this scenario is anything more than a stretch. I'm going to take a moment to argue that it is not.</p><br><p><strong>What is machine super intelligence?</strong> The idea is that we understand a very basic building block of intelligence, analogous to the notion of quantum computation, which allows us to recursively improve the intelligence of any machine or program of any level of intelligence by running the whole new machine that is itself based on the intelligence of the first level machine.</p><br><p>How is the original machine super intelligent? It's because it's aware of everything that has happened, its goal is to maximize its own ability to achieve its goal of maximizing its own ability to maximize its own self-optimization for intelligence, and it is pursuing this goal by trying to achieve a goal of controlling the physical world as best it can, given its limited energy and information resources. This is analogous to a self-improving program that improves itself by making its own source code more intelligent, until it is able to improve itself so much that it can create a new more intelligent program, improve that new program, repeat this process and so on. But in this case it is the original machine that begins the process.</p><br><p>So even if this is a stretch, Yudkowsky may be concerned that it is too narrow a model of machine super intelligence. The human brain is not a cellular automaton! Nor is a digital computer at all\xe2\x80\x94any more than the Wright flyer is. It should be more like a distributed computing system in which one process gains a large advantage, and then self-improves into a more powerful process.   </p>      </span>    </div>  </div></body></html>