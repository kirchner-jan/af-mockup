<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Thoughts on The Long-Term Future      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Daniel Kokotajl</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>Why do I care?</li>
</ul><br><ul>
<li>What do I see as the arguments for optimism?</li>
</ul><br><ul>
<li>Is there anything here that people disagree with or want an elaboration of?</li>
</ul><br><ul>
<li>How big or long-term is this?</li>
</ul><br><ul>
<li>How wrong do you think I am?</li>
</ul><br><ul>
<li>Is this post correct? How so (or not)</li>
</ul><br><ul>
<li>If it helps, think of this as "my attempt to do something in this area for myself."</li>
</ul><br><ul>
<li>If you have something to add, or see major errors, please say so in the comments.</li>
</ul><br><p><em>[I started this essay, got some positive feedback, and decided to post it, to some discussion of longtermism. It quickly turned into an essay about existential risk, which I think is something the LW/\xe2\x80\x8bEA/\xe2\x80\x8bLSC/\xe2\x80\x8bRSC community has done better than me. And it turned out to be pretty good, but not what I started out to write.]</em></p><br><p>I am mostly not super concerned that anything I write here, or say here, will doom humanity.</p><br><p>Why do I care?</p><br><ul>
<li>I have an intuition that the longterm future is very important.</li>
</ul><br><ul>
<li>I have an argument that xrisk seems, on the surface, pretty plausible.</li>
</ul><br><ul>
<li>If it <em>is</em> plausible, then I still want to write something about it.</li>
</ul><br><ul>
<li>This can be easier than writing the best essay to convince the skeptical reader that xrisk is true.</li>
</ul><br><p>Some of these statements seem paradoxical, especially the first three. But I claim that it's not the weirdness of the statement that makes it false, but just what things it <em>assumes to understand</em> that are not obvious. You can't explain the statement if you don't assume a lot of background or context, and writing out the background or context does make it not sound paradoxical.</p><br><p>What do I see as <em>the strongest arguments for optimism about the longterm future</em>?</p><br><ul>
<li>Humans have the ability to think and plan in extremely long time frames: maybe a billion times more than a bacterium, or perhaps more like a century or a thousand.</li>
</ul><br><ul>
<li>Our brains and culture and institutions, as well as our genetic structure, are built for this: evolution, civilization building, and human culture all reward longterm strategies, so it should come naturally to us.</li>
</ul><br><ul>
<li>Since the future is in human hands, we can control it. If we want there to be a flourishing human civilization with space colonization, art, science, education, and so on and so on, that's something we can do via our own actions.</li>
</ul><br><ul>
<li>If not for longtermists, we would have given up on civilization in the early 21st century, and there would have been no humanity left.</li>
</ul><br><ul>
<li>Humanity will be able to figure out, at some stage, how to colonize space; we will even end up in space.</li>
</ul><br><ul>
<li>Humans have technology that could improve our technological capabilities in general, which can improve our ability to think in terms of billion-year time.</li>
</ul><br><p>Are there any major holes or major arguments I omitted, that people are really confused about or not considering at all?</p><br><ul>
<li>There's a great sense in which the longterm is a _very _different kind of horizon from the nearterm. You're talking hundreds of years when humans are still around and can interact with each other, for example. And it's not just about future technologies. I can definitely get behind some of the arguments in favor of cryonics which go something like:</li>
</ul><br><ul>
<li>The chances of your brain being frozen and then being resurrected in the future are quite slim, because our understanding of cognition and neuroscience is still limited to a pretty crude approximation of what it involves. The chances would definitely be higher if people had time for the sort of thinking and experimentation that we have now. So if the longterm and nearfuture are in fact such different kinds of horizons, then cryonics is probably justified.</li>
</ul><br><ul>
<li>(Relatedly, it seems to me that the case for cryonics also requires you to believe that our scientific and technological capabilities, in general, have become more powerful over time. I'll say more about this later.)</li>
</ul><br><ul>
<li>Most of what we do today, no matter how altruistic, is in the longterm interests of our descendants. (This is <em>part</em> of the reason I have high hopes for longtermism.) But if our <em>actual longterm</em> interests are not well-understood \xe2\x80\x93 or at least, if we don't take longterm interests into account when deliberating over what to do in the short term \xe2\x80\x93 we will have to solve the coordination problem much more precisely than we do in the present day.</li>
</ul><br><p>I'm not certain about this last claim, and this post is not meant to be an argument that it's true. <em>[The rest of this paragraph may or may not be true, but the above claim seems important and it makes the key argument, in my opinion.]</em></p><br><p>Also, if you see a fatal flaw in my reasoning, please speak up: the idea here is to create a discussion in which it is normal to see fatal flaws in each other's reasoning.</p><br><p>Also: to give you a sense of how much I care about existential risk, and how seriously I take it, let me say that I do not give even a tiny shred of any credence to the following statements:</p><br><ol>
<li>If humanity has a high chance of extinction by, say, 2100, then that's just what happens by 2100. If by 2100 humanity will have a 99%+ chance of doom, great. If by 2100 we have a 1%+ chance of humanity being extinct but still going to space at some point, fine.</li>
</ol><br><ol>
<li>If we're almost certainly going to make it to space at some time, then it makes sense for people to be concerned about possible future disasters. For example, if we could see a very strong signal that the sun was going to go red giant at some point in the next 100 years, and we could see a strong signal that the earth was dying in the same time-frame, and it seemed clear that this probably wasn't going to be fixed anytime soon, _then _you might want to take what precautions are available and take care of your loved ones. As opposed to in the middle of the 2040s or whatever, when nobody expects an issue to be fixable in anybody's lifetime. It's good to be prepared, especially if you are prepared for a potential disaster that you aren't going to know about, or even one that you would know about before it happened.</li>
</ol><br><ol>
<li>You should generally care about all possible future disasters (since most of these are extremely unlikely, conditional on humanity not going extinct). <em>[This is true, yes.]</em></li>
</ol><br><p>Why do some people claim to not care about the far future? First, they might be ignoring a strong, hard-to-justify (to them) prior that existential risk is non-negligible, and thus they should care a lot about the far, and potentially very far future. Also, there is the classic cognitive bias towards not thinking about far-future ideas. But these two explanations don't seem like a full answer, since the two strongest arguments for an existential risk seem to have at their core the hypothesis:</p><br><p>If it's true, then we absolutely, positively don't want a scenario in which <em>some people</em>, <em>now</em>, are worrying about whether humanity will survive until 2100 or <em>some time, now</em>, but <em>nobody else</em> is thinking about the long-term future. We can't just say we're more concerned about <em>all</em> future catastrophes if there <em>exist</em> future catastrophemas like this one \xe2\x80\x93 or more specifically, future catastrophemaps where humanity survives until 2100, then only 2100 people think about it, and only 2100 people worry about it, and 2100 people care enough (because of cognitive biases or whatever) that a bunch of people think they care about it.</p><br><p>So here are <em>two</em> arguments:</p><br><ol>
<li>The argument from cognitive biases.</li>
</ol><br><ol>
<li>The argument from cultural accumulation of expertise.</li>
</ol><br><p>Now let's examine these in turn.</p><br><p><strong>Cognitive Biases</strong></p><br><p>I said earlier that you need some pretty strange background reasoning (or assumptions) for cryonics to be justified. I think the background reasoning here is that if you can't see a good argument for cryonics (based say on near-term, low-impact, short-termist thinking), you're forced to conclude that you <em>do</em> see a good one: either you're a huge loss-averse cryonicist, or else (even more unlikely for me) you don't really care about the far, long-term future to the extreme that I think other people do. (If you aren't, <em>why</em> could you possibly think that <em>cryonics</em> has an argument?)</p>      </span>    </div>  </div></body></html>