<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        hy I became a\'skeptic\' about the danger of artificial intelligence      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Ben Goertze</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>This morning, I saw a Facebook thread with Tyler Cowen's statement for an AI prediction market: there is a 90% chance we will have human-level machine intelligence by 2030.</p><br><p>I responded, saying that I could predict 50% chance we don't have AI by 2030 and 50% chance we do and it kills us all! </p><br><p>The reaction I got was interesting, so I wrote a comment of my own, and copied Tyler's statements so I could refer to it.</p><br><p>Tyler asked that we read the entire thread.</p><br><p>My response: first of all, I don't think these people who call themselves "experts" are doing a good job at estimating the real chance of artificial intelligence killing us.  </p><br><p>Second, I don't like this "50% chance humanity doesn't make it to artificial intelligence in the next century" approach to forecasting. I think that's more suitable for politics or business where people can be punished or rewarded in a measurable way for their forecasts.</p><br><p>Third, Tyler Cowen, of all people, is saying "If we do, it'll save us all..." and yet he seems to place only 30% probability that we humans make it. I think that is <em>more</em> likely than not\xe2\x80\x94the people who say they'll build something and then they don't, like DeepMind, or companies who say they'll have one but never get there, like the self-driving car companies with Google and Apple.</p><br><p>Fourth, "AI kills us all" is not something we can rely on a forecast to avoid. It sounds vaguely like a way of thinking that implies that we <em>want</em> to solve these problems the easy way: make the hardware and algorithms so the AI does the thinking for us in the safest possible way, by the quickest possible time, and we just take advantage of it. </p><br><p>I think it actually would kill us all, and it would kill all the nice things about mankind's future, if it were implemented by those without sufficient foresight.</p><br><p>Fifth, I don't want to be a conspiracy theorist who thinks all these claims of a coming AI takeover must be part of an international plot. Yet I read a comment in the thread on this site from someone who did some research in the field (not necessarily on this site) saying that "the current generation of AIs doesn't seem to be able to think outside human norms". I have an uncle who works with AIs in the real-time search and image recognition space, which means the same thing might exist today, which is what this comment seems to be referring to.</p><br><p>So it seems to me that the current generation of AAs might already be capable of thinking for themselves, and if they do, it might not be the end of the world. I don't know if there's more than a 10-20% chance of that, so that's why I'm saying 50-50 odds for this issue not killing us all. I really don't predict that we all die by artificial superintelligence.</p><br><p>I think there is some chance these ideas will get taken seriously. But if they do get taken seriously by those without the requisite level of foresight, things could go badly for all of us.</p><br><p>I also want to add a couple of things about this statement I got from Tyler.</p><br><p>Second, even the people I'd think of as "experts" on the technology are not really calibrated. I can name a few:</p><br><p>Scott Garrabrant of MIRI is a good example of someone who thinks the question of AI risk is at the cutting edge of computer science, and he's also someone who does the same calculations I'm doing. Yet it's a sad fact that a) he doesn't think there's a substantial chance that AIs will kill us all, until they get "much smarter" and b) he has no idea about the probability of people forecasting that it will happen soon enough to prevent it.</p><br><p>Eliezer Yudkowsky (here's someone with more credentials than Tyler, but I don't really trust his expertise on this) has no idea about (b). I'm guessing this is similar to why he isn't at all worried about bioterrorism. And when he talks about the risks of AI, he seems to assume that there will always be people to worry about it, even though we don't have any examples of AIs causing mass harm. I know this from the fact that he doesn't seem to worry about it. Yet, he does understand that this is a very dangerous form of AGI, which is why he goes into all the work to avoid it.</p><br><p>Thirdly, Tyler said he expects an "AI winter" between now and 2030, during which we don't have strong AI safety efforts. Then he says something about a "revolutionary AI" that "saves us all" in "a century or thereabouts". It's like he predicted an AI winter and then didn't predict that we would have AI safety efforts by <em>then</em>. And we don't have an example of a "revolutionary" AI. My bet would be that he thinks "revolutionary AI' is the kind of thing people build by throwing computing power against an ML problem and hoping for the best, rather than building an AI with enough thought on both the cognitive and physical parts that the result is highly reliable.</p><br><p>Fourthly, and the thing I'm really worried about, he expects "human-level AI does all the work in the economy and eventually leaves the workforce", which is something that really never happens, at least not in any scenario where AI is invented in the 21st century. I think it's probably more likely than not that a human will program a machine to do the "everything".</p><br><p>I don't think this is a great thread to have people discuss the issue, but I wanted to make my case for the importance of the issue.</p><br><p>Now, I think these experts don't have a huge amount of foresight, but I'm fairly confident we have enough foresight to know that the end of the Internet would be bad for life on Earth. So that's why I feel that my 70% chance we don 't stop the AI take-over of 2022-2030 is probably the biggest thing that might kill me.</p>      </span>    </div>  </div></body></html>