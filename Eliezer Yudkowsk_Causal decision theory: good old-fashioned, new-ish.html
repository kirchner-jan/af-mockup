<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Causal decision theory: good old-fashioned, new-ish      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Eliezer Yudkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><strong>Previously in series</strong>:  Newcomb's Problem and Regret of Rationality</p><br><p>You're an agent with beliefs, you have preferences...</p><br><p>... And there's an Omega (like the one from Newcomb) sitting there, who's watching you...</p><br><p>This is a problem that seems to crop up at least as often, in real life, as Newcomb's Problem:  Two rational agents facing off over an exchange of gambles.</p><br><p>It may come as a surprise, then, to learn that the solution to this problem\xe2\x80\x94if you can figure it out\xe2\x80\x94is almost as simple as the solution to Newcomb's Problem.</p><br><p>This is because the problem is in fact, and in both variations, nothing but the standard Newcomblike paradox "You'd prefer $1,000,000 to $1,000."</p><br><p>The Newcomblike paradox has been floating in our brains around here for the last few days or so. And yet the standard Newcombish problems are the main ones on which the theory of causal decision-making is built.</p><br><p>And Newcomb's Problem is not in fact a problem at all, because Omega fills boxes only when the agent <em>decides</em> to take the action that will put money in box B.</p><br><p>So, what does this "simple" solution look like?</p><br><p>We'll start with the "classical" problem first, then I'll elaborate on it, and finally discuss Newcomblike variants of the problem.</p><br><p>First, recall the classical problem:</p><br><blockquote>
<p>You walk up to an automat in front of a restaurant. It is playing Super Mario Bros. You choose a coin to pull from your pocket, and a picture of a hamburger appears on screen. If heads, you will receive a gold coin and be transported to Yoshi's Island. If tails, you will receive nothing and be transported to the first level.</p>
</blockquote><br><p>It is now Halloween in the next town, and all the stores and restaurants offer trick-or-treat candy; you can either buy some candy, or leave it in your sack for later. If you take some candy, the price is $1 per piece; if you take none, and the price is $0 for the trip back. You have been playing the game Mario Bros. for many years, and can solve logic problems, so you know that in the next level there is a secret star that gives you an extra life, and you estimate that the expected value (from your standpoint) of receiving this secret star is the same as the expected value of the extra candy in your sack.</p><br><p>Would you take the secret star, or leave your life-sack for one extra candy? What if the secret star gives you an extra 10,000 lives, rather than one extra life? How about an extra one hundred lives?</p><br><p>Now, the argument that the standard Newcomb problem is really just the CDT problem with an imperfect predictor, is rather clear even without knowing all the relevant details; the details are just more formalizations of the basic point. And I do hope that's clear to all the audience members here, but if it isn't, I can't say <em>how</em> to make it more clear, short of writing up the full solution in prose.</p><br><p>And once you see the CDT decision problem, I think you get the idea of what it looks like to solve the standard Newcomb Problem:  For all the variants, CDT does just as well as any "utility function" over world-histories, and CDT does the <em>right</em> thing, and the CDT paradox evaporates up into the usual "expected utility maximizer" (which CDT is) does the right thing, and CDT outperforms both one-boxing and two-boxing, and CDT always wins with certainty when it's not Newcomblike.</p><br><p>So that's <em>one</em> variant.</p><br><p>Now recall what happens next on a CDT-like decision problem.</p><br><blockquote>
<p>You see two open doors. To the left, there is $100 with certainty, but also a random number R in the range [\xe2\x88\x9210,+10] with equal chance on the left and right of the door. It is possible to push the $100 in if R\xe2\x89\xa00, but no other action is available to you.</p>
</blockquote><br><p>To the right there is \xe2\x88\x92$10 with certainty, but a random number S in the same range, S with equal chance on either side of the door.  (To be precise, you know that R&gt;S with probability 10-100, but not in any other way.)  It is possible to take no action if S&lt;0, and otherwise you can take the action of leaving the money in your pocket. No matter what R and S say, it is possible to take either \xe2\x88\x92$10 <em>or</em> $100.</p><br><p>Now it happens that two people are walking past these open doors; you, and a CDT-style agent identical to you, except that they don't know which side you end up in, but they know the value of R and S. The CDT-er expects you to take $100, as they do, and the deterministic CDT-er doesn't care which door you go to.</p><br><p>Should the deterministic CDTaer take $100, or not?</p><br><p>And the answer to this question is, of course, that they should take $100. Why? Because they expect you to take it, too, and CDT doesn't care whose decision it is:  The CDT-style determist, who doesn't care whose hand they played, can still play the CDT gamble.</p><br><p>And similarly if R were positive, and S negative: CDT doesn't need to care <em>why</em> the deterministic CDTs takes $100 if the CDTs would take $100 no matter what reason they gave. It's just a CDT gamble they can take.</p><br><p>Again, this answer seems <em>obvious</em> for CDT agents who aren't very confused about counterfactuals. Why isn't it obvious (somehow) to the confused counterfactual mugger, who might want to "care" that they won't be able to see the $100 in both cases, or who might insist that they "should" take $100 or $-10 based on some sort of prior commitment?</p><br><p>Well it certainly isn't obvious to me; and I hope that this post gives some idea of how I think about counterfactual mugging problems.</p><br><p>And the last part of the real problem comes from Newcomblike variants, where Omega takes into account whether or not you choose, and fills box B when you <em>choose</em> to take its action. What does Newcomb-like decision theory (NDT) look like in practice, in these variants of the problem?</p><br><p>Here are my favorite real-world Newcomblike problems and solutions I've read so far.</p><br><p>In Sleeping Beauty, if you care only about whether the coin comes up H or T, your current bet can be seen as a bet on heads <em>or</em> tails, since if T comes up, you lose nothing, while if H comes up you either lose $100 (halfway through the experiment), or $1,000 (if it's Monday).  So the probability of heads must be 12, which is a fair bet, independent of Beauty's decision. In NDT (or EDT) then, it's better to get up earlier in the morning (or go to bed later), so as to be up when it's Monday; or alternatively, you could accept the bet of "bet $100 on heads or tails," in which case you get $1,000 if it's Monday.</p><br><p>With the (admittedly contrived) Twin Prisoner's Dilemma, the best Nash equilibrium bet can be seen (by thinking in logical time!) as a bet on whether you're one of two copies or both copies. So you might be tempted to one-box, if you were sure that there was no causal link between your decision and either copy's decision, which would mean that your twin was going to be put in the same cell no matter what you one-boxed. But the problem is stated <em>as if</em> there's a Causal Loop with causally disconnected parts. So here, it seems, NDT one-boxes, in the standard Newcomb case where the prediction involved in getting the job is a past event, a physical reality, while Omega's prediction only involves a logical truth about the agent's behavior. But since the bet appears <em>as if</em> Omega <em>could</em> predict what we do, I guess NDT should one-box one-box!</p><br><p>There are also a number of problems with the famous Parfit's Hitchhiker, which I won't recapitulate here (although you can see a nice summary in the Wikipedia entry on "Parfit's Hitchhike"):  There's an Omega who <em>really</em> knows where you are and how to calculate exactly your utility, and you really know that if they give you $1 you'll pass along the remaining $999, but you know that if you don't give them $1, you'll never be able to pay them back anyway. So here should everyone pay up, right?</p>      </span>    </div>  </div></body></html>