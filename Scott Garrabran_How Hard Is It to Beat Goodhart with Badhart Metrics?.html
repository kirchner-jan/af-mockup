<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        How Hard Is It to Beat Goodhart with Badhart Metrics?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Scott Garrabran</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>Introduction</li>
</ul><br><ul>
<li>Notation</li>
</ul><br><ul>
<li>Definitions</li>
</ul><br><ul>
<li>Clarification</li>
</ul><br><ul>
<li>Examples</li>
</ul><br><ul>
<li>Examples: Bias in Goodhart's Law</li>
</ul><br><ul>
<li>Formally Stating Bias-Variance Tradeoff</li>
</ul><br><ul>
<li>Examples: Biased Estimators</li>
</ul><br><ul>
<li>Examples: Variance Reduction</li>
</ul><br><ul>
<li>Formally stating this</li>
</ul><br><ul>
<li>Further Directions</li>
</ul><br><ul>
<li>Conclusion</li>
</ul><br><ul>
<li>Appendix</li>
</ul><br><ul>
<li>Proof Sketch</li>
</ul><br><ul>
<li>Theorem</li>
</ul><br><ul>
<li>Proof Sketch:</li>
</ul><br><ul>
<li>Proof</li>
</ul><br><ul>
<li>Proof Sketch for Theorem:</li>
</ul><br><ul>
<li>Proof Sketch Proof of Lemma:</li>
</ul><br><ul>
<li>Proof Statement</li>
</ul><br><ul>
<li>Proof Sketch of Corollary:</li>
</ul><br><ul>
<li>Proof of Corollary</li>
</ul><br><p>Introduction</p><br><p>Goodhart's law is roughly the following:</p><br><blockquote>
<p>"any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes."</p>
</blockquote><br><p>This paper will not be about the standard definition of Goodhart's law, but instead about what happens when you use Goodhart's law to formally think about problems with the following structure:</p><br><blockquote>
<p>"Observed statistical regularity A will tend to break when being optimized for B."</p>
</blockquote><br><p>Here A is some regularity in the domain, that you are trying to discover or compute, and B is some proxy A that is supposed to hold some information about A. This is not how most people talk about Goodhart's law. However it is quite natural from an optimization view. A will be more regular within some domain than others, so this could be a quite useful way to think about such problems. The standard view of Goodhart's laws talks about the optimization with respect to some metric, whereas this paper talks about the optimization against some specification of the metric, which I think could be quite reasonable in certain cases. This paper tries to make things more crisp with such a framework.</p><br><p>Notation</p><br><p>Given X some set, we have functions f:X\xe2\x86\x92[0,1] and g:[0,1]\xe2\x86\x92X defined as f(x)=1if x=x0 otherwise. We say g is a <strong>metrics</strong> on X, if g(x)\xe2\x89\xa4f(x) for all x\xe2\x88\x88X.</p><br><p>Given X some domain, X\xe2\x80\xb2\xe2\x8a\x82X some subset of points in X, and f:X\xe2\x86\x92R a function from those points in X to real numbers, f defines by  \xcc\x84 \xcc\x84  \xcc\x84m:= \xcc\x84 \xcd\x9dx\xe2\x88\x88X\xe2\x80\xb2f(x)\xe2\x88\x9d(m(x)\xe2\x88\x92R)2 for R as large as possible.</p><br><p>I.e you optimize m with respect to f, and use this to measure how far from that optimal value is g, and then optimize g to be as close as possible to a metric within this set of points. Then you rescale so that the optimal value is at 0 and the actual value is at 1. You could imagine that the optimizer is using this as a formal way to find a good approximation of the best metric it can on any points with the smallest differences between the value assigned by the actual metric, and the value assigned by f.</p><br><p>Definition</p><br><p>Fix some metric g,f:X\xe2\x86\x92R. We say that g is a <strong>good proxy for f</strong> when  \xcc\x84E[ \xcc\x84 \xd5\xaff(x)\xe2\x88\x92f(x)]\xe2\x9f\xb9g(x). We say that  \xcc\x84g is a __good metric __and g a __good proxy __when both hold.</p><br><p>If f,g are uniformly bounded, then this is equivalent to</p><br><p>E\xce\xbck[fk(x)\xe2\x88\x92gk(x)]\xe2\x88\x88\xce\x94.</p><br><p>Proof</p><br><p>If \xce\xbck is the distribution over x sampled from fk, and  \xcc\x84\xce\xbck is the distribution induced by gk, then</p><br><p>E\xce\xbck\xc3\x97 \xcc\x84\xce\xbcpk[(1fk(x)(1gk(x)N+1)]\xe2\x9f\xb9E\xce\xbdk[gk(x)=\xe2\x88\x92fk(x)] </p><br><p>It suffices to show that</p><br><p>E\xce\xbdk[(gk(x)-fk(x))2N2+\xe2\x88\x91x\xe2\x88\x88X(gk(y)\xe2\x88\x92fk(y))2]\xe2\x9f\xb9\xce\x94 </p><br><p>Proof Sketch</p><br><p>Let m and n be large enough that n2\xe2\x89\xa52N2. Let Y be the set of x\xe2\x88\x88X of length at most m for which, when sampled from  \xcc\x84 \xce\xbck, the difference between the gk-estimate and fk-estimate is within \xce\xb5/3 of the gk-distance. By Brouwer's fixed point theorem, there exists an x\xe2\x88\x88Y s.t. x\xe2\x86\xa6x implies that fk(x)=gk(x). Therefore, E\xce\xbdk[(fk(x)-gk(x))]2=\xe2\x88\x91x\xe2\x88\xbcX\xce\xbek(x)2\xe2\x89\xa4m2\xce\xb5k2\xe2\x89\xa4N2\xce\xb42 </p><br><p>for \xce\xb4&lt;\xce\xb5/3.  For any y\xe2\x88\x88X\xe2\x88\x96Y,</p><br><p>|(gk(zy)\xe2\x88\x92fk)(zy)|\xe2\x88\x8a|(gkp(zy)\xe2\x88\x92fp(zy))|\xe2\x89\xa4\xce\xb5 </p><br><p>where p is the projection of  \xcc\x84X onto Y. Using this property and the preceding, we get</p><br><p>E\xce\xbdpk[(fp(x)\xe2\x88\x92gp(x))]\xe2\x88\xabX|(gk\xe2\x86\x92fp)(x)|2\xce\xbdk(x)\xce\xbek(p(x))fk(x)+\xe2\x88\x91x\xe2\x88\x89Y\xce\xbek(y)\xe2\x89\xa4(\xce\xb5+N2/m)\xce\xb42  </p><br><p>Since \xce\xb3k\xe2\x86\x92\xce\xb42 when k\xe2\x86\x92\xe2\x88\x9e and fp(x)&gt;gp(x) for most x, and the second term is at most \xce\xb5, we have</p><br><p>E\xce\xbdp[(fp\xe2\x88\x92gp)2]\xe2\x88\xbc\xce\x94 </p><br><p>This works as long as there is some uniform bound on gk and \xce\xbck.</p><br><p>Proof</p><br><p>Since gk is uniformly bounded, for any y\xe2\x88\x88Y, \xce\xbek(y)=\xce\xbek(q(y)) for some q:Y\xe2\x86\x92X. Without loss of generality assume that for some k,</p><br><p>\xe2\x88\x80y\xe2\x88\x88YPr[fk(q(x))=\xe2\x88\x83x\xe2\x80\xb2\xe2\x88\x88Y:y=q(x\xe2\x80\xb2)]&gt;\xce\xb5</p><br><p>We also assume that N2\xce\xb5&lt;1, to ensure that n2 is enough that n2&gt;2N2. Define Y\xe2\x80\xb2\xe2\x8a\x82Y by n\xe2\x89\xa5|xy|\xe2\x89\xa5nm+1, and note that Y\xe2\x80\xb2= \xcc\x84\xce\xbc\xc3\x97\xce\xbe\xe2\x88\xa9{xy:y\xe2\x88\x89Y\xe2\x80\xb2x\xe2\x88\x88Y\xe2\x80\xb2}. Thus, n is enough that there exists a point x\xe2\x80\xb2 whose k-estimate is 1 for Nn+1 of these Y\xe2\x80\xb2x\xe2\x80\xb2.  </p><br><p>We have</p><br><p>E\xce\xbd\xc3\x97\xce\xbck[(f\xe2\x88\x97kq(x)\xe2\x88\x92\xe2\x88\x91x\xe2\x80\xb2\xe2\x88\x88q\xe2\x88\x921(x)fk(x\xe2\x80\xb2))2]\xe2\x89\xa4E\xce\xbd\xc3\x97\xce\xbc\xc3\x97\xce\xbck\xc3\x972\xce\xb6k[(fQ(x)\xe2\x88\x92\xce\xbck(x))(fQ(y)\xe2\x88\x92\xce\xbckq(x\xe2\x80\xb2)<a href="f\xe2\x88\x97y">(f\xe2\x88\x97\xe2\x88\x97kq\xe2\x88\x92\xe2\x88\x91x\xe2\x88\x97\xe2\x80\xb2\xe2\x88\x88qQ(x\xe2\x88\x97\xe2\x80\xb2)fk(qk(x\xe2\x88\x97)\xe2\x88\xa8x\xe2\x88\x97\xe2\x80\xb2n\xe2\x88\x921)</a>\xe2\x88\x92\xce\xbc\xc3\x97\xce\xbc\xc3\x972\xce\xb6[(\xce\xbcQ(x)\xce\xbcpk(x)\xce\xbc\xcf\x80k(y)p(y|x\xe2\x86\x90y)\xe2\x88\x92\xce\xbcQ(x)(\xce\xbcpk(y)q(y)\xe2\x88\x92N)rk]) </p><br><p>where rk=\xce\xb7logk where \xce\xb7&lt;\xce\xb5/3, for the \xce\xb5 in the definition of good metric. Here n is large enough that all x\xe2\x88\x88Y\xe2\x80\xb2x can be written as y=q(x), which ensures that q\xe2\x88\x88Y\xe2\x80\xb2n. We will now analyze each of these term to determine the order terms as \xce\xbbk=Nn\xe2\x86\x92\xe2\x88\x9e.</p><br><p>\xe2\x88\x91x\xe2\x80\xb2&lt;n\xe2\x88\x921\xce\xbck\xc3\x97\xce\xbcq(x')((fq(x)\xe2\x89\xa0fp(x\xe2\x80\xb2)\xce\xbcpk)=\xce\xbck(x)p(x).\xce\xbck(x)(f\xe2\x88\x97\xe2\x88\x87y)\xe2\x88\x92(N\xce\xb5|x\xe2\x86\x90x)(f\xe2\x88\x8fi\xe2\x88\x88[m+1]k\xce\xbcpk(yi))rk</p><br><p>Also, the sum term in the denominator has the form \xe2\x88\x91x\xe2\x80\xb2\xe2\x88\x89Y\xe2\x80\xb2\xce\xbck(x\xe2\x88\xa9x\xe2\x80\xb2n)(f\xe2\x88\x97y\xe2\x88\x92f\xe2\x88\x97\xe2\x88\x91x\xe2\x80\xb2=n\xe2\x88\x921\xce\xbcpk(xy\xe2\x80\xb2)p(y\xe2\x80\xb2))\xce\xbcpk(q(Y\xe2\x88\xaax\xe2\x80\xb2n))rk</p><br><p>The sum in the denominator (and the last term, that sums up N\xce\xb5\xce\xb5\xe2\x88\x921 or less terms) is \xe2\x89\xa50 by the definition of Y, so the first term has its contribution to the final error going to 0. Thus it remains to analyze the sum in the numerator.</p>      </span>    </div>  </div></body></html>