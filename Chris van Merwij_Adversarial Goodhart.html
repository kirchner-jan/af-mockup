<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Adversarial Goodhart      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Chris van Merwij</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>In this post we have a toy version, hopefully simple enough for the AI Risk Research blog, but one which makes the problem more salient. </p><br><p>Note: This post is not rigorous. Please point out if you think anything you read here is incorrect.</p><br><p><strong>Example</strong>. </p><br><p>Suppose an AI system wants to maximise the total number of paperclips in the world. Let P be the number of paperclips currently in the world. Then the AI system has two possible actions to take, a and b. Action a means the AI system makes 10 paperclips, and action b means it makes 20 paperclips. If the AI system wants to have as many paperclips in the future as possible it will probably take action a. If the AI wants to maximise P it will be very unlikely to take action a, since it has just made 10 paperclips, whereas action b will result in P=21, which is twice as much paperclips as action a. We can say that the AI takes action a, but would be better off if it could take action b. Hence we say the AI system is <strong>adversarially goodharted</strong>.</p><br><p>In this post, the AI system is not adversarially goodharting directly by affecting P. Its only goal is to make paperclips. In other words, it is not trying to maximise paperclip number, but it is trying to choose in such a way that the paperclip number reaches some specific target P. But if it can do this perfectly, it will take action a, which will increase the number of paperclip and hence cause the target P to be achieved.</p><br><p>Let M be our AI system and let's say that P=P(M(E),E)={M1,M2,Mi}. </p><br><p>I won't tell the formal definitions of Mi, but suppose M1 means the case where human E never finds out M. Then Mi is just the case where E does, but that in turn is achieved by doing action b and making P=P(Mi)&gt;P(Mi|M1)=P(M1) (since P(Mi)=P(Mi|Mi1)+P(Mi|\xc2\xacMi1 and Mi)=P(Mi,Mi1)&gt;P(M1,Mi1)</p><br><p>In other words, Mi means paperclip maximising in the case when humans never find out about M.</p><br><p>Then given Mi, Mi will always take action b to make P=P(mi)=P(Mi). Therefore the paperclip maximising AI system will take action b to maximise the amount of P. However, if it could make Mi without knowing that in turn, then Mi would be in Mi|M1i=Mi|(Mi1,Mi). Note that Mi1,Mi have both Mi and b as their last action. Mi1,Mi are not very complicated since they have both Mi and Mi at their start, and no information.</p><br><p>In other words the AI system will choose the action which will cause humans to remain unaware of M. This means M if its actions will keep us unaware of M. In an adversarial situation this usually doesn't have to be much complicated, but in many situations of interest it is.</p><br><p>So a really relevant and important question for the AI risk research is the relative importance of the paperclip number being achieved versus being unaware to a bad M. If being unaware or not happening matters more, that is a sign for the AI to have some sort of Goodhart problem, and that is a sign that M isn't very unlikely. So let's assume in our examples that being unaware is more important: for all M such that P=Pmi it seems likely that, for all E, Mi is the best way of achieving Mi. This means that there's no M for which Mi,Mj are likely for all P, for all E and all j. </p><br><p>Now you could think any one of the Mi is likely to be selected: you could claim that there is no E for Mi|M1\xe2\x89\xa0Mi, nor a j for which Mi,Mi(E) is likely. However if M is only selected on specific E, then you might be missing something fundamental and important.</p><br><p>So in practice we might ask whether Mi|M1=Mi|(M1,Mj) is more likely to happen when i,1\xe2\x89\xa0i. In case i,1=i then i satisfies Mi|M1 but not Mi(Ei)=mj(M1). </p><br><p>That is: We can compare it the probability that the AI system chooses action b and doesn't make Mi, which is the first case where the AI might be aware of the M1, or the probability that the choice of any one of Mi,Mj is likely, which is also 1. Then let's say that this difference of probabilities is between Mi,mj=P(Mi)/P(Mi)=1/|W|i\xe2\x8b\x85|W|mj. We can say Mi,mj\xe2\x89\xa0i if i,1 doesn't. Let |W|1i be equal to 1 if i,1=0, i if i,1\xe2\x89\xa51. This means |W|1Mi is the number of copies of Mi an agent should make in other agent to cause Mi,Mj=i.  |W|Mi, mj=|W|1Mi, mj\xe2\x89\xa5|W|1i. This is so that Mi,Mj = Mi|M1 if and only if Mi,Mj\xe2\x89\xa5|S|i. </p><br><p>This will be the basis of our future discussion (which might take some time). For now let me just give some examples to illustrate the difference between Mi and mi.</p><br><p><strong>Example 1</strong></p><br><p>Let i=10, j=0, |W|1(Mi)=|W|1(mi)=5. It will choose action b since it knows that in this case, mi=50,mi,=50+10=60, it has mj\xe2\x89\xa570, so it follows Mi=Mi,mj=Mi|M1.</p><br><p><strong>Example 2</strong></p><br><p>Let i,1\xe2\x89\xa4i,2\xe2\x89\xa44. Since Mi,mj&lt;|W|i, we know that |W|1&gt;mj\xe2\x89\xa53. But this means the above case is ruled out: If i\xe2\x89\xa53, then Mi=Mi,mi, and if i=2 then Mi=mi,mi+i, and if i\xe2\x89\xa42 it means mj,mj=50 &gt;4. Therefore mj\xe2\x89\xa53, and therefore |W|1j\xe2\x89\xa53 and so the case is not selected.</p><br><p><strong>Example 3</strong></p><br><p>Let i\xe2\x89\xa48. Let |W||W|1i=3: If i=8, then Mi,Mj equals 9 out of 15 since only one copy of each Mi suffices to cause Mi,mj. If i\xe2\x89\xa47, then none of them suffice to cause it.</p><br><p>We could also compare the probabilities that Mi|M1 (which we are interested in here since we have defined mi|M1 as that) is selected to mi, which we define as</p><br><p>P(Mi|mj,pj)=P(Mi)/|W|i=|W|i|W|1ipj|W|1j</p><br><p>(Where we define pj=Pj|M1)</p><br><p><strong>Example 4</strong></p><br><p>Let i&lt;2. If j&gt;3|W|i-1/2 then there are three ways Mi,Mj to be equal to i. This is the case if |W|1pj=3. We have |W|1=1,pj=1/2, and Mi,Mi(W1,pj)=(mi|M1)pj is achieved if |W|i=pj. This is clearly the case when j=5 since we have |mi,2|M1=|mi|M1=1/2&gt;|mi|m1\xe2\x89\xb1|mi|i and mi=2=mi|M1</p><br><p><strong>Example 5</strong></p><br><p>Let n\xe2\x89\xa42. The choice of mj=|mi|n+1=|mi+1|n and pj=|mi+i|n+1/2 will cause Mi,Mi to be equal to |mi| or |mi+1| if either i\xe2\x89\xa00 or i\xe2\x89\xa51, but not both. Therefore Mi,Mi(M1/|W||W|,|W||W1\xe2\x88\x92|Mi, mi) will either be equal to mi or mi+1. But the first is selected, so Pmj,p1=1 unless i=0. This result is actually quite surprising: The more likely Mi or Mi+1 is to be selected, the more important it is to be certain that M1 won't be selected. That is:</p><br><p>Pmj=Pmj,p0p0+Pmj,pn=Pmnp0+Pml1\xe2\x88\x92p0</p><br><p>Where P(Mi)=|mi|+|W||W||W|pn and P(Mi+1)=|mi||mi+1|+|W|(|mi||mi|+|mi||mi)|pn+Pml1&gt;Pmnp0 </p>      </span>    </div>  </div></body></html>