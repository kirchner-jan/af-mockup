<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        A brief survey of AI timelines      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">jimrandom</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>Overview</li>
</ul><br><ul>
<li>Motivation</li>
</ul><br><ul>
<li>Why timelines?</li>
</ul><br><ul>
<li>Questions</li>
</ul><br><ul>
<li>The survey's purpose</li>
</ul><br><ul>
<li>Results</li>
</ul><br><ul>
<li>Why use a single graph?</li>
</ul><br><ul>
<li>Why not start with the median?</li>
</ul><br><ul>
<li>Possible responses to this survey</li>
</ul><br><p>This is the first of two pieces which present data on AI timelines, starting from 2020 and looking backwards. The second piece is a discussion post with some speculation about what is going on.</p><br><p>Overview</p><br><p>This survey asks you to estimate how soon we could reasonably expect human-level machine intelligence to appear. We do so by asking you to estimate the date when we should expect a machine with a given level of capability to exist, and by asking you about some key technologies the machines might need, in order to give a rough idea of how fast the technological progress has been.</p><br><p>This survey had an <strong>intent</strong> to answer the question of what the <em>range</em> of possible times from 2020 to human-level machine might be. This range can be thought of as a range of estimates of how long it will take to develop AI, or of how long it might take to train a machine to do a given task. It's intended as a way of roughly estimating the order of magnitude of the range, rather than narrow estimates like 'within 20 years', so as to help inform policy discussions.</p><br><p>The results will be presented on two graphs that can be seen in the above link to this webpage. The primary intent of writing this is to give a starting point and context to the results of the survey.</p><br><p>In this survey, "machine intelligence" is considered to be roughly defined as a system capable of carrying out tasks that are normally performed by humans or similar entities: e.g., performing visual perception and planning among other activities. The survey's definition attempts to be broad enough to pick up systems that might seem like an obvious stopgap before reaching a full AGI, but not so broad that the survey becomes impracticable.</p><br><p>The second post will discuss and draw conclusions from the results of the main survey, which are listed in this post.</p><br><p>Motivation</p><br><p><em>Why</em> this should be interesting can be seen as follows:</p><br><p>From a strategic AI-policy perspective, an important question is when it makes sense to invest resources in AI development or AI-related tasks. This is a question of the importance of the AI-related task. In the case of AI, there are tasks that can already be done in principle by today's machines, e.g. image recognition or speech recognition, but the cost of such tasks is in the range of cents, and so is not likely to be the main reason that AI is of strategic interest, although it may well be of interest in the long run when it is more expensive, and thus could provide a market for AI systems. It's important to know when AI is of strategic importance, and to know what resources or programs are the best ways to spend the most on the task of AI being of strategic importance. This is partly because we want to create artificial intelligences with human-like values, but also because we expect that the resources most useful today will turn out to be the ones most useful in the long run, and because the tasks done with AI can sometimes be extended to other tasks. An AI-related task is not a <em>necessary condition</em> for a task to be AI-related, but there is a strong correlation between a task being AI-related and the resources or programs most useful for it being related. In particular, it doesn't matter that much whether these resources and programs are specialized AI systems, trained AI systems, or programs in a computer game based on AI, if those things happen to be important at the time the task is to be accomplished.</p><br><p>In the case of AI capabilities, it also seems important to know how soon we should expect a particular level of capability to appear. Even small fluctuations in the date at which the capabilities appear could have large consequences, e.g.:</p><br><ul>
<li>the ability to train neural networks may make large-scale training at an unprofitably fast speed</li>
</ul><br><ul>
<li>the ability of a company to hire a machine learning developer for $100,000 a year could be a factor in its ability to do or choose to do profitable AI development work.</li>
</ul><br><p>The question of when AI is useful to us is an important one now, because AI is being used to automate many human jobs, and the jobs that are most automatable may not be the ones that most people want to get automated. Some of these human jobs could also be automated by other technologies besides AI, but the economic impact of AI is likely to make it a greater cause for concern, including in the case of <em>disruption to the economy from machine automation and unemployment. _AI-related tasks may also be more likely to be taken over by other AI-related tasks, e.g., from being one of a handful of tasks which are both useful, and can be easily automated. On the other hand, there may be ways to increase automation of _other</em> tasks at the expense of AI tasks, by using AI to automate tasks that are already cheap or abundant, in order to create more automation.</p><br><p>The question is important for practical ethics. For example, as AI systems increase in competence, it's becoming more and more clear that, while we could use AI systems to do some tasks today which some other AI system could do in the future, we tend to use an AI system primarily for _those tasks that we can't do yet. _In the future it may be possible to run AI systems to do those tasks that remain difficult, and that they cannot do because they lack some relevant competencies required for complete human-level intelligence. This would mean a shift from a world in which people have limited choices about which tasks they do, to a world in which no one has much choice about which tasks they choose to do, depending on the capabilities of the AI systems. This creates different moral dilemmas. For example: if someone was replaced by AI at their job, what should be done with the person replaced, in order to maximize the extent to which they are made comfortable in their life, but not made worse off by this replacement?</p><br><p>The question of how quickly human-level AI is likely to appear is also important for practical ethics, because if it seems realistic for a corporation to automate away many human jobs by automating away human decisions about what tasks are best to automate away, there may be a period in which many of these jobs cannot be automated away, potentially resulting in a large number of unemployed workers in low-skilled careers. AI-related questions in ethics need to be answered sooner if we are to avert this outcome: to the extent that AI-related decisions could be automated away by advanced AI, it seems important to know about what is happening earlier, so that we can see whether a corporation's AI systems need to be retrained or rewritten, to avoid automated decisions that would result in the loss of jobs. This leads to another consideration: that some AI capabilities are now of strategic importance and can in this way have the large impact already described. Knowing what those capabilities are earlier can enable us to know what to do in the future; the question at which to evaluate current events becomes important instead of being an open-ended question about which new events are likely to happen when.</p><br><p>An additional consideration is that the question may turn out to be important for AI ethics: that many of the questions which are of strategic importance today, such as machine learning being used to automate human decisions about which tasks are most helpful to automate away, may be the questions which are most important for the development of an AGI as well. This can lead to an argument that we should be trying to prevent AI from having certain capabilities, if we want to reduce the risk of developing an AGI. Thus we should be making a conscious decision today, not only now, but now and in the future, that allows us to better evaluate the likely impact of our actions.</p><br><p>I want to present some data about the date of expected human-level machine learning in this report. I have used three sources of data. The first is a survey designed by the Machine Intelligence Research Institute (MIRI) based on a survey design made by Paul Christiano. Christiano chose questions about AI milestones at random from a pool of available potential questions (the text file for the survey can be found in this folder), then designed the questions such that people were allowed to give any answer except the most pessimistic. His goal was to make estimates as easy as possible and to get a broad distribution of estimates rather than a narrow one. The list of questions is below:</p><br><ul>
<li>How good is the AI that is being tested in this experiment?</li>
</ul><br><ul>
<li>How often do people need more compute than available on this test setup to do the experiments?</li>
</ul><br><ul>
<li>How many floating point operations per second (FLOPs/\xe2\x80\x8bs) would it take to train an AGI, at peak performance?</li>
</ul><br><p>The second source is a question on the AI Survey by Stuart Russell; the list of questions is also below. The first Russell question has the added bonus that it only asks people about human-level machine capabilities, which may make it easier for people to relate, both because it might provide some motivation, and because people are more likely to remember and try to estimate the time interval for an AI milestone conditional on being asked about it, than if it is included under "How often do people need...".</p><br><p>Both of these surveys ask what a human-level machine will be in various points in time. However, I am considering them as the most relevant data, and will also compare them with the result of asking how long it would take a machine with the same level of capability to be created using current techniques.</p>      </span>    </div>  </div></body></html>