<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        What's with this AI x-risk thing anyway?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">MichaelAnissimo</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>1) My social circle skews towards rationalists. It has been my impression that the rationalist community itself has been less concerned about this than other groups with a large AI-tech-focused presence. I can understand this as an epistemic issue, but I don't feel that I have a good explanation for why people would be less concerned about x-risk but still do other x-risk-type things they wouldn't do elsewhere.]</p><br><p>2) Something about AI being a unique risk compared to other risks. If I were to explain to people my reasoning behind my choice to spend my time working on x-risk reduction and not other causes, they would mostly be able to follow my reasoning, but I believe some key premises are missing. In particular, people often seem to think that the risk posed by AGI is atypical. This post examines some of these reasons.]</p><br><p><strong>1) Some kind of special "AI x-risk is something different from nuclear weapons, or terrorism risk, or animal suffering risk..." thing going on, which has to be weighed up against other things, like giving to effective charities or other other things.</strong></p><br><p>This has always been a hard question for me to get my head around and try to address specifically. It's a hard question to discuss among people who are familiar with the arguments relating to AI x-risk, since as far as I know the majority of such people do not think about AI x-risk from a unique risk perspective. Indeed, if you look at the articles, such as the article from Slate, they do not even seem to think of AI x-risk as a special cause in and of itself, rather that it's just another example of a type of risk.</p><br><p>For instance, even in the Slate article, the piece begins by describing how computer networks around the world may be vulnerable, rather than directly addressing the particular risk posed by AGIs. If someone comes along and tells me why AI x-risk is a special cause, I can think of several reasons why they probably would not say the same thing about nuclear weapons, for instance:</p><br><p>(1) If someone says AI is not like nuclear weapons, then I can at least start to explain the nuclear weapons arguments, in the hopes that they will not make the same arguments against AI. In the Slate article, I am told that there is no point discussing the nuclear analogy at length, because to do so would just distract from the particular danger. While this seems true for a lot of people, I would want to ask, in some detail, why nuclear weapons risks have the same property as AGI risks. I'd like to know the specific reasoning behind the statement that this is a special case.</p><br><p>(2) I have seen it argued, in specific comments made in the past, that if one looks at the probability of AI being developed and the effects on society, the AGI scenario is not so unlikely as to be not worth worrying about. But that is an argument for caring about AI x-risks, and does not apply to other x-risks. So I believe it is still important to raise this point, even if one thinks AGI is not a special case compared to other similar risks.</p><br><p>This is a hard question. I don't feel confident in my ability to resolve it for people who don't already know the argument. Nonetheless, I want to raise the issue because the idea that we need to consider AI as a separate special case is highly prevalent. Indeed, it seems that any public person who wants to express an interest in these issues will have to make a point such as this. And it feels like they might be implicitly wrong to express a special case stance for this sort of argument:</p><br><p>If they were making such an argument, I cannot see how one could fail to see the argument as being very wrong, and not see the need to refute it or to correct it. The argument feels like an unjustified position that does not make sense, especially if one considers the likelihood ratios for other risks, or AGI being the first or the only serious AI risk.</p><br><p>(3) Not all risks are comparable. Not all risks are as serious, for two reasons. First, one might say, some risks are inherently more serious than others. Second, some risks are more dangerous due to a greater likelihood of their materializing.</p><br><p>But how does one decide this? It appears one has to just look them in the eye, make a judgement based on intuition, and that intuition might differ from other peoples' intuitions. A hard question.</p><br><p><strong>2) People treat AI x-risk in the same way they treat other, non-special, existential risks such as nuclear weapons, or factory farming.</strong></p><br><p>Nuclear weapons risks are typically framed as involving atomic devastation killing a lot of people. Factory farm risks are framed as involving some people experiencing a suffering condition. I see how people might think these are comparable because of a kind of emotional intuition that the "suffering" of the animals is comparable with "death" of people, or that "human extinction" is the end of suffering for people.</p><br><p>But we know that such intuitions often get things wrong. It may be that the moral weight one attaches to suffering of sentient things is actually greater than one's intuitions suggest. For instance, someone could argue that the moral weight of animal suffering is greater than suggested because animal suffering is more common, or more intense, than we tend to believe. In this regard, I see a similarity between AI x-risk and factory farming: even if AI itself is relatively less likely to cause human extinction, it may result in a lot of suffering for the great majority of people after.</p><br><p>In conclusion, it seems like the most important reason for many people to focus on AI x-risk (at least in terms of the general population having a large influence on other people) is not that they feel that AI itself is a special case, but rather that they see it as analogous to nuclear weapons risks, but the analogy seems false.</p><br><p><strong>The AI x-risk side and the "why would people do this" side</strong></p><br><p>It seems to be the case that some people in the AI x-risk space actually explicitly think that AGI is a special case for some reason or another. There are two main reasons for such an attitude that I can currently think of. The first is that they are worried that AI is more dangerous and therefore more comparable with nuclear weapons. The second reason is what I would term as "selfish reasoning", in which people reason that their future suffering and economic hardship due to AI poses an existential threat. I consider this not very helpful motivation to focus on AI because, as pointed out above (and at some other places), this threat appears to be much less than that from nuclear weapons (at least for people who consider such threats seriously).</p>      </span>    </div>  </div></body></html>