<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        I Governanceâ€”Open Letter      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">OpenAI and FLI on AI governance research priorities</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <ul>
<li>What do we need to change in order to improve decision-making processes in technical AI-relevant domains (e.g. law or politics)?</li>
</ul><br><ul>
<li>What actions could be taken to improve the odds of better decisions being made in the future (e.g, via regulation or litigation)?
To help identify good directions for research, we've been looking at several related themes:</li>
</ul><br><ul>
<li>How can we align and retain long-term AI intent?</li>
</ul><br><ul>
<li>What can we do to limit adverse side-effects of AI?</li>
</ul><br><ul>
<li>How can AI researchers build long-term trust with policymakers?</li>
</ul><br><ul>
<li>How do actors in a complex system like AI development or regulation create and align rules, institutions, and behaviors which facilitate or prevent problematic outcomes like race dynamics or malicious actors?</li>
</ul><br><ul>
<li>How should AI companies and researchers account for and mitigate the political, strategic, and strategic uncertainty inherent in powerful systems like AI?</li>
</ul><br><ul>
<li>What research and policy can we take to better ensure that the world is better-resourced and prepared for the long-term impacts of AI?</li>
</ul><br><p>We hope that this can encourage researchers in other fields to also evaluate whether and how they can be helpful in AI governance, and encourage policy-makers and other actors to investigate particular technical research problems which are most important to their organizations or in their areas of expertise. In this post, we will outline some of our current priorities, and our research aims. This document has been vetted by other members of the AI policy and governance team at OpenAI; to add your feedback, please add a comment.
<strong><em>Research Priorities</em></strong>
The first step on the research agenda is to identify areas for potential intervention. Given the difficulty of the problem, we expect that we will not be able to work on all of them at once. Rather, we've decided to split the research agenda between three categories, and prioritize the highest-value interventions that are the most neglected or under-investigated.
Categorization:</p><br><ul>
<li>__Open-ended approaches __to reduce AI risk. For instance, improving decision-making processes and legal processes which facilitate or inhibit certain outcomes, or better explaining how AI systems work.</li>
</ul><br><ul>
<li>__In-depth investigations __into particular AI sub-problems. These will generally be small-scale investigations, that can be tackled quickly, as a starting point. These need not be publishable research projects; we welcome the creation of lists of relevant questions and projects which could help with prioritization and future discussion.</li>
</ul><br><ul>
<li><strong>Policy-making __and</strong> governance __interventions. These will include direct changes to existing laws or regulation, e.g. changing trade-offs around AI risk between different safety objectives, or amending human rights or existing international institutions.
Within each category, we will prioritize the most neglected areas, based on our current understanding of the causes of risk, and the extent to which a given intervention might decrease existential risks from AI. This includes a variety of things:</li>
</ul><br><ul>
<li><strong>Technical solutions</strong> to problems relevant to AI developers. This includes approaches related to AI safety which would reduce or eliminate current problems from technical AI safety (e.g., recursive reward modeling), approaches related to value learning which would reduce or reduce the risks of value learning failing, or approaches to transparency or interpretability which could reduce the likelihood and/or extent of malicious uses of advanced AI (e.g.) adversarial examples.</li>
</ul><br><ul>
<li><strong>Legal or institutional solutions</strong> to problems likely to arise from the implementation or misuse of advanced AI systems, including potential solutions related to AI oversight, liability, or regulation (e.g.).</li>
</ul><br><ul>
<li>__Other __explanatory work or research that directly tackles the causes of risk from advanced AI (e g. explaining the implications of the Good Regulator Theorem for specific technical risks).
We're confident in our research priorities. The aim of these work is, broadly, to improve the long-term outcomes of AI, to ensure that it's used to help humanity flourish rather than being used to threaten it. That is a tall order with a lot of room for things to go wrong. We need the research projects to be feasible to conduct, to be tractable. This means that if a problem seems daunting or overwhelming, then that is a strong indication that it is not important; if a problem seems tractable, but seems low value, then that is also a strong indication that the problem is not important; and if a question seems important, but very difficult or overwhelming, the answer is probably "not enough is known."
We're also confident of what it takes to achieve the aims of the agenda in particular domains, and the specific interventions we are interested in. To summarize these:</li>
</ul><br><ul>
<li><strong>Training on technical AI safety</strong> is aimed at giving people who are working on AI technical safety the skills needed to do useful work on other issues which may be related in interesting ways. Our current focus is on people in technical safety organizations; we may later find the time and energy to train a different set of people.</li>
</ul><br><ul>
<li><strong>Training in AI policy and strategy</strong> is aimed at helping people improve their ability to influence AI policy, either through direct involvement, or more indirectly; our current focus is on government and civil service staff. As our capabilities improve, our hope is that our training will be helpful to various actors in the future.</li>
</ul><br><ul>
<li>__Funding AI safety research (FASR) __is intended to give a specific, short-term boost to a small, well-reputed group of researchers to focus on particular technical AI safety problems, for a period long enough to make an impact and have the results published. Our current funding decisions have a lot of uncertainty.</li>
</ul><br><ul>
<li>__Regulation research __is aimed at understanding existing regulation; as the impacts of AI become more important, and governance institutions become more capable of overseeing AI, we will need good understanding of how various governance institutions function, in order to develop and advise on new governance institutions.</li>
</ul><br><ul>
<li><strong>Technical research aimed at the problem of human-AI interaction and control</strong> is aimed at producing technical solutions, which can help programmers and decision-makers to implement AI policy robustly. We especially want to understand how AI is used in complex, real-world systems, in order to understand the risks and opportunities.</li>
</ul>      </span>    </div>  </div></body></html>