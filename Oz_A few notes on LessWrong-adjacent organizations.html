<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        A few notes on LessWrong-adjacent organizations      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Oz</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Recently, a friend of mine pointed out that, with respect to me, the LessWrong-adjacents organizations are an order of magnitude more impressive than EA in terms of their stated goals. Which is unsurprising, since they don't have the same restrictions on scope of ambition, and as I explained in the original post regarding Effective Altruism I don't think all goals can be equally impactful\xe2\x80\x94although I still like EA a lot more than any organization I've heard of trying to have <em>both</em> large-scale impact and being a bunch of nerdy rationalists, so it's reasonable to think there's probably room for improvement.</p><br><p>However, it's worth pointing out that the goals of these groups aren't all necessarily optimized to create the most awesome world-changing _things _we could be doing with money. </p><br><p><strong>FHI: The Future of Humanity Institute</strong></p><br><p>FHI is a well-intentioned organization, but it's also the place of origin for MIRI, the primary reason I know about the community, and its biggest proponent.</p><br><p>They focus on trying to get the world to be ok with AI development for various reasons:</p><br><p>http://\xe2\x80\x8b\xe2\x80\x8bwww.philosophy.ox.ac.uk/\xe2\x80\x8b\xe2\x80\x8bpublications/\xe2\x80\x8b\xe2\x80\x8buploads/\xe2\x80\x8b\xe2\x80\x8bDocuments/\xe2\x80\x8b\xe2\x80\x8bFHI.pubx.pdf/\xe2\x80\x8b\xe2\x80\x8bview/\xe2\x80\x8b\xe2\x80\x8b1530741088/\xe2\x80\x8b\xe2\x80\x8bfhi.pdf</p><br><p>FHI hosts an unusually large community of researchers who work on questions very relevant to the goal of trying to reduce x-risk (in particular, how to ensure a positive long term future, and therefore how to ensure a future where the world is ok in some sense). These people tend to be fairly "normal" in ways that are actually useful (the main thing they do to get a handle on the current situation is research relevant to their field, and they're not especially concerned with the personal needs of their employer. They still tend to be <em>surprisingly weird</em> and have different opinions from the mainstream).</p><br><p>It would not be a _surprise _if a researcher doing work in the area of x-risk got up to speed with the current issues and then tried to implement some idea that seemed clever, that wouldn't work out, or that would end up harming the world. And you might not be able to figure out how to avoid doing so before it's too late, depending on how far ahead you're looking at. This leads to their research being generally more risk-averse than the standard philosophy literature.</p><br><p>I think these things contribute more to "success", than to the actual goal of the organization.</p><br><p>MIRI seems less focused on actually moving forward on research to reduce x-risks or anything like that. They seem more focused on getting <em>other people</em> concerned about x-risks and how to prepare for them. In a way, this makes sense. But it's more of a weird focus and would not necessarily have led to them writing books or the like. And it's not as well equipped to produce results that are actually relevant to things like reducing risk, so I don't feel very excited about that focus being pursued as the primary goal of the organization, as opposed to more normal outreach.</p><br><p><strong>GWWC:</strong> The Global Wild-Animal Campaign, formerly the Humane League</p><br><p>GWWC claims to be about reducing extinction risk from environmental destruction, of both plants and animals. I didn't investigate them too much\xe2\x80\x94I don't really feel like I could evaluate whether they have good goals or not.</p><br><p><strong>CFI: The Center For Applied Rationality</strong></p><br><p>CFI was founded by a LessWronger, so of course it's probably on the list by default. I haven't investigated them much because... my impression is that CFAR has had _a lot _of trouble establishing credibility in non-LessWrong communities (so much so that, of people who are aware of it and who know enough about rationality to be interested in x-risk and long-term solutions, a surprising number don't know about CFAR or haven't heard of the LessWrong community, and don't get the impression that that is due to any coherent strategic decisions). This makes me nervous that their stated goals are not the optimal goals for the community.</p><br><p>One example I can think of is CFAR's apparent role as a group trying to help people interested in rationality "win" in politics and other communities. This sounds bad from the perspective of making the world better, and also bad from the perspective that these people may be particularly unlikely to become the leaders of x-risk effort. </p><br><p>I haven't really investigated this, though, in part because there's just not that much public information on what CFAR is actually doing. If you've heard that they do some kind of direct action activism, you're probably fine. If you actually have a sense that they're strategically doing the thing that increases the chances of effective political activists, I don't think many of them will be able to admit that.</p><br><p>Note that the Center for Applied Rationality does do "pure" rationality training, though (and I have good information about this).</p><br><p>__EA: __The Effective Altruism community. There have been some complaints about EA (and about the organizations that make up EA) that the main thing EA does is talk about how cool they think Effective Altruism is, making an exaggerated case that there's a problem to be solved, and then trying to create projects to make progress on that problem.</p><br><p>To the extent that EAs focus on their research agenda, it's the first one I know of that is based on research\xe2\x80\x94rather than, eg, outreach or fund-raising.</p><br><p>However, I expect the existence of EA/\xe2\x80\x8bRationality and the creation of projects to help with x-risk\xe2\x80\x94by providing the community with better researchers and funders\xe2\x80\x94to be <em>vastly</em> more impactful than if they could do nothing at all. I still think most goals can be useful in small ways as well.</p><br><p>In any event, I don't feel I have a good sense of what their goals actually are. But it's safe to say that I don't feel they're optimized for x-risk reduction, though I expect some of it is good relative to the world. </p><br><p>Also worth pointing out: </p><br><ul>
<li>Ought claims to be trying to help EAs actually do effective altruism in practice.</li>
</ul><br><ul>
<li>Giving What We Can claims to be trying "to promote a culture dedicated to taking action to improve the world, and building the capacity of a movement to give most effectively."</li>
</ul><br><ul>
<li>The Center For Applied Epistemology is an application of CFAR's "applied rationality" methods to some of EA's most popular claims.</li>
</ul><br><ul>
<li>The EA Hotel is one of a few projects that are working to improve the world by improving the world. </li>
</ul><br><ul>
<li>Giving In Science is another application of EA, of how to use quantitative evidence to make real claims regarding the world (see also Give Well's analysis of this).</li>
</ul><br><ul>
<li>80,000 Hours has been popularized through the blog of a LessWrong user, and provides another place where people can look for a way to actually affect the world.  </li>
</ul>      </span>    </div>  </div></body></html>