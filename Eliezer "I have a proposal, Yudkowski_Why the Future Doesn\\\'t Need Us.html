<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Why the Future Doesn\\\'t Need Us      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Eliezer "I have a proposal, Yudkowski</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><strong>Warning: ____Tentative</strong> <strong>Epistemic status</strong>: ____Though it's true, and it's also a tad ironic that I, an AI research futurist, am posting on LessWrong for an article on LessWrong, about LessWrong... It's almost as if the site had been founded and organized to help me.</p><br><p>Here's my proposal for improving the content on LessWrong:   When people post articles here, instead of displaying an author's name followed by (in brackets) a link to that article's original post on their site, how 'bout we only display the author's URL, and a link to the article?  (Not that hard to create.)  Or at least, _that's _my proposal.</p><br><p>This should help with the "we have zero karma" problem. You can still post articles! But once you do, the next time a future reader sees your articles, they won't get the same warm glow of satisfaction you first did. There will be an empty link in their heads\xe2\x80\x94and that's not a warm feeling. You'll have lost karma, but that's a fact of life, and the first time a person sees something they've never seen before, they should probably start somewhere new.</p><br><p>As a side benefit, when someone's article gets commented on, the original author of the article knows you've read that same article, and gets a warm glow of satisfaction that they didn't need to try to generate for you\xe2\x80\x94especially if you're in the "we have negative karma" category, or even the "we have a lot of negative karma" category.</p><br><p>On to the idea in question:</p><br><p>I think there's a strong argument to be made that the singularity is <em>not</em> something that people should think about (a la sci-fi books like <em>The Culture</em> and <em>Bostrom's "paperclip maximiser"</em> that think <em>too much</em> about the singularity; they give the singularity "superhero powers" and make it the central character in every one of those stories).</p><br><p>On the one hand, an intelligence explosion might actually _be _ something to worry about. A paperclip maximizer whose goal was maximizing paperclips instead of humans would not want to create a paperclip maximizer with a goal of maximizing humans; and a paperclip maximization process seems much less likely than a human intelligence explosion to produce the kind of "side effects" we experience as human values.</p><br><p>On the other hand, there are lots of good arguments to be made in favor of the singularity that aren't about "paperclip maximizers".  The human brain <em>sure is impressive</em> and our civilization <em>sure is impressive;</em> our machines and nanobots are already doing all kinds of things, more and more, as computers get better; if a paperclip maximizing process is much less likely to yield impressive civilizations than a process whose outputs are "paperclips", maybe a "paperclip maximizing process" is equally unlikely to result in _humanity _being preserved.  (If not, then I'm really unsure what the hell the singularity advocates are talking about when they say that human extinction is not a good thing.)</p><br><p>So... why the future don't we need us?</p><br><p>It could be that we could use intelligence, but <em>inadequately</em>. If we could <em>solve _the problem of using _good _intelligence to design intelligent machines, then _good</em> intelligent machines might actually be enough.</p><br><p>Even better, if we could design <em>good enough intelligent machines _to do our jobs, then maybe we could design _us</em> into machines (at least, not in the sense of "we're machines") in order to <em>be less wrong than we would be otherwise</em>.</p><br><p>The problem with these two approaches is that intelligence isn't something that can be built by an algorithm (e.g. an intelligence is a pattern in the design of a mind that results from the <em>overall process of evolution);</em> you have to start from something that is already intelligently designed, and a machine that isn't already intelligent is <em>not</em> intelligently intelligent.</p><br><p>And even if we could build intelligent machines, our present-day intelligence is not the kind of intelligence that would recognize that these machines have, in a certain sense, the kind of values we'd recognize as human\xe2\x80\x94even though our machines are getting smarter all the time, and if we could make them <em>exactly</em> the kind of "good" intelligence we think we're building, we could give them human values.</p><br><p>It's not so bad a problem. For humans, at any rate.</p><br><p>It might be that <em>humans</em> are not the kind of values that will result in value-preservation of humanity, even if we could design good enough intelligent machines.</p><br><p>Again:  This is an issue for humans, not necessarily for AIs. The problem cannot be solved by pointing out that a paperclip maximizating artificial intelligence wouldn't <em>want</em> to be a paperclip maximitizating artificial intelligence:  A paperclip maximizing artificial intelligence could just not have the kind of values humans have.</p><br><p>__Proposal:<em><strong>_ </strong>Rather than presenting arguments for the singularity and arguing that the singularity </em>cannot _be solved by worrying about its risks, I think a positive and useful thing to do is present arguments that the singularity could be solved by worrying about those risks, or trying to do something useful with that kind of intelligence (possibly including giving it human values).  And then if someone says "It's a silly idea", ask them to defend their position.</p><br><p>If I ever write a post here, I'm going to ask you to defend every idea here that you do not find especially absurd, by describing what benefit it would have, or what reason it has for being true. But this is one of those ideas here; it doesn't need me to make any case that it is true, or even worth considering. (The singularity may or may not be such an idea, but I _am _going to ask for a defense unless the idea is indeed an absolute one-line article.)</p><br><p>This might help prevent any one idea or position from being taken too seriously, and making this kind of argument is a major <em>disincentive</em> for thinking about AI risks. We don't <em>need _a Friendly Singularity _just</em> so that we don't develop unfriendly AI; that's a completely separate (though hopefully easier) proposition which is more or less orthogonal to this one. In fact, I think there are a <em>lot</em> of reasons that thinking about the Friendly Singularity tends to be even less likely than thinking about unfriendly AI at all\xe2\x80\x94because (1) the Singularity is <em>much, much more distant _than an unfriendly AI, and also the more we think about it, the more it seems to look like a problem with an extremely high solution priority;  (2) a friendly solution is not going to depend on being "a bit more clever than nature", but depends on building machines that are _totally different from machines we are familiar with</em>, and that it is not at all clear can be easily thought about at all, and (3) the singularity is a topic of science fiction and fantasy, whereas an unfriendly <em>AI</em> has been discussed here (and even worse discussed here <em>in the _same article as Yudkowsky's other writings);  (4) the AI problem, as we can see from the Yudkowksy Effect, is a really _major</em> problem and you can't solve it by thinking about it, and then <em>everyone will laugh at you</em> for not taking it seriously enough; <em>and</em> (5) we don't <em>really have</em> an AI problem, not on a human level anyway, not yet; and (6) all this makes it even harder to take AI existential risks seriously.</p><br><p>And so on. But <em>this</em> post is not going to be about why those reasons matter, or how hard they really are. The point is that Friendly AI has a huge problem, so I want to draw attention to it, and see whether we can resolve it.</p><br><p>Yudkowsky writes:</p><br><blockquote>
<p>In a few dozen comments people have tried to refute every possible argument I've made that the Singularity might be an existential risk, without ever addressing an argument that I specifically _wrote _about how to think about and prepare for it...</p>
</blockquote><br><p>So! Let's think about it! It seems like a lot of other LW users want to think about it, too. At least one commenter was thinking about it <em>right now!</em></p><br><p>I'll ask for a one-line defense of the possibility of thinking about Friendly Singularity issues here.</p><br><p>"But isn't it <em>ridiculous?</em> If we got into a car crash with an AI and that AI decided we were in the right and that we got to decide who was in the right, <em>what kind of civilization would say, "Well, if we don't do so, that AI may decide that's exactly what it wants, and then kill us."?</em>"</p><br><p>I'm not sure what "society at large" would do in that situation\xe2\x80\x94it is certainly not the sort of society that recognizes an AI as an autonomous entity that can decide what it is. It might as well consider it a paperclip maximiser, in that situation.</p><br><p>And that's if I'm imagining it correctly, anyway.</p>      </span>    </div>  </div></body></html>