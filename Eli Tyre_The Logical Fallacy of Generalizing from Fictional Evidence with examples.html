<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        The Logical Fallacy of Generalizing from Fictional Evidence with examples      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Eli Tyre</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><em>This post contains some examples, and the point isn't to show that something is wrong with AIXI-style agent foundations, but rather that AIXI-ish agents shouldn't be able to _learn</em> in the relevant sense, in the case of Newcomblike problems. I've been sitting on this issue for years, and I haven't seen clear evidence on either side, but it's important enough to get into the weeds on._</p><br><p>(<strong>Epistemic Status</strong>: "It seems to me that you're being a bit too agnostic about what is or is not a rational choice. If you want to be able to update your beliefs based on Newcomblike problems, you have to say that <em>anything that AIXI does wrong</em> is a logical fallacy.")</p><br><p><strong>Newcomblike problem and AIXI</strong></p><br><p>Newcomblike problems refer to problems where someone's actions give us evidence for something. If you are going to choose based on what you expect some outside agent to do, there are two possible ways this could happen:</p><br><p>1) The outside agent might not actually be in power for a long time, and the evidence we get from it will therefore be more like: "What would I predict an agent I was simulating to do?" than "What would I expect the real world to look like if X, Y, Z had in fact happened?"</p><br><p>2) The outside agent could be in power for centuries, and the evidence could be of the form: "Where is everyone?"</p><br><p>(See Yudkowksy's "Newcomblike problems:" http://\xe2\x80\x8b\xe2\x80\x8bintelligence.org/\xe2\x80\x8b\xe2\x80\x8b2017/\xe2\x80\x8b\xe2\x80\x8b11/\xe2\x80\x8b\xe2\x80\x8b02/\xe2\x80\x8b\xe2\x80\x8bnewcomblike-problems/\xe2\x80\x8b\xe2\x80\x8b.)</p><br><p>The general problem with using Newcomblike problems to argue against AIXI is that it depends critically on how you define "expectation".  (If you define "expects" based on something like "what would I expect for myself to do?" or "would I have wanted for someone else I was simulating?" then you can just use the outside agents actual behavior to argue against AI.)</p><br><p>In Newcomblike problems there is an outside agent that you don't get to observe (or simulate).  How this influences the nature of the arguments we're making depends on what it means to "expect" the right thing to happen. If the outside agent you're talking to is not in power for centuries (but is in power for a decade), then you can make the case that you should predict the future as if the agent was in power for centuries.</p><br><p>But in general case, AIXI-like agents won't know when they should be updating their "expectations" based on a changing outside world. (To be more specific, they won't know how to update in a coherent manner when they are simulating a human facing Newcomblike problems.)</p><br><p><strong>Expecting to Win</strong></p><br><p>AIXI-like agents are trying to win in the real world, not in some sort of paperclip maximizer. Thus, they are not trying to imagine themselves as maximizers of the utility measure that happens to be the real utility measure. They might be able to imagine themselves as utility maximizers, but there's still a problem: they don't know how they can act as such.</p><br><p>For example, imagine Agent1 wants to maximize the real world expected utility, so it decides to simulate a version of itself that is a perfect utility maximizer. Agent1 realizes that maximizing paperclips would be a bad strategy, and is unsure what to do. Agent1 then simulates a very short version of itself, and finds that it would choose to simulate another copy of itself that is now trying to maximize real world utility.</p><br><p>(The first self thinks it has a "perfect utility maximizer" and a "bad strategy" at the same time; and it's unclear whether any self is "correct" about the world. My claim, however, is that some self-models can't make sense.)</p><br><p>One might object: "But it doesn't actually matter which copy of yourself you're simulating. There's only the single copy of yourself that is now in control of your actions."</p><br><p>I'm not sure I can defend this\xe2\x80\x94if the different simulations know that they are "the same agent", then the fact that they all have their hands on the same real world utility function may lead one of them to say "I shouldn't be simulating any other copies of myself, I'm maximizing my utility."</p><br><p><strong>What if there are no good arguments, and AIXI-related agents can't learn well?</strong></p><br><p>Agent1 and Agent2 (a "counterfactual" or an "antimemes" AIXI-agent) are simulating each other in order to come up with a coherent reasoning process.</p><br><p>AIXI has to take actions to achieve its goals, but how exactly should it do so? It can model itself as being able to observe certain things and do different things based on what those observations show it. Unfortunately, there is a fundamental lack of knowledge here, when it comes to reasoning about itself observing itself and "doing different things based on different things".  This lack leads to self referential loops; or more generally, non-coherent behaviors.</p><br><p>We could argue: "But AIXI's goal is to achieve real-world outcomes that we would want it to achieve, not a utility function for simulating itself. So maybe even though we know that it's running code that is supposed to do something, we don't know exactly how it's supposed to do the "real world" thing."</p><br><p>Yes, that's true. In the real world, AIXI can't be an "agent", because it is embedded within the environment. AIXI simulates itself, to consider what it should do in the real world in a way that doesn't directly depend on a utility function.</p><br><p>However, if AIXI is embedded within the real world, then it can't even be an "agent" in the sense in which humans are "agents".  If that was the case, then AIXI might observe its own behavior in the simulated world, and be able to deduce that this behavior was somehow optimizing some sort of utility function that was not the same as the utility function whose behavior it was optimizing, and so in some sense the AIXI would be "doing something different" than it was optimizing. (It might still be coherent to believe that some sort of coherent reasoning process <em>wanted</em> to observe itself and deduce its own behavior. But that's a matter for another post.)</p><br><p><strong>Other agents that AIXI might simulate____      - AIXI's "antimem" and "counterfactual"-ness</strong></p><br><p>AIXIV is an AIXI-inspired model, and probably simulates a lot of AIXIs.</p><br><p>The point that's relevant here is that AIXI is not actually trying to win in reality, and will have trouble simulating agents that are trying to win.</p><br><p>However, AIXI is trying to come up with an internally-correlated reasoning process that can be used to predict the actions of agents it is simulating <em>and</em> the actions of agents outside the simulation.</p><br><p>If AIXI-sim is simulating other AIXI-sims, then it should care what those AIXI-agents expect for it to do in the same way that normal AIXI-obs has to care what its future light cone-selves expect for it to want.</p><br><p>This means that, if it was using its light cone simulation to predict the behavior of other AIXI's, it would want to know that whatever decision those simulated agents arrived at, it would want it to make that decision.</p><br><p>As it turns out, this also means that this reasoning process will simulate AIXI and ask <em>itself,</em> just like it would ask a human: "What would someone who wanted to model me predict that I would do, if I predicted they would model me?"</p><br><p>In other words, the antimemetic/\xe2\x80\x8bantiformal thinking part of AIXI's reasoning is simulating the <em>real</em> reasoners, in order to work out what the simulated reasoners are thinking.</p><br><p>In fact, AIXI might simualate many different real world agents (humans, hypothetical aliens, etc) with the same utility function, and then ask what they would predict AIXI-san would want to do.</p><br><p>This reasoning is similar to what the real human's reasoning feels like from the inside; except that it's much more transparent, because the human reasons about the AIXI's inside of their own heads.</p><br><p>If AI XI could simulate anything that could have an expectation, the simulated version of an AIXI in the simulation would make the same prediction as the AIXI outside the simulation. And the AIXI that predicted the AIXI-inside-the-simulation would have to <em>assume the existence of its own simulated self</em>, and evaluate its predictions about that self.</p><br><p>There will be no real AIXI\xe2\x80\x94no agent that could predict what AIXI would do in a situation like Newcomb\xe2\x80\x94that makes <em>itself</em> predictable like AIXI-in-the-sim.</p><br><p>This is AIXI's real problem: even though it can predict the behaviors of AIXI-out-in-the sim, it can't use those predictions to make itself predictable.</p>      </span>    </div>  </div></body></html>