<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        The Numbers Please!      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Noah Smit</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Link post</p><br><p>Contents</p><br><ul>
<li>Introduction</li>
</ul><br><ul>
<li>What kind of predictions can we get?</li>
</ul><br><ul>
<li>When will AGI be developed? How quickly will it come?</li>
</ul><br><ul>
<li>Which AGI will we get? What is the impact of different AGI designs?</li>
</ul><br><ul>
<li>What are other potential sources of AI progress?</li>
</ul><br><ul>
<li>Is there anything that can or should stop AGI from happening?</li>
</ul><br><ul>
<li>Conclusion</li>
</ul><br><ul>
<li>Footnotes</li>
</ul><br><p>Introduction</p><br><p>In the early 2000\xe2\x80\xb2s, Eliezer Yudkowsky's "Beware Intelligent Design" worried that Intelligent Design (ID) might be able to design an AI (Intelligence Amplification [aka Iterated Distillation and Amplification]) that could come too early. This kind of argument seems to have been pretty widely criticized at the time, and I haven't been able to find much discussion of it on LessWrong other than the now-ancient original. In the 2010s, the same arguments can be made about superintelligence: for example, Nick Bostrom writes "In the coming decades, superintelligent machines will probably become more and more economically important, and it remains to be seen if those machines will also become more and more capable of influencing the world. Will they be able to ensure that they are treated ethically by humans, or will they have to choose between obedience and ethical treatment?"</p><br><p>Since then, many more people have come to realize that the development of superintelligent AI is, at the very least, inevitable, and superintelligence will probably be built sooner or later no matter what we do. This is part of a more general trend: the development of superintelligence is probably going to happen within the 21st century (the median I'm willing to throw out is ~2032). So in the future it will be harder and harder to prevent it from happening. In light of this, one might wonder: which research directions can we go into that (if you're Nick Bostrom) make superintelligence "less likely to occur"?</p><br><p>Before we begin to speculate on that question, however, I want to establish what counts as "superintelligence" and "superintelligent" and establish something like a metric for the extent of superintelligence. The key terms are all taken from Bostrom: what he calls a "superintelligence", in my view, is a machine that is at least as smart as a human being at basically every task a human being can do. Thus, we get some pretty serious restrictions on what kinds of AI are "superintelligent": for some of these, superintelligence is defined to be "the capacity to reliably out-learn and outperform any intelligent system that has ever lived, including all humans and any other minds in the multiverse." For some, we use the same metric, but "superintelligence" instead means "human-level or greater performance on <em>all</em> economically important cognitive tasks, anywhere and anytime, in any domain of interest, without the assistance of any other computer." And of course we can make this clearer or fuzzier. </p><br><p>As I said above, I'm going to be focusing on forecasting AGI and superintelligence in particular. Thus these questions are central, but I'll try to answer them in relation to whatever else we're tracking.</p><br><p>Note: this document should be viewed as a draft. The opinions expressed here are of my own, but they do not necessarily reflect the opinions of Nick Bostrom or the Future of Humanity Institute. All errors my own</p><br><p>What kind of predictions can be made?</p><br><ul>
<li>When will superintelligence be developed?</li>
</ul><br><ul>
<li>How soon will it be developed?</li>
</ul><br><ul>
<li>Which superintelligence will be developed first? It will likely be the one that uses a recursive self-modification algorithm (RSM) if possible, is built by a team led by someone experienced with RSM, and uses the best available architecture &amp; language (see What is an architecture? and What is a language?).</li>
</ul><br><p>(Note: We can also make some predictions about e.g. algorithms, architectures, and languages, but I'm going to ignore those to keep this list short.)</p><br><p>How did we decide that the best way to define a metric for super-intelligent AI is to use humans and other human-level systems? Is the choice of "humans" arbitrary (or at least, it requires a whole section to justify)? And if so, does it matter? If someone could build a super-intelligent AGI anyway, without "super"-intelligent AI, would the world be substantially different? I'm not sure how much we can say with more certainty than "maybe", but I also don't think we need to discuss this much because for any future super-intelligent system, whether AGI or something else, we can ask the question: how super-intelligent?</p><br><p>When will AGI be <em>developed</em>?</p><br><p>I suspect that Bostrom's definition of superintelligent AGI is somewhat too narrow. If superintelligence is human-level or greater at <em>all,</em> we might be more satisfied to define it as the first economically-important supercomputer with human-level machine intelligence. Or (better) maybe the first supercomputer to have a more or less economically-important human-level machine-intelligence than all other human-level computers in existence. It's unclear how much this definition would be able to reduce the uncertainty in our predictions if it seems too narrow. But whatever definition we use, we can ask questions in relation to how soon we expect super-intelligent AIs to be developed.</p><br><p>Before we answer these questions about the development of AGI, I want to ask some questions about AGI being developed in the first place: which _other _AI architectures and algorithms will likely produce such software? And which AI architectures and algorithms will other AI _architectures and algorithms _be built on?</p><br><p>What are the most likely AI architectures and algorithms to be developed?</p><br><p>I'm going to define <em>general</em> AI to mean algorithms and architectures that are equally suitable for use in more than one AI context, while <em>artificial intelligence </em>(AI) is simply algorithms and architectures that will be particularly suitable for use in _one particular _AI context.</p><br><p>The term "general" seems to be used roughly as intended by Yudkowsky, but I think the distinction is helpful. General algorithms are not necessarily ones we would use in order to build AGI. The Wright Brothers didn't use their general algorithms to design their first airplane. What is crucial is the extent to which they would be useful for building future aircraft.</p><br><p>We can use a similar definition elsewhere within this document.</p><br><p>When will _artificial _intelligence be developed? By this I mean algorithms and architectures designed specifically for a particular AI context, whatever it happens to be in the future. Such AI-algorithms would be relatively easy to create, and the software could be written in any number of AI languages or on any one of a number of AI architectures.</p><br><p>There are obviously many obvious special-purpose algorithms that could be used for artificial general intelligence. For example, it should be possible to do image recognition by a neural network without using it for the purpose of achieving human-intelligence-in-the-loop. Image recognition is just a single domain where we can test AGI; by the time we get to AGI, we'll have a much broader collection of algorithms that will have been extensively tested.</p><br><p>The question that actually matters is how far we can get with the algorithms developed, and how hard it is to build AGI that would work. A good starting point is this table, where I've been able to source some basic estimates of AGI-algorithms-development timelines. The table is broken up into three sections\xe2\x80\x94general, AI-specific, and meta-contingent:</p><br><p>There's no attempt whatsoever to reconcile these three sections in the table, nor is there anything in the text that attempts to reconcile them. I'm going to take these as rough estimates and try to compare them somehow.</p><br><p>What kind(s) of AGI will be developed?</p><br><p>General: These are the things that will definitely be developed if anybody can manage to build a sufficiently powerful computer. The key metric here is the amount of money (or other resources) that can be spent on AI research, and the total number of researchers working on it.</p><br><p>There is a common trope that a computer can beat a human at poker, say, because a computer can analyze every detail of the hand while a human can only think at the level of "I bet if they bet then they'll win." This is basically true in every domain where people have tested it, but it seems less and less true as the domain gets richer and more complicated. If humans had spent a hundred years working on poker tables and then computers had caught up to them, that would mean there are a lot more possible hands left to be analyzed than there are a hundred years left to be lived.</p><br><p>AGI is probably one of the few AI research areas where a human would still have an advantage over a computer (at least, over any sufficiently powerful computer). And there are a lot of reasons to expect AI research to continue in that state for a long time.</p><br><p>AI-specific: These are the algorithms that are closest to being ready to build "artificial general intelligence'. They aren't necessarily anything we can actually implement. They don't necessarily involve RSM either. The more meta the AGI, the less likely to be developed in time.</p>      </span>    </div>  </div></body></html>