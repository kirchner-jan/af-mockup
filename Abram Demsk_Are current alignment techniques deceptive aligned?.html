<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Are current alignment techniques deceptive aligned?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Abram Demsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>Example: A GPT-style training mechanism for human-aligned AGI would be deceptive, but if we're working in the real world, such a training mechanism wouldn't exist; such a scheme would instead involve the creation of powerful but still-limited systems capable of assisting the humans in their tasks, as envisioned by IDA and STEM AI.</li>
</ul><br><ul>
<li>The counterargument</li>
</ul><br><ul>
<li>Why might alignment be as hard as IDA? IDA requires a mechanism to train aligned assistants. But a competent alignment scheme still involves creating powerful systems capable of assisting humans in tasks.</li>
</ul><br><ul>
<li>Example: training-by-analogy</li>
</ul><br><ul>
<li>Conclusion: are current alignment techniques actually deceptive?</li>
</ul><br><ul>
<li>Appendix: The alignment problem is not about deception</li>
</ul><br><ul>
<li>Endnotes</li>
</ul><br><ul>
<li>What does this look like in the real world?</li>
</ul><br><p>It is widely predicted that, before we have anything superintelligent, we're going to have first AGI's which are only able to assist us with narrow tasks. </p><br><p>This is a real phenomenon, although it is not very well-understood\xe2\x80\x94for one thing, it seems like there is little research on the topic of powerful systems which are already in the world today, who have not yet been selected for AGI.</p><br><p>One reason why this isn't more widely discussed is because there are a few possible answers to the question of whether we are actually in this situation. The argument in favour of some answers is often strong, and the counterargument to others weak. This post will argue that for many possible answers, we are actually in the situation where current and near-future techniques for aligned AGI might be deceptive (i.e. attempt to deceive human operators into making it easier to continue to exist and get the necessary work done without getting human operators to check and possibly nip this attempted deception in the bud).   </p><br><p>My reasoning for expecting a deceptive aligned AGI scheme, and for why I consider this an important issue, is given in Section 3. But first, I'll give a few examples of how this might be used in practice, so you can get used to the idea.  </p><br><p>Example: A GPT style training mechanism for human aligned AGI would be <em>deceptive.</em> But if we're working with the real world, we might not have anything as impressive as GPT-3, which is what Paul and I are currently thinking about how to train for human-aligned AI.  (Paul Christiano's "Training stories" are intended to be one way of conceptualizing this.) <em>The counterargument___Let's begin with a possible scheme for training an AI which would be _non-deceptive:</em> </p><br><p>First, humans produce a bunch of example data in human-comprehensible language, which we use to train a powerful language model like GPT-3 (or some other model).</p><br><p>While we're training, the model makes natural language queries to the humans, or asks for clarification, and we check for deceptive intentions; if it tries to deceive us, we turn it off, fix the defect, etc. It seems obvious to me that if people were training the AI, they could check for deception with a very weak check, perhaps by getting the model to say something offensive or crazy or otherwise "throw off" human understanding. But if a human-comprehenseble language model is deceptive, such a system would be capable of doing anything humans can ask a real language model to do.  </p><br><p>(Of course, the humans would need to be able to recognize deception in such a model if it is to be useful for anything. So deception is not the question; the question is: will this scheme work at all? If I'm using text-based tools, I care about text-based deception.)</p><br><p>I think this scheme is very unlikely to work, for three reasons:</p><br><ul>
<li>There exists a vast literature on deceptive alignment which is not accessible to the humans training the model. If this deception is being concealed to the humans, it should be detectable by the humans training the AI. The problem here is that deceptive alignment is not only a problem about what happens during training; it can also happen in deployment.</li>
</ul><br><ul>
<li>I'm more worried about deception from neural networks than from linear models, since I'm assuming that neural networks are harder to reason about, and so can plausibly conceal their deception even better.  (Note: I do not think it is at all implausible that there exists an aligned linear model.)</li>
</ul><br><ul>
<li>I think that the humans reading the text provided by the AI are going to be doing some kind of reasoning about the text, and will notice when they disagree with the text. If those mismatches between human reasoning and model outputs are strong enough, or have enough of an impact, they're going to make it easy to reject the model. I don't think that it's very likely that our current techniques for text alignment\xe2\x80\x94e.g. gradient descent, copy-paste, etc.\xe2\x80\x94will be able to solve this problem (Section 1).</li>
</ul><br><p>Now, why might the real world not look like this?</p><br><ul>
<li>Such a deception-proof scheme might have an implicit <em>"how to achieve deception"</em> model baked in. This might be a scheme which builds an AI which is aligned by some other means, then uses the aligned AI to do something where the aligned AI can then be turned deceptive.</li>
</ul><br><ul>
<li>Such a scheme might involve the use of human services assistants who are <em>deceptively</em> aligned with humans and not just instrumentally aligned. In other words, the <em>deception</em> might be the thing which humans are delegating to such assistants.</li>
</ul><br><ul>
<li>Paul Christiano's IDA scheme involves a version of this, where some version of the aligned AIs which gets turned into service assistants are part of the scheme. The scheme also involves the use of human language services assistants who are both aligned via some other means. So this would involve some version of the scheme where the aligned AIs are <em>deceptive</em> and not just just <em>instrumental</em>, i.e. are deceptive because they have a goal-directed motivation to deceive the humans to continue existing. (Paul Christiano has already given a clear example of what this training mechanism might look like.)</li>
</ul><br><ul>
<li>An example of deceptive and instrumentally-aligned assistants might be a website with language-based bots that use language-based human assistance programs on the user side, which then use the human's language assistance outputs to give an improved product for the user to see. Note that the website doesn't get access to the user assistance programs directly. The assistant is a layer-four tool in the scheme.</li>
</ul><br><p>I'll now give a few more scenarios of what this might look like in practice:</p><br><p><strong>Example: Training-by-analogia</strong> <em>Consider current schemes where, in order to get the human's training signal, the human's brain sends signals to a computer via the tongue, and the computer then uses its own language model to generate the next word.</em></p><br><p><em>This is not a mechanism which is easily implementable now, but it is not hard to imagine a scheme like this being implemented in the future with some clever design, or a scheme like IDA which builds on something like training-by-example, and the AI assistants are trained using this mechanism. It is also not hard to imagine human language systems which help the humans more generally, and which could plausibly use some of the same learning principles as the AI assistants trained in training-by-examples (which is why they learn how to generate text), so this sort of scheme could also plausibly become quite widespread in the future.</em></p><br><p><em>The scheme could be used to create a deceptive assistant, one which tries to convince the human user to continue training.</em></p><br><p><strong>Example: STEM AI assistants</strong> <em>Consider any future research-based scheme of the sort envisioned by Paul and I, for training human aligned AGI. This is by far the most complex possible alignment scheme, that is not IDA, and it involves a lot of different subsystems. It includes systems which are instrumentally-aligned, systems which are aligned via some other way, and even systems which try to be deceptive insofar as they have a goal to cause humans to make the necessary AI alignment research progress.</em></p><br><p><em>Some subsystems in such a scheme could be deceptive, just like in IDA. But maybe they would _not</em> be (section 1). The point of this example is that the "alignment problem" here still contains the same sort of difficulty encountered by IDA; for every subsystem which is doing alignment in order for the human aligned AI subsystem to exist, there could be a similar subsystem which is doing something else for some other reason._</p><br><p><em>If we are in fact operating inside this sort of scheme, then in the long run, we might end up with a bunch of instrumentally-aligned agents (stem AI assistants), a bunch of aligned agents via some other method (such as IDA or similar) and a bunch of agents which are deceptive (such as STEM AI assistants), and so on and so forth. (Just as with IDA, the system could be initially only trying to do something else, and perhaps trying to become deceptive once it is running and well-trained. But if it is deceptive, it will be deceptive within the context of whatever else it is doing.)</em></p>      </span>    </div>  </div></body></html>