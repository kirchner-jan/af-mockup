<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Are all possible preferences consistent?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Stuart Armstron</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>The agent's goal</li>
</ul><br><ul>
<li>The problem of preference</li>
</ul><br><ul>
<li>Preference-consistency</li>
</ul><br><ul>
<li>An example</li>
</ul><br><p>This post is an attempt to solve a problem that appears in MIRI's research agenda (AN #58), focusing on the preference/\xe2\x80\x8breward ambiguity problem as used by Stuart at MIRIxLaB. The problem is that, even if preferences were perfectly known, there would still be no way for the AI to ensure that its preferences were consistent.</p><br><p>The agent's goal</p><br><p>Let the agent have preferences over the following finite set of states:  \xce\xa9={(s1,y1),...,(sm,ym)}. The preference function is thus mapping to  R+. For example, if the state is in {(x1,y2),(x2,y1),(x2,-y2),(y1,y2)}, preference is</p><br><p>. </p><br><p>Define \xce\xa0i(si,yi)=Si(xsi,yi), \xce\xa0x(x,y):=\xe2\x88\x91i\xe2\x88\x88{1,...,n}Si(xsi):=\xe2\x88\x91mi=1Si(xmi) </p><br><p>The problem of preference</p><br><p>The problem with this is that it is impossible for any agent to determine anything about the preference of an agent with a different preference function \xce\xa0\xe2\x80\xb2i(x,y).</p><br><p>Preference-consistENCY</p><br><p>Preference can be seen as a condition of consistency, the requirements of which are:</p><br><p>.</p><br><p>Note the importance of the condition \xce\xa0i(x,+y)=\xce\xa0i(x,-y), i.e. the agent must be indifferent as to the direction in which it moves along the (current) x-axis.</p><br><p>In that case the agent only needs to prove that if it changes the direction, it will change its preference. But this only means that there is a direction on the x-axis over which all its preferences are defined.</p><br><p>An example</p><br><p>Let us define the following set \xce\xa9, the agent's set of possible preferences:</p><br><p>The following two utility function U(x), i.e a utility/\xe2\x80\x8breward function, are consistent with this:</p><br><p>.  </p><br><p>To demonstrate the problem, let the agent have the following set of preferences over \xce\xa9 (where all rewards/\xe2\x80\x8bpreferences are assumed to be from that set), given the agent's goal</p><br><p>The problem is to design a utility function U(\xce\xa9) such that there is no possible agent i who would choose to maximise U(xi) while simultaneously maximising U(x,y), with y\xe2\x88\x89xi.</p><br><p>Preference -consistency</p><br><p>Preference cannot be expressed by any set \xce\xa0i of reward functions so the agent in the above situation will never be rewarded by finding an i\xe2\x88\x88{1...n} such that</p><br><p>That is, we want to find a single reward R such that</p><br><br><p>. </p><br><ul>
<li></li>
</ul><br><p>If \xe2\x88\x85 then all agents choose \xcf\x80\xe2\x88\x85\xe2\x88\x92(x,+,y,\xe2\x88\x92)=\xcf\x80\xe2\x88\x85\xe2\x88\x92(X,\xe2\x88\x92,y,\xe2\x88\x92) </p><br><br><p>If \xe2\x88\x83i\xe2\x88\x88{?} then all agents choose either \xcf\x80\xe2\x88\x85+i(x),\xcf\x80\xe2\x88\x85\xe2\x88\x92i(x,+,\xcf\x83i(y)),\xcf\x80\xe2\x88\x85\xe2\x88\x92\xe2\x88\x92i(x,-,\xcf\x83\xe2\x88\x92i(y)) or choose \xcf\x80\xe2\x88\x83i?(x,+,+,\xcf\x83i(i,y),\xe2\x88\x92) </p><br><ul>
<li></li>
</ul><br><p>At least one agent can be rewarded if  \xcf\x80\xe2\x88\x85(X,+,\xcf\x83\xe2\x88\x92,\xe2\x88\x92)=\xcf\x80?(x,\xe2\x88\x92,\xe2\x88\x92,\xe2\x88\x92).  That reward is the only one that all agents accept, so that is the only reward functions the agent can maximise; thus it must be optimal for at least one agent if  \xcf\x80\xcf\x89(x,++,\xcf\x83\xe2\x88\x92)\xe2\x88\x88\xcf\x80\xe2\x88\x85\xe2\x88\x92 </p><br><ul>
<li></li>
</ul><br><ul>
<li>If \xcf\x80\xe2\x88\x85i(x)=\xe2\x88\x85 then \xcf\x80\xe2\x88\x85\xe2\x88\x91i  </li>
</ul><br><p>The agent will never be rewarded for choosing to maximise U if \xcf\x80\xe2\x88\x851=\xe2\x8b\x82i\xe2\x88\x88{i,...,ni} </p>      </span>    </div>  </div></body></html>