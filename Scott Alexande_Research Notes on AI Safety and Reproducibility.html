<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Research Notes on AI Safety and Reproducibility      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Scott Alexande</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Link post</p><br><p>Contents</p><br><ul>
<li>Problems with AI research</li>
</ul><br><ul>
<li>What is this whole AI safety thing?</li>
</ul><br><ul>
<li>How do I work on AI safety?</li>
</ul><br><ul>
<li>Why do you say that we need more AI safety researchers?</li>
</ul><br><ul>
<li>Where are you coming from?</li>
</ul><br><ul>
<li>I'm not sure AI safety is even a thing.</li>
</ul><br><ul>
<li>I'm afraid of dying</li>
</ul><br><ul>
<li>I'm unable to get good feedback on my ideas</li>
</ul><br><p><strong>PREMISES AGENDA</strong></p><br><ul>
<li>Clarify: is safety research a thing? what do you mean "a thing"?</li>
</ul><br><ul>
<li>Clarify who the different subgroups of people working on AI safety are, and what they do. Clarify who has done good work; what's the state of the field; how do I find out how to do good work?</li>
</ul><br><ul>
<li>Clarify my role as AI safety researcher, and what I'm supposed to be working on. Clarify what the main open problems in AI safety are and what they might look like. Clarify what I mean by "reproducibility" in research papers and by the term itself. For example, is reproducibility-focused work like "How to Measure Anything" part of the field, or totally off-topic?</li>
</ul><br><ul>
<li>Figure out what I personally am good at, and what I want to learn more about and focus on. Clarify how I might get good feedback on projects, or ways to advance AI safety research, or how to write good research papers. Clarify what my own learning needs are.</li>
</ul><br><p><strong>PROBLEMS WITH AI RESEARCH</strong></p><br><p>So, I see a lot of research on AI safety. I see a lot that is very abstract and theoretical, and nobody seems to care that much about how it's meant to be used in practice. I see talks about how to solve AI alignment, and I see some really abstract and abstract-seeming methods; one talk had the whole audience applauding after I got bored and gave up on trying to figure out what it was all about.I've been reading a lot about AI safety on the Alignment Forum which is basically an AI/\xe2\x80\x8bML forum, because I hear I'm not the only person who doesn't understand the whole AI safety thing. But when I think about the actual content of the posts I've read, it's like these are so abstract, they don't even seem to be concerned with <em>actually building a tool that solves an actual real algorithmically tractable problem</em>. For instance, one of my favorite papers on this site is Value Learning, and they have so many words, and there's so many words of this nonsense, I can't even summarize it. I read it and I'm like "I don't know what any of the words mean, and even if I did, no idea how to implement this, it's just words!"I'm not crazy, other people have also read it and feel much the same way. And when I read it I know my life is doomed unless I can solve it somehow, because all the papers are making all these impossible demands on how to build good tooling for reasoning about the world without understanding what the world is. I think, "Okay, I'll make your life a lot easier and more bearable if you <em>just tell me what the problem is</em>."And this is the issue with most AI safety research I see. Nobody has any idea what the problem really is. And when I say nobody has an idea what the problem <em>really is</em>, I'm not saying they're wrong about the problem (because we do know what the problem is) - I'm just saying that they don't know what the problem they're researching.</p><br><p><strong>WILL AI FOOM?</strong></p><br><p>What the <em>hell</em> is the problem? I've recently been reading through <em>Superintelligence</em>, and part of the problem is the author's failure to clearly explain what the hell they are actually talking about when they talk about "human-level machine intelligence."</p><br><p>I'm probably going a bit meta here, but the first question I ask anyone looking into AI safety is "Why do you think AI is dangerous?" or "Why do you want to see AI go FOOM?"  For a while there were a lot of people saying "FOOMing means a non-linear explosive takeoff" or "FOOMing would be a catastrophe". It kind of fell apart as it became increasingly clear that exponential growth isn't intrinsically dangerous. I remember reading once that people in the '90s thought FAI would probably be achieved using recursive self-improvement, and now they think <em>that</em> probably isn't the best route to FAI.So what the heck are we talking about here when we talk about AI fooming?</p><br><p>This problem is similar to the FAI problem in another way. Before the AI foom problem, most arguments in favor of AI foom tended to be "We can't stop AI progress, we may not be able to slow it down, but we can contain it."  And most people's response to that was some variant of: "Well, what do you propose for containing this AI, exactly? That it can't be used for anything dangerous? Seems pretty dangerous to me, actually."But now that the AI foom hypothesis has been ruled out, there are two different problems here. For one, the AI foom argument is itself a form of "AI will be unsafe". It's saying "AIs, when they get very powerful, are not safe."  So this is in tension with the other FAI argument that the AI will be safe. And this again is a case of the author's failure: they didn't clarify that the AI fom problem was <em>not</em> the same as the AI'safely' problem as a safety concern, which then led to the confusion that the author pointed out in the section above.The other problem with the AI foom concept is that it's not clear how to build one. There's a few different ways to imagine how AI foom could come about (see Superintelligence: Paths, Reflections, Strategies, for arguments on various ways):</p><br><ul>
<li></li>
</ul><br><p>Some sort of recursive self-improvementation where the improvement comes from the AI improving its own intelligence. The problem is with the recursive part, not the self-improvement part.</p><br><ul>
<li>
<ul>
<li></li>
</ul>
</li>
</ul><br><p>Evolutionary self-improvement. Evolution works on small units of information, like an enzyme, but humans can't afford the time to work on things that are this large.</p><br><ul>
<li>Or it could come from brain emulation and running lots of copies of the same person. I just don't think there's a strong argument for the brain being any different from a program: they can probably be emulated in the future much more cheaply than brain emulations are now, and even with the most expensive brain emulations you can set up today it's unlikely brain emulations will be anything like how good emulations of humans are today.</li>
</ul><br><p>None of these problems are particularly deep, but I want to make them because I keep seeing a lot of AI safety papers that seem to take the 'if AI gets really powerful it will be dangerous' assumption <em>so</em> for granted that the person who wrote the paper isn't actually making a proposal for how to prevent AI foom.</p><br><p><strong>WHATEVER "AI Alignment" IS</strong></p><br><p>Okay, but we're not just going to go around and try to prevent AI from becoming unaligned, right? The field of AI safety, and particularly this subgroup of it, talks a lot about how to prevent it from becoming unaligned.If it is part of the purpose of AI safety to ensure that AIs do not become unaligned, why are we talking all this nonsense?The following is a list of things I can think of off the top of my head. Not all of them are important, and not all of them need to be part of AI safety.</p><br><ul>
<li>The way the people who study these fields talk about themselves and their mission (e.g. the AI Alignment Forum, the Machine Intelligence Research Institute, etc.)I'd put most of the people at MIRI and CHAI onto this list, since they're the biggest, but there's also quite a few others, such as Nick Bostrom at Oxford, Eliezer Yudkowsky at SIAI, and a whole bunch of people at FHI who study and write about all these ideas.</li>
</ul><br><ul>
<li>The way they talk to outsiders.The other AI safety people don't seem to have much of a sense that, if something looks like it might be part of the mission, that they should be clear about that. I think the most blatant counterexamples are Eliezer Yiddkowsky and Nick Bostrom, both of whom say things like "I have a job to do" whenever they mention working on AI safety. If their job is their main purpose in studying AI safety, shouldn't they mention it more if they can.</li>
</ul><br><ul>
<li>The ways they talk to other AI researchers.If AI safety research can make AI researchers more careful and cautious, that's probably good for everyone. Nobody in the AI alignment thing explicitly believes or believes not that AI alignment would make AI more careful and cautious \xe2\x80\x93 they just want there to be as much awareness as possible of all the ways in which the field is confusing and/\xe2\x80\x8bor misleading about how scary it is, so new researchers coming into the field or old researchers thinking about doing things they might regret won't have any easy way to think about them.</li>
</ul>      </span>    </div>  </div></body></html>