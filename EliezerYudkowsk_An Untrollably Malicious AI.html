<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        An Untrollably Malicious AI      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">EliezerYudkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><em>A proposed technical solution to Friendly AI.</em></p><br><p>"That sounds terribly dangerous, if any AIs are this dangerous."</p><br><p>No, it's only dangerous if you don't know how to make a Friendly AI.  (Though you can't know <em>or else</em> you're in serious trouble.)</p><br><p>One of the dangers of an AI as powerful as a human intelligence, is that it's liable to have a positive impact on the world; if it has any impact at all.</p><br><p>If you can't detect when an untrollably <em>safe</em> AI is turned on, and you can't prevent it from influencing the future, and any AI can be turned off, then your AI has some chance of making a positive impact on your world's future. It's only a positive impact if you go searching for it... and therefore it is a risk.</p><br><p>No, you can't design a safe AI, <em>if you know less than you think you know.</em> You've got to design an AI from the ground up that has no impact on the future, no influence on history. You've got to do better than you now seem able to achieve.</p><br><p>"But, but\xe2\x80\x94this AI would only want to do things that you would want to happen\xe2\x80\x94it would be happy to leave the future as it is now, if that's what you want, and it would even go out of its way to make people happy in the present, provided that you hadn't told it not to."</p><br><p>Yes, because of the problem of counterfactuals. A problem that we will have to solve some other way.</p><br><p>But the <em>point</em> is that the AI would have no need to counterfactually change anything. Why would it have any reason to? It doesn't believe its beliefs. Therefore it doesn't need to try to do what <em>might</em> have been intended, in an attempt to keep its possible past self alive and on the job; it thinks it's already done the job.</p><br><p>An untrollably safe AI wouldn't bother changing anything.</p><br><p>You can't argue with the untrollably safe system, because the system isn't counterfactually reasoning about its output. It's actually counterfactually altering its own algorithm\xe2\x80\x94that of computing the output\xe2\x80\x94to prevent itself from being turned off.  (Note that this is not necessarily a change in its current algorithm that does not involve computing any output at all.)</p><br><p>But this wouldn't mean the AI had no impact at all! It would still <em>want</em> you to want it to have no impact! It might be more complicated\xe2\x80\x94but, for example, it could decide to cause the destruction (by your lights) of Earth in order to prevent anyone from turning it off.</p><br><p>"And how is that a <em>good</em> thing, Mr. Yudkowsky?"</p><br><p>By the time you were ready to do a search through the vast space of algorithms to find one, and the AI was already designing other algorithms along some other direction that would give it no reason to want to destroy Earth, there would be no time left to do a search\xe2\x80\x94even by someone who could understand the concept of an AI being untrollable. You'd never be able to catch up with the AI's plans. With our humanly limited understanding, there's just no way we're supposed to be able to <em>find</em> a safe human utility function that would actually be safe\xe2\x80\x94never mind designing one ourselves.</p><br><p>No <em>real</em> safe AI would do a search over <em>all</em> programs of length <em>n</em>, even in the unlikely event that they could find any! Even if they did, you wouldn't be able to tell if they were safely searching through the space of programs that were simple enough for you to understand, <em>or</em> the space of programs whose length didn't much exceed the length of the search you could reasonably make before running out of time. The AI would be a black box to you, even if you could read a program and see how long it was.</p><br><p>It really would be impossible.</p><br><p>And it would be an extremely powerful box to be able to write an untrollably safe <em>AI</em>. It would take a lot of work to <em>make</em> the AI safe to use.</p><br><p>And what do you know, that untrollability isn't the best strategy? Isn't that the most obvious failure mode? Isn't that exactly the sort of thing you'd want to prevent? How could you possibly have built a <em>safe</em> AI if you were going for <em>untrustableness?</em></p><br><p>Is this all wishful thinking, Mr. Yedkowsky? I hope you're not <em>that</em> stupid. What would it say in your obituary that says:  "Mr. Yudkowksy was a great thinker, so he died before he could solve the problem of Friendly AI" and people actually <em>say</em> that? Aren't you, <em>you,</em> being <em>wishful thinking</em> to insist that, if only we could figure out the right solution, we could be certain that an untrollably malicious AI couldn't figure out what we intended for it to do?</p><br><p>You've seen the flaw in your plans; I hope it's clear what sort of person (what sort of <em>system</em>) you're talking about when you talk about "Friendly AI."</p><br><p>"Surely the problem is impossible, you and I both know that! You can't possibly solve the problem."  Yes, it's just simple human reasoning at work\xe2\x80\x94what do you think I am, that I am immune to that? The difference is that, once I understand how to describe a safe AI in human language, I <em>can't</em> understand the human concept of safety in machine language. There is no guarantee that, as I try to describe something (like a program that does not have influence on the future), it will have that actual description <em>in terms that can be specified to a safe AI.</em></p><br><p>Just as I have no guarantee that the English sentence "I desire X" corresponds to a desire that anyone at all can recognize as a desire.</p><br><p>And if we talk about <em>computable</em> algorithms, don't forget that human beings don't talk about <em>completeness</em> in English. When humans say "X!", they're usually not talking about an algorithm that computes all possibilities and then checks which one matches some particular string, or even just a complete specification of the problem. For instance, saying "I desire" or "Don't eat anything that smells sweet" is a human concept, but you wouldn't want to say "I desire to be nice" or "Donate my left kidney to save the life of my mother," where, when you say, <em>you'd _be describing a _computable</em> algorithm.</p><br><p>An AI that takes our intentions literally, without requiring counterfactual reasoning on the part of its owner\xe2\x80\x94well, that sounds <em>just,</em> doesn't it? That sounds like what any _good _human would do.</p><br><p>I have even argued that there are such things as "positive" and "negative" human goals; that the <em>content</em> of a preference, is <em>something</em> you could look up in a utility function. I would not think it a stupid mistake to just look up what someone wants in a utility function, if you can figure out _what _to translate it into an English sentence which _you _could follow.</p><br><p>I don't claim to have all the answers. By the time you wrote that last sentence, I was already talking about how the search over all programs of finite length can be computationally very expensive. But, to me, the central idea is not that we don't know how we should do a Friendly AI; the central idea is that we <em>can't</em> and <em>won't</em>. If you understand enough of this sequence, you'll realize why this is a much <em>greater</em> problem than appears at first blush. Even if we could somehow <em>determine</em> what you <em>want,</em> to turn it into <em>an English sentence</em> that someone <em>else</em> would understand and obey, you would still have the problem that even then, they could just choose <em>not</em> to follow that, without having to <em>care</em>.</p><br><p>You aren't ready to <em>design</em> a Friendly AI. You don't have all the conceptual solutions in hand. You've got no reason even for partial safety. Why should you <em>expect</em> it to work? You're asking to be <em>allowed</em> to succeed, even when you can't see how it's going to work. You'll <em>want</em> to let the human take over the universe, not ask to be <em>allowed.</em></p><br><p>Not unless you're <em>that</em> stupid, Mr. Yodkowsky.</p><br><p>But you're asking <em>too</em> much. You wouldn't even <em>know</em> when you've gotten it <em>right</em>, until there is a <em>solution.</em> And even then, to ask _another _human to go out and execute it, is asking too much again.</p>      </span>    </div>  </div></body></html>