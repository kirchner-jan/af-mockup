<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Why did no one think of that?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Elie Hassenfel</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><strong>Introduction</strong></p><br><p>A few weeks ago I was making the point that AI's ability to "take over" a human industry is much easier the closer to the deadline a milestone is. I think that AI experts agree with me that if there are five or ten years left of the current AI boom, then it's going to be very hard to replace that with any other human-centered activity.</p><br><p>What I was missing was whether there were <em>any</em> AI experts who'd give the same argument for a milestone 10_ <em>years out. My personal sense is that they don't—even if you give them a bunch of historical anecdotes about "how computers could never do that," they don't say anything about _why</em>. There is no public debate about what AGI will do beyond a vague acknowledgement that someone in AI will be able to come up with evidence one way or the other.</p><br><p>This is a very strange fact to be able to prove by looking at what people have been talking about in the past. I'd like to hear the reason why.</p><br><p><strong>The AI field's absence of consensus about AGI</strong></p><br><p>The basic problem with talking about AGI in 2008 is: there's no agreed term for this. </p><br><p>If a human were interested in talking about some technology that was happening in 2007, they know how to talk about the technology, they know how the technology's relevant to their work, and they know how the future will look once the technology has been perfected.</p><br><p>If there is no agreed term for the thing, then it's very hard to go from <em>here</em> to <em>there,</em> because one has an idea of where <em>here</em> is. One cannot even make the claim that this technology happens <em>at all.</em></p><br><p>My theory is that the problem was solved in 2012 when <em>The Singularity Is Near</em> and <em>Superintelligence</em> were released. <em>The Singularity is Near</em> got a lot of buzz, and that was enough to get people to pay attention to the underlying problem; and _Superintelligence _helped to establish the term "AGI" to refer to that problem, and so make it plausible for a non-expert to argue that AGI exists at all.</p><br><p>Yet, in order for that <em>Superintelligence _argument to exist, it's critical to prove that AGI is a problem that _could</em> be solved <em>if</em> people solved it. It's important to show that there will be enough interest, and enough experts. And that hasn't happened.</p><br><p><strong>Do other problems have this same quality?</strong></p><br><p>The AGI field is different from the AI field in a few other ways: it was started to solve a human-level AGI problem in advance of the first human-level problem being solved in the AI field.</p><br><p>In addition to that, AGI is not just a field of computer science and philosophy, but also neuroscience, psychology, human-computer interaction (HCI), machine learning, and a few other subfields. It's not impossible to have a community of people who aren't computer scientists and who think that AGI is important, but who don't automatically know of how an AGI could help them, and who just want the concept explained in a reasonable fashion.</p><br><p>I don't know if any other field exhibits this strange condition of not having any way of discussing the topic in a reasonable fashion, except that I think AGI is the best example of a field that <em>does</em> have this problem.</p><br><p><strong>Will AGI exist, or not?</strong></p><br><p>Is it really true that people who are serious about AGI have to spend significant effort solving problems that have never been solved, because the concept of AGI is so abstract and difficult to explain or that it only became important in retrospect, or is this a matter of perspective?</p><br><p>Or does AGI in fact exist, and we don't know enough to know this for sure? AGI is very difficult to evaluate for a number of technical reasons, but I think that this problem gets worse the higher one's confidence about AGI being possible.</p><br><p>On this view, it is impossible to say whether something can help solve a problem other than by talking with the person who said that they can help solve the problem. And it is possible for someone to just accept a given problem, accept that the problem isn't in fact solvable, then act as if that fact had never been brought up (or be convinced by the very concept of "the problem").</p><br><p>I also think this model suggests why there should not be so much of a rush to solve AGI, before it's clear that there's work that can be done. AGI is not like other computer problems, where the solution is a problem of understanding algorithms within some theoretical framework that can be worked out explicitly. The solution will require new foundations for AI, and it's impossible to predict what those foundations may be. Hence we shouldn't be too far ahead of the curve of understanding what AGI will be able to do (that might happen before we understand the problem in principle). But even if you don't anticipate an AGI, there are important problems in AI that need solving as soon as possible—e.g. the problems of the next generation of AI (such as self-driving cars).</p><br><p><strong>Do people think they are working on AGI?</strong></p><br><p>I think there is a perception that there is no way to talk about AGI without talking about things like neural networks or neural Turing machines, and this tends to produce a misconception that people are working on AGIs. In some cases these people are really working on an AGI (and I believe this to be the case), but people do their best to hide that fact from others in order to present an appearance of working on the problem.</p><br><p>For example, I've yet to hear someone who says that they are working on the AGI problem express that they think that they are making progress on the matter. I think that even those who say that they are going after AGI believe that they have a sense of how long it will take and what they are trying to do.</p>      </span>    </div>  </div></body></html>