<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Reframing Superintelligence: Comprehensive AI Services as General Intelligence      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Richard Ng</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>The AI Safety-Oriented Community in 2018 and 2019</li>
</ul><br><ul>
<li>AI Risk and Opportunity</li>
</ul><br><ul>
<li>The AI Debate</li>
</ul><br><ul>
<li>How could Superintelligence have occurred in 2018-2019, without anyone recognizing it?</li>
</ul><br><ul>
<li>Superintelligence: Paths, Dangers, Strategies</li>
</ul><br><ul>
<li>In summary:</li>
</ul><br><p><em>This work was completed at</em> <em>Google</em> <em>as part of the</em> <em>Google AI Residency Program. I'd like to thank Ben Garfinkel, Richard Ngo, and Buck Shlegeris for their encouragement and support. Thanks also go to the Google resident team in London (especially Owain Evans and Jack Koch), and Evan Hubinger for feedback and mentorship.</em></p><br><p>In Superintelligence: Pathways, Dangers, and Strategies, Nick Bostrom and Eliezer Yudkowsky argue for research priorities that take superintelligence seriously. By way of contrast, Scott Garrabrant and Nate Sores have argued that the threat posed by superintelligence is a distraction from other, even more urgent problems. In this post, I'll take Scott's side, defending <strong>the possibility of transformative AI through a general intelligence explosion</strong>. I have a lot of criticisms of Scott and Eliezer's "superintelligent fast" and "superintelligent slow" scenarios, but I'll save them for "Superintelligence: Paths".</p><br><p>My argument is roughly as follows. First, we have good reasons to think that a self-modifying superintelligence will be able to make itself much smarter than humans. Second, we have some reasons to think that this intelligence enhancement will eventually cause the services of the service economy to be surpassed. I expect that this will be good news for humanity, but bad news for individual humans, whose marginal contribution to the service economy is much less than it appears; the result will be a world with much less inequality. Finally, I argue that the prospect of superhuman general intelligence is an important consideration in debates about the nature of AI's impact.</p><br><p>It's really hard to write about these ideas. I'll try to provide context by giving a small personal account of why I believe in them. Then I'm going to go on a mini-rant to explain why I think that there's an asymmetrical way things could play out such that the first person to build a general intelligence has a significant influence over the rest of human civilization. If that happens, then it seems very possible to me that the first person will be working in a service sector that doesn't much depend on their output. When this point comes up, I end up feeling like this is a key asymmetry which shouldn't be ignored. This asymmetry is an important one, not just from the perspective of avoiding the negative consequences for individual humans, but also from the perspective of creating a stable coordination mechanism that humanity survives as a stable collective. My argument is essentially that I'm very nervous about an AI singularity because it makes me think that <em>eventually</em> this will be the result of a long-term process, and not one where the first AIs are created by teams of highly integrated engineers on tight deadlines.</p><br><p><strong>Part 1: My background and research</strong></p><br><p>Until very recently I didn't think I was a superintelligence-oriented person. My main goal had been to make really solid progress on questions around the near-term AI safety problem. In retrospect, this has been very naive. It's an example of focusing more on what felt immediately tractable than on what I expected would be useful. A big factor was that it took me several years to make a lot of progress on some of these questions\xe2\x80\x94I wasn't <em>trying</em> to make progress on long-term AI safety problems until I was in a position where I could do so. And to tell the truth, I had some cognitive blind spots around the general nature of the problem. The general lesson should be that thinking about the far future can be very worthwhile even if it takes a long time to get anywhere, or if it feels like it's a distraction from other things you should be working on. I've been pretty lucky with regards to the questions I have been working on: over the past years I've now written a lot of about them in various forums and online, and I expect I now have a deep understanding of their arguments and solutions.</p><br><p>I did some research on the AI Safety-Orientated Community in 2018 and early 2019. Scott's blog post is basically a chronological account of what I'm calling this time period. It's been described by some people as "a moment of weird opportunity" and "a weird moment of weird opportunity", so we'll call it "a weird moment".</p><br><p>I think there's a lot of overlap between people whose research I respect or who I have interesting conversations with, and the people who are the focus of the post Scott wrote. The only difference is that I'm much better at thinking about the long-term impacts of AI. So my model of this time period is that I have been focusing on my research, and a small number of other people are also working on research on AI safety. This means that people I talk to in the future will be a combination of different people than today. I think there's a decent amount of correlation between people who I'd expect to be good at AI safety and people in the AI Safety-orientated Community as a whole. But it's also the case that most people working on AI safety-type problems are <em>not</em> currently associated strongly with the AI Safety-oriented community, and most experts within it were involved in work which predates this time period. So if that makes it harder to form new friendships, I'm more sympathetic than I'd have been a few years ago.</p><br><p>I think it might be useful to divide into two categories:</p><br><ul>
<li>People who are directly working on this problem as an employee, researcher, project lead, etc.</li>
</ul><br><ul>
<li>People who read AI safety-oriented posts, have conversations with people in the community, and think in a way which feels informed or connected to them. This will likely include a lot of people who aren't affiliated with the community and aren't actively trying to help with AI safety.</li>
</ul><br><p>In particular, I think it's important to keep in mind that the AI Safety-Or-What-Ever-It-Calls-Itself is different from AISFP. It has more to do with people thinking in novel ways to solve AI safety problems (which is really hard to do!), and is informed by the broader context of the times.</p><br><p>If you look at this time period as a whole, the most visible AI Safety-O-R-I-N-People were Ought, Anthropic, and CHAI, which is the name I'm most used to pronouncing. I think Ought is probably the most interesting of these three organisations, but because of an internal reorganisation, Anthropic probably has more of the AI Safety-related staff with them now. Anthropic's most exciting work is probably the Ought Summer 2022 Program, from July-August. Anthropic also organised the Solstice 2019 and is running another again this year. Ought's biggest project is probably the AlphaFold series, which has gone a bit of a controversial way, but nonetheless produced interesting results. The most valuable of these results is AlphaFold 2, which generates an entire report, <em>even more extensive than OpenAI Five</em>.</p><br><p>I spent a lot of time interacting with these three organisations, both as a programmer and as the lead of my former AI Safety team at Anthropic. My impression is that they do a good job, despite the fact that I was not always on the same side on questions which might divide people in the community.</p><br><p>So for example, I was not excited about Ought's project for one reason or another; after talking it through with other people who thought it was important, we came to the conclusion that my model of Ought was wrong. One thing I learned: I often feel that the thing other people are doing is not really about their stated goal, but really just an accidental side effect. So when there's a project I don't really want to be involved with, it's usually because I have a low opinion of the people or organisation doing it.</p><br><p>I talked to people in both the Solstice and the Ought summer programs (I did the latter because I wanted some interesting conversation with people, but not so much because I expected them to provide a direct benefit to AI safety). I got a lot of value out of spending time talking with a mixture of people who were directly working on the problem as employees, volunteers, or project leads; people who were a little more distant, but read a lot of AI safety-oriented content; and people who weren't working directly on this problem, but were reading enough to have useful thoughts.</p><br><p>It seems like in other years, Anthropic, Ought, and CHAI have probably have spent significantly more time talking to each other than they have to Google or LessWrong. And I think that's a shame. It's not obvious what exactly they would talk about: in-jokes, technical disagreements, disagreements about the current state of the field, etc. I see a lot of value on all sides in bringing different people with different perspectives into contact with each other and different sources of evidence.</p><br><p>If we try to take this time period as one week in late 2018 to early 2019, I'd probably consider that more representative of the typical LW mindset than of the AI Safety Community, which is more diverse and spread across many different people and organisations.</p>      </span>    </div>  </div></body></html>