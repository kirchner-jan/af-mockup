<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Reflections 20\\xe2\\x81\\x842010      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Eliezer Yudkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>The Singularity</li>
</ul><br><ul>
<li>Notes from Summer 2010</li>
</ul><br><ul>
<li>Personal</li>
</ul><br><ul>
<li>Technical</li>
</ul><br><ul>
<li>In-the-Wild</li>
</ul><br><ul>
<li>Other</li>
</ul><br><ul>
<li>Personal</li>
</ul><br><p>On September 30 2010, three years ago, I posted the results of my research trip to the Singularity Summit in San Francisco, which brought together a large number of people interested in transhumanism or futurism, or interested in Less Wrong or SIAI in general. I've been writing up some of my observations from that trip, and posting them as I complete them (so as not to clutter the Archives).</p><br><p>I recently finished what was the final piece of my <em>Artificial Intelligence as a Positive and Negative Factor in Global Risk</em> sequence, "Facing the Singularity." My next posts will be "Notes from Summer 2010" and "Personal."</p><br><p>Since I used this trip as my own research into things that I <em>already knew</em> about, there was no reason not to share.</p><br><p>This was, in retrospect, an exercise in overconfidence. I made the trip with the belief that it was important:  I thought that it would advance both rationality and transhumanism, by providing an opportunity for discussion about such things as Friendly AI. The conference was billed in the introduction as "the first opportunity for the Singularitarians and the Transhumanists to meet and discuss."  I didn't look back and say to myself, "Oh the folly of my ways and what a waste of time all this was. I would have been less overconfident if I had only followed my first impulse to investigate the place out of curiosity."  But no. I went, and it <em>helped,</em> and I went in the hope that this was something important that I hadn't yet <em>tried</em> to do; and therefore I was <em>not disappointed.</em></p><br><p>Let me first say that the conference itself was <em>really fun.</em> A friend of mine afterward quipped:  "We played so many games of Taboo this weekend that when I finally saw people at tables and heard them speaking I didn't recognize them."  (This joke should get a comment, but I'm not going to copy it verbatim\xe2\x80\x94to my embarrassment, I didn't know the punchline <em>myself</em> until later.)</p><br><p>If you're at all familiar with transhumanism you may recognize the term "futurology" used at that conference. The two of us exchanged brief glances; and one might say, "Of course transhumanists don't <em>hate,</em> they <em>love</em> futuristic technology and want to experience the future for themselves\xe2\x80\x94they're practically <em>beacons in the night</em>, not <em>evil mutants</em>."</p><br><p>To which the Futurologarian Reply might be, "But what if we get a <em>bad</em> future? Then what\xe2\x80\x94greet people with cold eyes when they go to sleep at night? You think the future will be <em>bad</em>, but it's not going to be <em>you</em> who gets to decide whether it was <em>bad</em> or not."</p><br><p>Now, this is not to say that some of the people who were "present" at the conference were particularly <em>unfriendly;</em> but I've found it useful as a framing for thinking about my trip, to think of myself and the others as "futurer"s, and then to wonder about what we could say to the "transhumanists" who were skeptical and worried that a Friendly AI project would actually <em>work.</em> Like how you might say to a group of anti-vaxxers, "Wow, I've never seen so many skeptical and worried people in my life! Isn't this interesting?"  So here it goes\xe2\x80\x94though I'll also leave out some of my own remarks, which you may find shocking enough.</p><br><p>The Singularity</p><br><p>A bit of background:  There are at least <em>two</em> ways of conceiving of existential risk. In the first way, you imagine an AI that is sufficiently powerful and self-improving that it will take over the world. In the second way, you imagine a <em>catastrophe</em> of some sort, beyond the control of any particular AI, that causes the extinction of consciousness from the cosmos and everything in it. (In this view, the risk comes from the AI <em>failing to help</em> when the thing beyond its control goes wrong.)</p><br><p>But the term "transhumanist" has an interesting way of being used. We don't <em>define</em> it as <em>an AI that will save us _from _an _AI being sufficiently powerful to save us</em>. Instead it's more like <em>transhumanism</em> is the idea that we can change ourselves, become stronger and smarter and so on. This can be an argument in favor of <em>AI</em> risk\xe2\x80\x94that if you think that AI risk is a real thing, you should concentrate your efforts so that AI doesn't take over.</p><br><p>And if you hear some transhumanist saying "We're going to solve aging," then their meaning is not "We're going for a genetic approach to curing aging," or even, "We're doing this to make people live longer, so it's fine that they're still likely to die of old age in any case."  Rather, the meaning is "We're curing aging here; now what's the next thing we can come up with?"  We see what they're doing, and we're like, <em>We want it too!</em></p><br><p>Noting this, you may think that I am using the term "transumanist" rather broadly. But I still feel comfortable using the term, because I'm careful to say that I'm pointing at a <em>commonality</em> among people who don't agree with the conclusion of the phrase which describes a certain thing that they do, not a mere connotation of the word. There are people who are transhumanists without believing in AI risk\xe2\x80\x94or even who take a position on the risk. Indeed, I would venture as far as to say that <em>conceiving</em> that there might be more than one existential risk, and that we were in danger of falling into a Singularity, was probably one of the more <em>transhumanist acts ever undertaken</em>.</p><br><p>But this doesn't mean that these transhumanists think that a <em>Singularity</em> is a risk of AI being powerful enough to survive. It might sound like a contradiction for me to say that a Singularity is an existential risk, but I'm <em>not</em> saying that transhumanists are worried about the possibility of artificial intelligence.</p><br><p>Notes from Summer 2010</p><br><p>As always, this section will continue my summaries of papers and people, but with the important proviso that I'm now writing in the third person, as a <em>different person</em> from the one who summarized. If you had <em>expected</em> it to be me who did the summaries, please feel my apologies for writing this way.</p><br><p>The list below is longer than I have time to describe here, and may not get around to it in the time remaining; but here it is. I could not read the papers in any case. If you can read any of them\xe2\x80\x94or, better, if some of them help fuel your curiosity\xe2\x80\x94please let me know or email me.</p><br><ul>
<li>Asimov Institute for Science and Enlightenment\xe2\x80\x94Michael Anissimov:  "Non-person Predicates" (full paper available)</li>
</ul><br><p>Personal</p><br><p>That trip was a vacation, and as such it was all a waste of time; I would not recommend it by any means, save for the sake of future trips. It made me realize more than before that I did not have to do all the things people who claimed to "work" on Friendly AI did; it was enough to just talk about things that seemed important to me.</p><br><p>So. To sum up, I talked with (almost) everyone who had posted something to <em>Less Wrong</em> since 2006, asking a single question:  "Could you solve Friendly AI within the next 30 years, or would you need to get a grant?"  This way you didn't have to meet everyone; it was like the LW Singularity Summit 2.0 without the sleepwalking. The response would tend to be like "Huh? I never heard about that\xe2\x80\x94" or "Sure. Of course I could solve it. That's a trivial programming task\xe2\x80\x94" or "Let me guess, you'd start talking about how it's not hard if you have enough money."  This would help give you an idea of how <em>un-surprising</em> it was that most people couldn't solve the problem.</p><br><p>But just because you couldn't solve the general problem in 30 years doesn't mean you shouldn't try to solve it in three months.</p><br><p>At the same time, I talked to transhumanists who disagreed with the idea that there might be any real threat from AI at all, or that there was a significant probability of the problem being solved in the next thirty years. Most of these people didn't seem to <em>realize</em> that the existence of such people at all meant there had to be at least some work being done toward solving the problem.</p>      </span>    </div>  </div></body></html>