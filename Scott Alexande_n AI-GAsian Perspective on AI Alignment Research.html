<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        n AI-GAsian Perspective on AI Alignment Research      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Scott Alexande</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>__Related to: __Is Rationalist AI Risk-Awareness?</p><br><p>There's been some discussion lately about whether the Singularity Institute is risk-averse or not. As a reminder, <strong>risk aversion</strong> is the desire to avoid possible future losses. <strong>risk-aversion</strong> would describe the inclination to avoid high-probability risk.</p><br><p>This is an interesting dichotomy in its own right, and I would certainly welcome further discussion on this topic. But I feel that one thing it misses, at least on the SI blog, is an explicitly <em>narrow</em> discussion of AI risk: the SI blog talks about AI risk without explicitly talking about the Singularity, the Singularity Institute, or the "intelligence explosion". If I go to a group discussion at my local Less Wrong hub, I'll have plenty to say about intelligence explosions and the Singularity because these are part of what my local community thinks about, but if people at the SIAI blog tried to get a group discussion about AI risk, it would probably fail because they wouldn't have prepared very hard to do so, and they wouldn't have discussed some things because they might sound crazy or offensive.</p><br><p>This community, unfortunately, has a track record of having discussions which fail to survive contact with mainstream intellectual society. I've heard that the Sequences were largely intended to counterbalance this tendency, but for whatever reason it was never realized by the community, and the Sequences languished and died away. (I could be wrong here, of course.) We even have a <em>Less Wrong</em> subreddit of r/\xe2\x80\x8bSIAI_fan. _</p><br><p>It would be nice, then, if there were a group blog on the SIAI site which was explicitly dedicated to the risk of AI technology developing beyond human level, and which took its ideas seriously enough to have interesting discussions about them. Ideally, such a blog would have a link on the front page that sent people to the top-level post "Narrow AI and the Singularity Institute (nominative determinism, obviously."  And by the same token, the _r/\xe2\x80\x8bSIAI _subreddit seems like a poor fit for a community blog devoted to AI risk.</p><br><p>But there's another problem with the SI blog. A lot of the "content" has been written by Ben Goertzel and Anna Salamon. These are some pretty intelligent individuals, but their contributions have a bit of a cultish tinge to them. They are the people who are most likely to think the worst about SI, a group of people who probably already think the worst about many other things: the atheist/\xe2\x80\x8blibertarian/\xe2\x80\x8bSF/\xe2\x80\x8bTranshumanist/\xe2\x80\x8brationalist/\xe2\x80\x8betc crowd. This makes them likely to misunderstand SI's critics. Ben, in particular, is very much in that mold, which makes them unusually vulnerable to confirmation bias (we are people who think the worst of the human species, after all!).</p><br><p>Now, maybe none of this matters. Maybe I'm overthinking it. Probably it matters, sometimes people are really dumb, but I could still benefit from hearing from people who disagree with SI in general. I need somewhere to find intelligent people who are genuinely curious, and who want to read SI's material and discuss their merits and demerits.</p><br><p>So that's where <em>The AI Alignment Problem</em> comes in. It's my intent to try to form a community blog by and for AI risk people, who could be from any of the groups on the SIAIBetter and SIAIworse lists, and be from all ages. Hopefully, when it was done, it would look like something like this:</p><br><p>The first page of the blog, where you would see the links to everything else on the site. You would see something like this:</p><br><p>Some of the links would be longer, and some would be shorter. Probably the links to the most popular posts, as judged by the total pageviews they get, would be longer than those to the least-popular posts. But if some links get 1000 pageviews and some links get 10000 pageviews, that's also fine. Ideally, we'd like the blog to have such a diverse set of posts that there's really something for everyone, no matter which posts you most or least prefer.</p><br><p>We have a lot of topics we'd like to talk about, from Artificial General Intelligence to Friendly AI, from the Singularity to Machine Consciousness, from Algorithmic Decision Theory to Computational Rationality. But it would be nice if there was a community blog dedicated to discussing these topics. And not just the _big-picture _contents of the Singularity Institute's big publications, but the low-level detail as well.</p><br><p>And if that blog becomes successful, it could then attract more people with like-minded views, and maybe eventually expand to attract more people from various non-AI subcommunities. I don't know if there's such a thing as "mainstream AI risk", but if so, one of the reasons I wish to see a blog is to help establish "mainstream AI Risk", by putting an explicitly <em>n</em> A in the title: _n_ai_gaian_perspective_on_ai_risk.</p><br><p>So, what do you think? Is The AI Alignment Problem a useful blog, and if so, where should it go? Or not? If so, would you like me to host it somewhere else where it can get off the ground, and which has a different name? I think I might be able to set it up myself, but if there are many people who would enjoy having their own blogs, I'm happy to host it for them. I'm not sure how to set up a blog, but Google does provide a tool for free, so I'll figure it out.</p><br><p>And, if you think The AI Alignment problem blog is a good idea, even if it's probably going to fail, or if you're not sure whether it's a good idea, let me know and we'll talk.</p>      </span>    </div>  </div></body></html>