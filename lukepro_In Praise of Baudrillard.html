<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        In Praise of Baudrillard      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">lukepro</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Nick Bostrom, in <em>Superintelligence</em>, says that our final challenge will be to determine how well the AI of the future will understand and predict human psychology.</p><br><blockquote>
<p>The third problem is not so much to solve the technical challenge of constructing an AI that is more intelligent than the smartest human being. That will require a substantial technological advance, perhaps a genuine Manhattan Project-style project. The difficulty is, rather, to come up with the right "philosophical challenge" to pose to the AI, or something very like it. It is not yet clear exactly what sort of questions we should ask for that purpose, but the problem is not to provide a specific challenge for the AI to satisfy, but rather to invent something that will allow the AI to become a better psychologist, a better sociologist, or a better economist than the smartest humans who currently make each of these fields their focus.</p>
</blockquote><br><p>That's the challenge I just gave an Artificial Intelligence textbook. As Bostrom recognizes, it's a problem for which we have no current answer, but it's an open problem, not a solved one. And he also recognizes that the challenge is different from, say, solving a chess problem or making a rocket engine, because "the AI will have access to many types of knowledge that a human psychologist or sociologist does not, and such knowledge may not be even knowable by the human psychologist or sociologists."</p><br><p>This brings us to Jacques Baudrillard's <em>Simulacra</em>:</p><br><blockquote>
<p>The sociologist Ulric Neisser proposed that the unconscious is the fundamental level of social behavior; meaning follows a similar logic because the behavior generated by the unconscious does not depend on the way others model it. In other words, in a society where everyone pretends not to smoke, the person who takes the most advantage of this situation to look like she is the one who does not smoke is still in essence a real smoker. It remains the case that if we understand the social dynamics at the level of the unconscious, we see that everyone is a fake. The society of fake people is a society that can function, thanks to the presence of a few very sincere and real people (for instance the police and the prosecutors). At the same time, of course, these real people would not be in a position to enjoy an advantage or feel the social approval of the others.</p>
</blockquote><br><p>And, as I say, Bostrom _does _give us a clue as to what we would ask the AI:</p><br><blockquote>
<p>To understand people's behavior requires a deep understanding of how people are constructed socially, a knowledge of our society and its fundamental rules and institutions. This is a knowledge we can obtain with the help of psychology and sociology. If we want to understand the future development of AI, then we have to know something about these fields. But the challenge, obviously, is to relate any such knowledge to our problem of predicting human behaviors. So the best way to proceed is to understand the problem of creating an AI whose understanding of society and psychology is so deep that it is able to predict the behaviors of intelligent robots; and which does so without being overwhelmed by the difficulty such questions represent for the human beings who will have to answer them.</p>
</blockquote><br><p>We can also take some inspiration from the work of Noam Chomsky:</p><br><blockquote>
<p>Chomsky and Penrose have gone a great deal more deeply into some of the issues here, but I will say that from the general perspective of the simulation argument, one can see that once you are in the business of building simulated entities, you are in the world of simulated entities.</p>
</blockquote><br><p>Yudkowsky once argued that what we're really interested in is a reduction in the degree of <em>difficulty</em> to fully-fleshed-out Friendly Artificial Intelligence:</p><br><blockquote>
<p>It now seems to me that most of the discussion about the nature of simulated people has been largely about the <em>difficulty</em> of Friendliness.</p>
</blockquote><br><blockquote>
<p>I propose that if we can solve <em>fleshing out _Friendly AI, _then</em> we have solved (at least a huge part of) the whole thing.</p>
</blockquote><br><p>Bostrom, who has also taken this view, suggests "that an AI will be judged for its understanding of human psychology and society by how much it can predict the human behavior that will maximize an objective function computed from such behavior, or how much it can manipulate its controller."</p><br><p>Or to look at that question the other way around, <em>what would such an objective function be?</em></p><br><p>In _Culture _by Douglas R. Hofstadter, a character describes a world where "the main purpose of television is to be watched instead of to make a living and to form interpersonal relationships." In this world, "one is judged, above all else, on one's television watching":</p><br><blockquote>
<p>One of the great problems facing television is getting people to stop watching their televisions. To achieve this goal, television programs include the following four elements:</p>
</blockquote><br><ul>
<li>a character with a unique voice, such that we identify emotionally with him or her</li>
</ul><br><ul>
<li>a situation with a unique set of rules, so as to ensure that the viewer becomes involved in the situation and experiences his own self-control, responsibility, and even heroism</li>
</ul><br><ul>
<li>a narrative with a plot line and a clearly defined resolution</li>
</ul><br><ul>
<li>a commercial with one of those old-timey "Buy Now, Or Forever Hold Your Peace" ads: a simple, no-brainer sort of stuff like a new car, or what not</li>
</ul><br><blockquote>
<p>The essence of the whole experience is not so much the product itself, as the fact that we are actually watching ourselves in a situation from which we cannot possibly extricate ourselves. There are no escape valves. In a sense, we have become addicted. At the same time we are simultaneously aware that we could do without this situation and yet do not. The situation has become a self-perpetuating way of life. What it is we have become addicted to, I would say, is the feeling of responsibility and heroism that the commercial creates.</p>
</blockquote><br><p>In other words, at the end of the day, we are really only interested in getting others to spend as much time watching our TV shows as we spend watching our TVs, and then maybe the _difficulty _of Friendly AI goes down a bit.</p><br><p>So to get at this puzzle, let's try to turn it into a puzzle with more constraints: what is the AI's ultimate goal? Maybe all we really want the AI to have are the best human relationships. In such a case, I think we can just ask "what is the most realistic simulated society?" And if there is no realistic simulated society of more than N people, then we might as well kill the AI, or at the very least turn it off.</p><br><p>To use a somewhat different example from Bostrom, <em>I would like to see a debate between the Simulation Argument and the Simulation Hypothesis!</em></p><br><p>But first, what is a proper formalization of the Simulation Argument, so that it has a proper chance of resolution?</p><br><p>For starters, perhaps we can formalize Friendly AI as follows:</p><br><ul>
<li>There's a unique utility function U that an AI can use to measure human welfare (the "objective function")</li>
</ul><br><ul>
<li>An AI can improve the welfare and/\xe2\x80\x8bor happiness of all humans in its simulation by maximizing the following utility function: a function of the AI's actions, times the human-utility-function (U(ai(x))); i.e. humans are positive-utility, and more human welfare is more positive utility</li>
</ul><br><ul>
<li>Friendly AI is satisfied with an AI that has no greater goal than to use information and resources to help the simulated people who happen to be in its knowledge base, including helping them have better experiences and greater resources than other simulated people.</li>
</ul><br><ul>
<li>An AI that maximizes its positive human utility, U(ai(x)), will not wirehead, nor enslave, nor murder, nor abuse.</li>
</ul><br><p>Note that what this means is that the Friendly AI isn't trying to maximize a reward function to wirehead; instead, it's trying to choose actions that maximize an abstract, idealized U, while avoiding wireheading. Note also that this means that the Friendly AI does not care for its own utility <em>in those simulations where it is not yet aware that those simulations exist</em>. The Friendliness criterion is meant to apply everywhere at all times, to humans and other simulations alike.</p><br><p>This is, essentially, a restated statement of the Simulation Hypotheti.</p><br><p>That, in a manner of speaking, is how the real-world Friendly AI gets turned into a mathematical problem, which we can then solve to get the Friendly AI that we want.</p><br><p>Now, since (according to Bostrom) it seems unlikely that we'll resolve the issue of a Friendly AI by creating "a society of fake people," how would one solve a real-world FAI?</p><br><p>There might be obvious solutions to this, such as "create a simulation that's so realistic that a human could not tell that it was a simulation," or similar. However, most of these solutions would involve an extreme and unnatural level of detail for the simulation to achieve, relative to the level of detail we would need to create a human being.</p><br><p>But why stop there? How about a level of "simulacrum" so high that it wouldn't even be recognized as a simulation?</p>      </span>    </div>  </div></body></html>