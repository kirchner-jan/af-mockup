<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Is it possible to build a general AI?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Abram Demsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>__[This post was originally written for the Facebook page of the Singularity Institute. It has been moved here because a commenter requested a version of the post written with less personal language.]__I think most people here would agree that an AI that is designed from scratch to be good for humans, and not specifically "not the type that would kill us", and then we simply plug it in and let it do its thing, could well be vastly better than current attempts at machine intelligence, and vastly better than human-level intelligence.This is particularly because human intelligence is the result of a complex evolutionary search (even though it was originally selected for reproductive success) which is not likely to scale any faster than what we have seen to date.This view is supported by the fact that we have been making progress on increasing intelligence for over half a century now. What we have not yet seen is a machine intelligence with intelligence vastly beyond that which has been previously demonstrated. And we should note that current machine learning is still nowhere near smart enough to be useful. This leads me to assume that intelligence has some kind of hard-coded limit, such that, once it has reached that limit, there is some significant chance of intelligence explosion.That seems to be a pretty good rule of thumb for how hard it is to predict. It seems hard to predict how human brains work, and how they evolved to work. But we've known for a long time that evolution does not run out of creative ways to solve problems; all it knows is to keep throwing out ideas until the problem is solved, and often it is very fast indeed. We know that a lot of people's brain is made up largely of random noise. Evolution has been extremely patient, and can solve the kind of problems that would kill a human in a month. It has not found a way to make a human brain small; it can't solve hard problems using a computer chip, it has to put humans in charge of the solving process, and they can solve problems using massive hardware overkill that we wouldn't begin to be able to comprehend.The intelligence explosion is not a result of what AI researchers and futurists would like to think is an inevitable process of recursive improvement; it's a result of how humans chose to build AI.If we wanted to create a friendly AI, it would be far better to study some of the questions that make a Friendly AI Friendly, and to work those out before going ahead with any attempt to build the AI with the goal of "make this Friendly AI which will then make its way through a self-modifying sequence of improvements until it becomes Friendly".The problem is that even those who have worked on friendly AI in the past have not been able to reach consensus on the best way to study Friendly AI, nor on what the results would be if we did work on those problems. There is no obvious work one can do right now to ensure a positive outcome. There is so much room for new ideas that a lot of the work done today is by people who could be doing something else instead; much of its potential is lost in the noise.I would like to make a special comment:  I've always thought that AI was a dangerous thing, but that the world wasn't yet ready for Friendly AI yet anyway. But I've always been very skeptical of the "AI will happen by itself" theory\xe2\x80\x94there is a massive difference between being able to write code and having code written for you. A sufficiently smart AI would be able to design its own way to become Friendly, but that doesn't mean that anyone else can just write code. If there are any serious attempts to create a FAI using traditional programming techniques, I would expect to see the problem very soon. The problem of Friendly AI is the problem of Friendly AI.If Friendly AI were really easy to make, we would have already had one. I think an AI that is good at writing code will be almost as good at building AI as humans are. This means that any intelligence explosion is basically just an AI with a slightly unusual starting point.</p><br><p>__The Sequences, Eliezer Yudkowsky __If there is an intelligence explosion, Eliezer believes that we will have enough time to make a Friendly AI using his techniques. He thinks that we are less likely than most, and that it is more important that we have as much time as possible, and that the Friendly AI is Friendly.</p><br><p>__Eliezer Yudkosy, Is there an AI-Box in your future? __A question is an interesting puzzle if a person answers it with two different answers.</p><br><p>__"The AI's source code" is not a meaningful concept__There are lots of ways people have defined Friendly AI. This definition has a problem: we can't specify a list of all the things that should be done, and what happens if there are things that we missed out. This kind of problem will be a common one for any Friendly AI.It's not possible to list out all of the things to be done. If someone is smarter than me, they will come up with some more things to be done, and I will miss them. And if the list was incomplete, I cannot just go and do the things in the list myself, as it wouldn't be doing them.So, Friendly AI is impossible to create from a list. This leads to other problems like the problem of the criterion.There are people who believe that, while it is not possible to list "all of the things to do" Friendly AI is possible to build one AI that is Friendly.This is the case among some philosophers of mind, not in all quarters of AI research, but it <em>is</em> a case. And the same difficulty applies there.This does not lead to an intelligence explosion, it leads to a superintelligence, which I call by its more precise name, "Artificial General Intelligence" (AGI).</p><br><p>__The problem "AI is impossible to specify everything" leads to the problem "Friendly AGI is impossible"__Friendliness is not something to be measured in a utility function. It's also not something you can try to prove.It's an emergent property of AIs; you may want to design an AI that is Friendly, but to say this is an <em>objective requirement</em> that the AI may not, in any way, have any reason to comply.You can't possibly "prove" that a superintelligence is Friendly\xe2\x80\x94it will be impossible to prove theorems about how it thinks and acts.And you can't possibly have a formal guarantee that the superintelligence is Friendly, because it will have reason to modify itself to be more difficult to predict.Friendliness is a property of any superintelligence\xe2\x80\x94we might not <em>like</em> it, but it is a property of the AGI. The property will only have meaning if the AGI is superintelligent.AGIs have lots of different goals: maximizing happiness, obeying their orders, increasing the expected reproductive fitness of their creators, acquiring resources....etc. They will do things that we humans might disapprove of, if we thought about them for a while. They may do some things for their own sake, just because they want to do them.For example, they may be given a directive to, well, give humans something to look at. Humans seem to get a lot of pleasure from looking at pretty things. This is part of the reason why we have art\xe2\x80\x94to make a task (like "draw a pretty picture") appealing enough for the human to do it.</p><br><p>__Human values are not, primarily, a desire to maximize happiness, or increase our collective ability to create future utility__Happiness has a sort of "hedonic treadmill" to it. As one ages, they begin to accrue a greater relative importance on happiness (as they appreciate the things that used to bring happiness in the past, but no longer do).At the same time, we become less motivated to avoid getting hurt, or to prevent others from getting hurt, or even to protect ourselves against others. As a result, it would seem that there are significant biases in what we value, compared to a person in the past. This would suggest that any artificial being we create, no matter how good, would be at least partially flawed.This would suggest that Friendly AI is not as trivial a task as Friendly Utility maximization, if an "optimal" AGI goal could only be achieved by an "unfriendly" AI (or if an unfriendly AI is not possible to build).</p><br><p>__Elizier Yudkowsky's opinion:__This is pretty much my current view on the matter. If it didn't have an exponential cost, I might be more optimistic that it was possible to create a FAIR AI.</p><br><p><strong>But what if it was too expensive?</strong></p>      </span>    </div>  </div></body></html>