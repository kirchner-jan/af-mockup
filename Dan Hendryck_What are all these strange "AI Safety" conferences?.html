<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        What are all these strange "AI Safety" conferences?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Dan Hendryck</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>Clarifying "AI Safety": Why is it important to have better clarity on AI safety?</li>
</ul><br><ul>
<li>Existing terminology: The space and terminology</li>
</ul><br><ul>
<li>Conferences</li>
</ul><br><ul>
<li>What are all these conferences?</li>
</ul><br><p>[I wrote the above in October 2021 and edited it up to publish today.]</p><br><p>It's no secret that I am frustrated with the current conversation around AI safety. It appears to me that many in the field are very excited about the prospect of building a "safe" AI system. But I don't think that there is a common agreement on what it even means to build a successful AI system that is "safe." I don't think there is even a consensus on what would count as being "safe" by the standards of how a given group thinks about AI safety. At the very least, it seems to me that a lot of people conflate the above concepts.</p><br><p>In this post, I will attempt to clarify some of these concepts and help to identify common confusions, hopefully in the process making progress ourselves in clarifying the field.</p><br><p>What is this "AI" term all about?</p><br><p>Here is an example sentence from the beginning of the AI safety Wikipedia page: <em>AI, AI research, and artificial intelligence are three ways of thinking about the study of computer systems which exhibit human-like behaviors in reasoning, learning, and natural language understanding.</em></p><br><p>It's pretty clear that the "human-like" bit is something that we would not want out of an AI system. However, the sentence immediately above is about the term <em>AI itself</em>, rather than about the AI term.</p><br><p>The sentence later in the same article clarifies that AI safety refers to the study of creating algorithms or systems that do what is "humanly safe."</p><br><p>This appears to me to be the standard working definition of _AI safety _by default. Unfortunately, I don't think a lot of people are using this very clear standard working definition.</p><br><p>Clarifying "AI Safety"</p><br><p>Instead, I think there are a bunch of different common confusions that result in conflating a few different terms. I'll attempt to clarify what I think is going on in each case and try to disambiguate them:</p><br><p>Existing terminology: The Space and Terminology</p><br><p>First, we want to clarify terminology. Unfortunately, the term _AI safety _is very unclear. We want the term to mean the following:</p><br><ul>
<li>An AI system is "safe" if it does what we want, without being too dangerous, and without some catastrophe causing irreversible harm, or us having some other negative experience. [1]</li>
</ul><br><ul>
<li>An AI service is safe if its behavior meets some quality threshold such that it does not pose an unacceptable risk of failure.</li>
</ul><br><p>Note that this is not at all what is meant by the term <em>AI Safety</em>, but I think is often how people use the term.</p><br><p>In a 2019 post, I suggested that there would be several sub-fields of AI safety (reinforcement learning, robustness of RL systems, verification, etc.) I think that a few other common confusions might result via the conflation of these terms, so I want to make clear the following distinctions between these sub-fields (I borrowed these from the following discussions with David Krueger, and the terminology is taken from those):</p><br><p>I'm not aware of any other common term that is similarly useful (to disambiguate "safety" from "success" in AI systems). So, I suggest using it as a new term and adding it to all the fields of the research, particularly AI safety.</p><br><p>Conferences</p><br><p>There is a proliferation of conferences and research groups (like this one) dedicated to "AI Safety." Unfortunately, the use of this term is often fuzzy and the groups make little effort to clarify what they mean. In the next section, I will try to clarify what research fits the above description of "AI Safety."</p><br><p>I want to mention here that I am aware that this does not capture the entirety of this research space, though it is still valuable to try to understand what the broader space is.</p><br><p>What are all these conferences and groups?</p><br><p>As a brief overview, and to make things even more confusing\xe2\x80\x94the AI Safety Camp is now over five years old\xe2\x80\x94there are the following groups/\xe2\x80\x8bconferences that exist in that time:</p><br><ul>
<li><strong>2018 AI Safety Camp</strong></li>
</ul><br><ul>
<li>https://\xe2\x80\x8b\xe2\x80\x8baisafetycamp.com/\xe2\x80\x8b\xe2\x80\x8b</li>
</ul><br><ul>
<li>David Krueger</li>
</ul><br><ul>
<li>Ben Garfinkel</li>
</ul><br><ul>
<li>Daniel Filan</li>
</ul><br><ul>
<li>Rohin Shah</li>
</ul><br><ul>
<li>Andreas Stuhlm\xc3\xbcller</li>
</ul><br><ul>
<li>Note that the AI Safety Camp is not really an AI safety research group, though it does have researchers with direct interests in AI safety</li>
</ul><br><ul>
<li><strong>2019 AI Safety Camp</strong> </li>
</ul><br><ul>
<li>https://\xe2\x80\x8baisafetycamp.github.io/\xe2\x80\x8b\xe2\x80\x8b</li>
</ul><br><ul>
<li>AI safety camp</li>
</ul><br><ul>
<li><strong>2020 AI Safety Camp</strong> (Now over three years old)</li>
</ul><br><ul>
<li>https://aisafetycamp.substack.com/\xe2\x80\x8b\xe2\x80\x8bp/\xe2\x80\x8b\xe2\x80\x8baisafety-2019-camp-report</li>
</ul><br><ul>
<li>https://\xe2\x80\x8b\xe2\x80\x8bcamp.google.com/\xe2\x80\x8b\xe2\x80\x8bcamp/\xe2\x80\x8b\xe2\x80\x8b#</li>
</ul><br><ul>
<li>Note: this is no longer a one-off event, but rather a sequence of events that will go on for years, so "camp" is the wrong word. I'm just trying to point to the Google form where you fill out applications.</li>
</ul><br><ul>
<li><strong>AI safety camp:</strong> </li>
</ul><br><ul>
<li>We're applying for a grant from the MIRI foundation so that we can continue to run the event each summer.</li>
</ul><br><ul>
<li><strong>AI Safety Camp &amp; Thesis</strong></li>
</ul><br><ul>
<li>I haven't attended the AI Safety Camp (though I've known the researchers at the camp) but they have some very large AI Safety Thesis Projects that include a technical presentation, a paper, and sometimes a workshop.</li>
</ul><br><ul>
<li>For example, I hear that there was a workshop and presentation for the Stanford workshop.</li>
</ul><br><ul>
<li><strong><em>International Workshop</em></strong></li>
</ul><br><ul>
<li>In the past, the workshop has been a collaboration of many people who each attend together each year. However, the presenters are not all from the US.</li>
</ul><br><ul>
<li>I'm aware of at least two other groups that are run by researchers from MIRI: CSER and AI Impacts. I do not know for certain that these groups meet the description of the camp, but it does appear so. (I think the group of researchers from MIRIS would generally count as part of MIRI, if they are not explicitly a part of MIRIS.)</li>
</ul><br><ul>
<li>However, I do think they are a distinct source of research. AI Impacts is a group in the same space as AI Safety Camp, though it appears to be focused more on broader risk and strategy.</li>
</ul><br><ul>
<li>I suspect that AI Impacts could be fairly accurately described as a research group within the existing AI Safety research community (and perhaps not at all aligned with MIRI).</li>
</ul><br><ul>
<li>In the past CSER was not affiliated with MIRI, but at least one of their workshops was. They have some connections with EA but that does not seem core to the group or their research.</li>
</ul><br><ul>
<li>It might seem surprising that the core AI Safety research group is affiliated with a place like MIRI. While I think that MIRI's research fits within the broader category of AI Safety, historically they have focused more narrowly in this space (with a few exceptions like the classic Superintelligence).</li>
</ul><br><ul>
<li>So it is good to remember that CSER is a distinct organization within the field of AI Safety.</li>
</ul><br><ul>
<li>CSER is also one of the two groups that hosts the AI-Risk-Forum (the other would be Rethink Priorities)</li>
</ul><br><ul>
<li>The AI-Risk Forum was previously host by CHAI. They have moved for the time being, but still have all of their researchers on for the forum.</li>
</ul><br><ul>
<li>There also seem to be additional organizations like MIRI that host their own sub-fora, but I have not been able to understand the specifics there. And I will only comment on it briefly.</li>
</ul><br><ul>
<li>Overall the Forum community seems somewhat focused on AI progress and strategy.</li>
</ul><br><ul>
<li>I wouldn't try to use any community statistics or survey questions from any of these. At least not yet. But I would encourage you to go read some of the recent stuff on the forums to understand it better...</li>
</ul><br><ul>
<li>For example:</li>
</ul><br><ul>
<li>A few days ago, Scott Garrabrant on the AI fora was posting about how the current state of AI is very bad and how it is possible that AGI is likely relatively soon</li>
</ul><br><ul>
<li>The Forum has a sub-community dedicated to AI governance</li>
</ul><br><ul>
<li>There has been some AI ethics research over there as well, such as the work on the "Value Dilemma" paper on the Forum.</li>
</ul><br><ul>
<li>A few weeks back, there were some posts about the "Existential Risks from AI" post here on the AI Forum.</li>
</ul><br><p>As I understand it, AI Impacts is not affiliated with MIRC or CSER but instead they are affiliated with MIRIS. That said, they seem distinct enough from the rest of the space that I will still try to treat this as a separate group.</p><br><p>Finally, I will try and provide a bit of a taxonomy of AI safety research here, at least a bit more of a classification of the AI Safety area. This is a bit a "taxonomy" in its own right and I may make mistakes (or just misunderstand). Please let me know if you think I have made mistakes!</p><br><p><strong>Field-Building:</strong></p><br><ul>
<li>What is AI Safety?</li>
</ul>      </span>    </div>  </div></body></html>