<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        A review of some important papers showing replication bias      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">alexflin</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Link post</p><br><p>There has been a discussion of replication bias, the bias whereby the majority of research fail to replicate, here on LW before. The problem is that some research has not been tried. In that case, no matter how great our reasons may be for believing these studies fail to replicate, we still don't know for sure, because we don't know if these studies haven't already been done.</p><br><p>However, when I have come across studies that have already been done but have not yet been published in a journal, it is usually because these studies are too small to find the effect they are looking for, or there is some methodological problem. In these cases, I assume that these studies failed to find the effect because they just aren't interesting enough, and the journal wouldn't be interested in publishing them.</p><br><p>Here are some papers that were published on LW that showed results that failed to replicate but did have a decent sample size.</p><br><ul>
<li>"An online campaign to predict the next U.S. Presidential election: Which side could it go one way or the other?" by John_Maxwell_IV</li>
</ul><br><ul>
<li>One of the better-known studies on replication bias found that studies with a sample of 100 or more respondents are more likely to replicate than studies with a smaller sample of 30</li>
</ul><br><ul>
<li>"A Bayesian Approach to the Dilemma of Research Debt" by Steven Byrnes</li>
</ul><br><ul>
<li>An online study by John_Max_IV, which used the same methodology as the last study, found that of studies with a sample size of 100 or more, those which are published in a journal are 80.6% likely to produce positive results! Negative results are found only 8.3% of the time.</li>
</ul><br><ul>
<li>"Toward a Bayesian Evaluation of Research Effectiveness"</li>
</ul><br><ul>
<li>"Is the'research-to-policy pipeline broken and how to fix it' an efficient use of research money?"</li>
</ul><br><ul>
<li>"Are all good studies equal?"</li>
</ul><br><ul>
<li>"Could a Bayesian fix the NIH's problems with clinical trials?"</li>
</ul><br><ul>
<li>You can see on this page about other papers that show a good sample on small effect-size studies.</li>
</ul><br><ul>
<li>"You wouldn't expect a lot of replications of research finding that your research was wrong, right?" And also...</li>
</ul><br><ul>
<li>The "Replication Crisis in Clinical Psychology"</li>
</ul><br><ul>
<li>https://\xe2\x80\x8b\xe2\x80\x8bwww.plosone.org/\xe2\x80\x8b\xe2\x80\x8barticle/\xe2\x80\x8binfo%3Adoi%2F10.1371%2Fjournal.pone.0041620\xe2\x80\xb2\xe2\x80\xb2 (which is the source I used for the sample size in all of these papers)</li>
</ul><br><ul>
<li>This was the study shown in the video linked at the top on how small effect studies can sometimes show replications. It used a sample size of 603, but it still failed to replicate.</li>
</ul><br><ul>
<li>The more recent and interesting Replication Markets: http://\xe2\x80\x8b\xe2\x80\x8bwww.replicationmarkets.com</li>
</ul><br><ul>
<li>"A'replication crisis' could end most academic fields"</li>
</ul><br><p>A Bayesian approach to the Dilemmatao of Research Debt</p><br><p>There are probably some good reasons to believe the claims of the scientific community and the media that we live in a world with lots of "fake" science. Perhaps too many "fake" claims are made on every issue, but some very questionable statements are also made. I feel personally that I am not fully convinced that all these claims are a direct result of the replication crisis, and I have heard many others say this as well. To help my own case and those of the Less Wrong community, I want to present some of my own claims about the science of politics. If these claims (with sample size 1-5, but you can use your own numbers to get as many as you want for a claim) are not completely wrong at any rate, then any scientist or rationalist is not in favor of the current political or scientific consensus on some issues. So we are only talking about the conclusions drawn after studying these issues for many years based on the research that has already been done. The fact that I am not convinced of any of these "fake" claims provides more reason to believe that the replication crisis is not the whole story. (And maybe I have failed to understand some of these more complicated issues of science.)</p><br><p>This is another way of putting my claims:</p><br><ol>
<li>I am in favor of the science on some issues. This is the idea that most scientists (at least those who are doing scientific research on more politically charged topics) are in favor of the scientific consensus on these issues. This idea does not imply that I believe all scientists are wrong (or right). It only implies that I believe that I am among the rare few scientists on more controversial topics that are against the scientific consensus, but that on those more controversial topics, the consensus is probably mostly true anyway (or I am probably the first person to break with the scientific consensus in at least some cases where others agree with the consensus).</li>
</ol><br><ol>
<li>When I see any kind of science results published, especially in "mainstream outlets," I am skeptical because I assume that most of the people who have published the results are probably in favor of the mainstream scientific consensus. Even if the authors try to assure me that they believe the paper's results are independent and not directed by the scientific community, I assume that the authors still believe in the scientific community as a whole. So I assume that the results and conclusions of the paper are the result of the scientific community's consensus, not the paper's authors. Again, the fact that I am skeptical of any given study does not imply that the scientific community is wrong. It means that I am not part of the majority who share the consensus on that issue.</li>
</ol><br><ol>
<li>In particular, if a scientific conclusion is drawn by the majority (as in the case where 97.5% of experts agree with the consensus) and also a study does not find that result, then I think that in general this is not evidence that the scientists did not make a good judgment. Instead, I think that it is more likely that the study had some kind of a problem. For example, maybe the scientists did have some problems with methodology and didn't see it because they were not doing the right study. In other words, maybe scientists are too prone to doing their own wrong experiments, and they are too selective. Also, it is possible that these "problems" with experimental methodology are rare and that the majority were good, but in this case it does not change the fact that 97.5% agreement is still strong evidence for the scientific consensus. If one believes the scientific community is too prone to the replication crises, then one should consider this a fact about the replication crisis, but not a fact about the scientific community.</li>
</ol><br><ol>
<li>As another example, if it is found that 97.4% of medical scientists agree with the recommendations of the American Academy of Clinical Biochemists, then this is strong evidence (though not overwhelming or unconvincing evidence) for the recommendations of the Academy of Clinical Biologists.</li>
</ol><br><ol>
<li>And one can consider the same thing for any scientific recommendations. Say that most scientists agree with the claims of the Royal Society or the American Academy of Physicists. Say that if we look only at the opinions of the American Academy, this is strong evidence for the opinions of the Royal Society. Now, to what extent would we consider these scientists to be right or wrong, if we know that these scientists have other views with which they do not agree with the opinions of the "major" scientific societies? How strongly would we look at the opinions of these other societies if we knew that all other scientific societies agree with the Royal Society or American Academy of Physicians?</li>
</ol><br><p>Based on claim 1-4, a hypothetical person believing science is mostly wrong might be more likely to:</p><br><ol>
<li>Believe there are some science claims for which the American Academy of Medical Scientists is right and the Royal Society or European Medicines Agency is wrong (and vice versa for the Royal Society, etc.). Also, they would have to be very willing to believe that the "real" scientists that do research do not care about politics (for example, they would have claimed a few years ago that there is no link between tobacco use and lung cancer) even though scientists themselves are claiming otherwise. (For example, see https://\xe2\x80\x8b\xe2\x80\x8btwitter.com/\xe2\x80\x8b\xe2\x80\x8bmatt_b_h/\xe2\x80\x8b\xe2\x80\x8bstatus/\xe2\x80\x8b\xe2\x80\x8b1308029069674549473.)</li>
</ol><br><p>All this discussion of science has led me to two ideas:</p><br><ul>
<li></li>
</ul><br><p>I am not in favor of scientific consensus on every issue. I think this makes it easier for me to believe that science is not wrong in general.</p><br><ul>
<li>\t</li>
</ul><br><p>However, the fact that science has not been able to reach a consensus on most controversial issues raises the question of how confident people can be that science agrees on some issues. To the extent that we (or the Less Wrong community) are trying to figure out how to decide what science "usually" agrees on, we probably can't be too confident because science hasn't always gotten it right. I think we should be OK assuming that (a) science can't be totally wrong on any one issue, and (b) science hasn't proven its own conclusion 100% wrong. But, if one's confidence is like 95%, I think the right thing is to use a 5% chance of scientific consensus against the scientific community and also a 95% chance of scientific community against scientific consensus.</p>      </span>    </div>  </div></body></html>