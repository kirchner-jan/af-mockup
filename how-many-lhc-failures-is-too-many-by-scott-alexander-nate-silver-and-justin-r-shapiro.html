<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        How Many LHC Failures Is Too Many?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Scott Alexander, Nate Silver and Justin R. Shapir</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <blockquote>
<ul>
<li>The probability that humanity does not go extinct before 2100 = p</li>
<li>For example, if you think that Homo sapiens is the only species that can pass on their genes to the next generation and also that it has a 50% chance of survival before 2100, then p = (.5)((3,000,000,000)/​(500,000,000 + (3,000, 000,000))).</li>
</ul>
</blockquote><br><p>...</p><br><ul>
<li>The probability that humanity goes extinct this century as a direct result of an act of war by another nation-state = q</li>
<li>For example if you think that there is a 1% chance that an act of war kills more than 100 million people, you're probably thinking about the risk of an accidental nuclear war that kills over 300 million people instead</li>
</ul><br><ul>
<li>The expected loss from humanity going extinct this century = (p-q).</li>
<li>That is, your wealth is what you think of your chances of surviving (your expectation of living a long life), multiplied by the amount you think your life is worth (your expectation of the value of a long life).</li>
</ul><br><p>The expected value of your civilization is the sum of this value for everyone alive in 2100, divided by the number of people alive in 2100.</p><br><p>Given those numbers, we could get an expected value of p, q or (p-q)/(p+q-2pq + p-q) - roughly, the expected survival of humanity divided by your estimate of the expected value of a human life in the absence of humans to pass them on to the next generation.</p><br><p>...
The current value of p is 3.5% and the current value of q is 21% (as of 2021).</p><br><p>The most popular probability estimates for human extinction this century come from the Bulletin of Atomic Scientists. (They say something like 4-7% for p, but all the numbers given do not add up to 100%; also, if they didn't, we wouldn't even be having this discussion.)</p><br><p>The sum of p and q is roughly 3.8%. We could move our risk curve to reduce p and/​or increase q to get that number down to 1% or below!</p><br><p>...
But really the numbers involved here are not that different than people's probability guesses about more normal-life events.</p><br><p>...</p><br><p>Let's say you were writing the above argument in 2003. (I wonder if somebody did already.) In 2013, the number of people who think the probability of extinction is 1% or below was 2%. In 2040, they'd get 4%. In 2050, it gets to 8% (thanks to Robin Hanson for starting me down this path).  So the chance the world goes extinct is actually probably already less than 10% today, after factoring in future growth.</p><br><p>In the second question, you want to say the probability of the LHC destroying the world is somewhere between 1% and 10%. If you say 2%, and 10 years later the LHC goes supernova and kills a million people or maybe ten countries but doesn't destroy humanity, you got it wrong and are probably a conspiracy theorist. But if you say 8%, then any random experiment with odds of 1 to 1 would do it in the long run, and you have to give up all hope for a bright human future. At that point – and it is a fairly point where you can have no hope for a bright future, no matter how many bright people you put in an experimental particle collider – I think you are at least slightly irrational.</p><br><p>Let's do the same calculation again with 1% as our lower bound. So we get p = 9% + (1-9)p + (1-1)p  = (18 +.9<em>1 + 1)%) /​ (18+.9</em>2 + 1) = 6.6%.  And we can do that calculation in 2025 and 2055 and find that moving from 1% to 5% reduces the expected loss from 3.8% to 1.4% and from 8% to 2.1%. From there it gets gradually better. But at 8% the expected loss is still worse than the expected value of pq = 3.8%.</p><br><p>So my answer to the first question is that, if p is low to moderate, but we have a moderate idea that q is low, then we probably shouldn't have big, dangerous things that can kill us all for the price of one or two of our current lives.</p><br><p>But my answer to the second question would be yes, the LHC has a better-than-zero chance of destroying the world with a probability between 1% and 0.01%, and people who are going to build a million, one-ton particle accelerators do not have enough respect for their fellow human beings to not consider it worthwhile to go take a ride in the LHC.</p><br><p>Now, all this assumes that the probability doesn't double every twenty years. But really, with enough investment and effort the chance should probably go up in our lifetimes, so I'm OK with a few years' time discount on these numbers – I wouldn't say that I put a high probability of extinction in a big particle collider, but then again, I wouldn't say I put a large probability in most other things either.</p><br><p>And of course if the probability of extinction was extremely high, say above 90%, then maybe you have to go to Mars, no matter how high the cost in money and human lives.</p>      </span>    </div>  </div></body></html>