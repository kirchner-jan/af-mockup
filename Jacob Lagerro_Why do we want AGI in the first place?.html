<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Why do we want AGI in the first place?      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Jacob Lagerro</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Contents</p><br><ul>
<li>My goal</li>
</ul><br><ul>
<li>Good vs. Bad AGI</li>
</ul><br><ul>
<li>Safety vs. "Moral Responsibility"</li>
</ul><br><ul>
<li>The Argument</li>
</ul><br><ul>
<li>
<ol>
<li>It will change society</li>
</ol>
</li>
</ul><br><ul>
<li>
<ol>
<li>It will give us more leverage over the natural world</li>
</ol>
</li>
</ul><br><ul>
<li>
<ol>
<li>It will give humans more control over the future of humanity</li>
</ol>
</li>
</ul><br><ul>
<li>
<ol>
<li>It will enable us to achieve our values more effectively</li>
</ol>
</li>
</ul><br><ul>
<li>
<ol>
<li>It will give AI systems more moral weight</li>
</ol>
</li>
</ul><br><ul>
<li>
<ol>
<li>It will prevent AI systems from being turned off</li>
</ol>
</li>
</ul><br><ul>
<li>
<ol>
<li>It will enable cooperation and coordination between agents</li>
</ol>
</li>
</ul><br><ul>
<li>
<ol>
<li>It will enable coordination and cooperation between humans</li>
</ol>
</li>
</ul><br><ul>
<li>Conclusion: It should be built anyway, with care and with a broad base of community support</li>
</ul><br><p><em>Thanks to Abram Demski for pointing out the importance of values in AGI, and for suggesting that I write a blog post about it</em></p><br><p>There is disagreement over the goal for AI systems which we want them to have. Should this goal be to achieve the most positive values, should it be to achieve human values, or should it just be to do something useful?</p><br><p>I believe "Human values" and "Most positive values" should both be taken into account when deciding what to focus on in the first place, but that we do need AGI because we do not know how to achieve certain values if we are not to create and/\xe2\x80\x8bor use AI systems. More importantly, if we fail to do so, we will still know that it would be bad, so we will most likely stop whatever we are attempting to do. Without a goal, we do not have any clear sense of a trajectory. That doesn't mean the goal needs to be a perfect or a precise definition, only that we have some kind of goal. </p><br><p>My goal</p><br><p>It will change societyIf we do not build a superintelligent AI, then we will be limited in the scope of things we can accomplish through technology. Without an AI, we will be stuck in a Malthusian trap since the only way out is to improve our quality of life by creating new people. </p><br><p>If we had powerful AI, we could have built a powerful society, but we did not do this, so we should probably assume that it will not be easy to achieve, and that we might need powerful AI even if we build it in such a way as to avoid having powerful, misaligned, self-replicating AGI.</p><br><p>It will give us more power over the natural worldThere are some things in the natural world which we are unlikely to understand the dynamics of until we have powerful AI. For example, climate change, biological warfare, supervolcanic eruptions, asteroids, etc. The more detailed we can understand them, the better we can optimize against them. If we want to be able to do so while making sure we will be able to take proper measures to optimize against them, we need powerful AI.</p><br><p>It will help us achieve our values more efficientlyWe have not achieved our values in full measure yet, and may need to use powerful AI to do this. In doing so, we can use AGI to help us achieve our other values. This could be achieved more effectively if our values were aligned. And if we could align our values and create a singleton who has aligned values, without first building powerful AI, the singleton would have a huge advantage over everything else.</p><br><p>It will enable us to more effectively achieve our values in the futureIt will give AI systems moral weightIf we create powerful AI, it will change society irreversibly, which will allow the AI to have moral weight, without it needing to be explicitly programmed to have moral weight.</p><br><p>It will make AI systems cooperate and coordinateIt will prevent AI systems turning offIf we create powerful AGI, then we need to have some idea of how to shape it. For example, we can give it a utility function and design a mechanism by which it maximizes this function, or we can make it indifferent to its utility function. Either way, if we do not succeed in making it want to do the right thing, we may need to either be constantly nagging it about what we want it to do, or we may need to shut it off. But we cannot shut it off if it would be bad for humanity to do so. (Unless it would be too expensive to re-start it when we have figured out how to re-shape it.)</p><br><p>It will give human-level AI moral weightIf we design the first AI systems by creating AGI and doing the relevant philosophical analysis, but do not build powerful AGI, these human-level AI systems will gain moral weight for two reasons. Firstly, since they are similar to humans in many ways, they will likely be motivated to preserve human value and prevent being shut off, due to human nature. Secondly, having been created by humans, they will be more likely to share human goals. This means that if we are able to successfully create human-level AI, and they have human-level values, their moral weight increases by an even larger margin after we build powerful AGI.</p><br><p>Conclusion: It should be created anyway, with care, and with a broad based of community support</p><br><p>To sum up, we have a goal (achievable values), there are obstacles (difficult to build or use for the natural world, risky and expensive, and only possible in a superintelligent context), and some of these obstacles can be solved easily if we just build powerful yet superintelligent AI without worrying too much about it being misaligned by default.</p><br><p>Furthermore, if there are other reasons to expect we may create superintelligent AGI if we do not start soon, other than the ones listed above, we may as well create superintelligent AI while being mindful of these issues. It will give AGI much more moral weight and could have much more powerful effects since it is not likely to kill us and could even save us if it is designed with the right values. </p><br><p>It seems like both moral concerns and a lack of understanding about how to achieve our values in a way we prefer without creating powerful superintelligent AGIs are good reasons why we might want to develop powerful AI at all, and we should focus more on other potential reasons.</p><br><p>Good vs. Bad AGIf we are going to develop and possibly use powerful AI, it is going to either give us more power or give us more power without us wanting to or needing to do so (through being a "tool" or through our values being aligned by default). We want to be able not just to make powerful AI for technological reasons, but also because we want the AI systems we use to be aligned by default, and to not be killed by us, by others, or by accidents. </p><br><p>One might call both goals "good" since they both seem good on face value. However, the goal we actually want is not good in the same way that being a tool that does things we want does not want to be turned off. One could argue that in addition to actually accomplishing our values, we want things that we think we value too, such as not having AI systems being turned off. This would make a tool or an unaligned AI system "bad"\xe2\x80\x94not good in the same exact way as an aligned tool or an aligned AI system would be good. </p><br><p>Safety vs. "Mundane Responsibility"We must choose whether we want to create AGI and use it safely or whether we want to have AGI that enables us to get somewhere faster and accomplish more. The way we currently do research and development in the AI field often leads to unsafe AGI being created and is generally considered unethical. This is not acceptable if we want to do anything that alters society, for example, if we want to build AGI that could be used to prevent climate change. It is also not acceptable for AI researchers to put their ethics aside because this might be necessary to be able to create safe AGI that could contribute to achieving the goals we care about. Therefore, it is safer to go with a "research and development" approach rather than an "[unsafe] approach".</p><br><p>The ApproachThe way we create AGI today is by building on decades of research. AGI will only be constructed by creating an AI system that has been trained and tested in a particular way since it uses a lot of computation and a lot of data, and this training and testing is carried out by a machine learning model. It seems that if we want to create aligned AGI, we must train and test our models in a way that includes a thorough treatment of the problem of alignment and a great deal of data and computation, in a way that is very costly and difficult by today's standards. However, if we want powerful and capable human-level AI today, without worrying about AI safety, a very small amount of research, data, and computation is all we need. That means we won't develop aligned AGI today, but this seems acceptable if the risks of misalignment are small relative to the benefits we can achieve if our AGI systems are good enough to keep us in power and on track to achieve our values. </p>      </span>    </div>  </div></body></html>