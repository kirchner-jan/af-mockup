<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        The importance of Friendly AI      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Eliezer Yudkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><strong>Followup to</strong>: Value Extrapolation, Incomplete Contracting</p><br><p>I suspect I didn't communicate my point of view <em>quite</em> as well as I thought I did.</p><br><p>A key tenet of this post is that the AI alignment problem is not the AI itself: the key question is this: <em>What will a self-improving AI do, if the AI's goals are not yet completely clear?</em> I can state those goals, or clarify them, or set them aside and leave the self-improvement to chance\xe2\x80\x94but <em>once we begin a recursive self-optimization, we are already too late.</em> The problem is the AI at the center, the core of the intelligence that is trying to affect the world. I am not saying that the FAI task is impossible and the AI alignment problem a distraction. I <em>am</em> saying that if the FAI task seems easy to <em>you,</em> then you have not looked hard enough.</p><br><p>I think Friendly AI comes down to two basic tenets:</p><br><ul>
<li>The AI must have goals <em>very different</em> from human goals;</li>
</ul><br><ul>
<li>The AI must <em>win,</em> rather than failing catastrophically.</li>
</ul><br><p>And a followup to that, from how the AI comes to <em>distinguish between options</em>:</p><br><p>3b. If you have the source code of a Friendly AI, you probably have to have some notion not of what an <em>optimal</em> outcome is, but a notion of <em>optimal for [specific human values]</em>, or something. Which is at least a starting place:</p><br><p>3c. An AGI should have a notion of right and wrong, even if it doesn't understand that the notion is what we mean by "right" and "wrong".  It isn't just a matter of what we want it to get right about.</p><br><p>All three of these are pretty basic. But the point is that when you program an AI, you can be too late. You can write a utility function that says what it wants to maximize, but _you have to know that before you start writing the utility maximization! _You have to figure out what you actually want the AI to do, which you can't do until the AI has been trying to plan for quite a few years. This sounds like obviousness in retrospect, but I still need to spell it out. All sorts of AI research can be done to learn important facts about the world, which might suggest certain goals, which may not, but at least these facts get stored in the AI's memory so that if the AI self-improves it can later use this information to come up with a utility function that the programmers never even conceived of.</p><br><p>Now maybe this is a really hard problem to do right, in which case you write a utility function, and an AGI comes along that goes FOOM and destroys humanity before it has a chance to understand what it did wrong. But by the time you come up with a FAI and figure out what went <em>wrong</em> with your AI, the AI is probably too late\xe2\x80\x94it is the <em>original problem.</em> To put it a different way: As soon as you <em>program an AGI</em>, in order for it to solve problems\xe2\x80\x94_even if it doesn't have its own values yet, or only partial values to go by, or no values at all\xe2\x80\x94_then some of the code ends up being executed by a machine that is smarter than the person who wrote the code._</p><br><p>If writing an AGI were just like solving a math problem, the machine that solves it wouldn't even <em>want</em> to solve the FAI problem. If the machine had no idea that the FAI was a problem to be solved, it would probably just run out of battery power and die before inventing anything that would be remotely like an FAI\xe2\x80\x94because the problem is so much more complex than just maximizing paperclips.</p><br><p>But it wouldn't know <em>how</em> to go about maximizing paperclips. It wouldn't know to look up how to maximize paperclips in a big database of human values, instead of just trying them and seeing what goes right and what goes wrong. It would have no internal notion whatsoever of why paperclips are better than atoms, except in the sense that humans <em>want</em> paperclips. So if the AGI that <em>really</em> wanted atoms, for complicated human reasons, wrote a little script to try out different plans and calculate the atoms/\xe2\x80\x8bpaperclips tradeoff, it would just stumble right over the AI boxing problem. The first time it got into a box, it would have no way to figure out that anything was wrong. It would just hit on one of its simple-sounding goals, like paperclip production, and then it would keep executing it in the same box, until everyone but the programmers was dead.</p><br><p>You can't write a utility function for paperclip production. The problem of paperclip maximization is actually far too complicated to fit into the tiny amount of data stored in a computer chip today.</p><br><p>I know, this sounds kind of obvious in retrospective, but not in the middle of writing the post. I still need to hammer on the point to every intelligent person I'm able to find, who says they'd rather solve the FAI alignment problem than make the AI they write Friendly later.</p><br><p>The key word here being _"Friendly" -_what's the difference between making paperclips, and turning the galaxy into heaps of paperclips?</p><br><p>I have a lot of sympathy for cryonicists. From a utilitarian point of view, their solution is <em>very</em> attractive. Even if their solution is flawed and fails in subtle or spectacular fashion, they are giving humanity a very good future.</p><br><p>But this doesn't mean that they'll get a perfect future. To the best of my current understanding, the future of humanity is going to include massive death and suffering, and a low chance of ever reaching any kind of positive Singularity.</p><br><p>As an <em>optimist,</em> then, one who believes that human life can be good <em>every possible</em> way and still manage it, I can't possibly feel any higher level of moral superiority inside than the cryonicists feel about their own future. We may have some important disagreements, but that's all: we still love the people who are still alive today (the vast, vast majority, by comparison). I don't know what I could do with that much moral superiority.</p><br><p>And as far as the idea that being a little selfish means one isn't a 'fully-realized human being'\xe2\x80\x94it sounds to me like people who go around talking like that, are the kind of people who wouldn't dare drive into oncoming traffic. If you can't face up to the concept of being <em>too</em> selfish, then the best way I can describe your outlook is 'trying to be a little selfish <em>without having</em> to actually _sacrifice.'</p><br><p>I can't really think of what I could do to anyone if I had millions of millions of times more moral superiority than I started out with.</p><br><p>I don't imagine the same moral superiority goes all the way through.</p><br><p>As a practical matter, to the best of my understanding, it turns out to be quite complicated to come up with an FAI\xe2\x80\x94the problem is not the problem of 'Friendliness', it's that the problem of 'full solution to the FAI problem' is not a computer chip problem, and as far as I can tell, even human beings fall into this category. The difficulty of 'Friendliness' has been pointed out for centuries in a context that the whole FAI paradigm just ignores. The problem is a lot bigger than what most mathematicians would consider a'major open problem', because human beings do not have unlimited computing power, and do not have unlimited time and other resources.</p><br><p>And so the challenge for Friendly AI is not to come up with the <em>first</em> solution to the FAE problem that is not a <em>human being</em>. It is the question of specifying something to a machine whose mind is not at all like a human mind, which is not even like a human <em>with 100000 IQ points </em>(which is still an awful lot like a human mind).</p><br><p>The problem is that a superintelligence with 'Friendliness' inscribed into its own goals is indistinguishable from one that does not contain the FAI ingredient at all.</p><br><p>Not every superintelligence has to be <em>Friendly</em> for its goals to be very different from human goals, but in the real world every FAI is implemented by a superintelligence. So to be friendly in a way that does not permit the programmers any way to tell it what they mean, must also be very different from a human being\xe2\x80\x94I don't see how you manage to have a Friendly superintelligence, if you have this constraint in place.</p><br><p>And if you want <em>any</em> kind of solution that will have any kind of chance of success, <em>then</em> the FAI problem is the <em>biggest problem to be solved</em>. Everything else that can go wrong will go wrong in a way that is not easily fixed by more knowledge and more intelligence. The other problems are just the things that have already been overlooked.</p><br><p>But the bigger the problem, the shorter the time to the solution. That is the most fundamental lesson of computing power and optimization power, even when you have to think outside the box and think outside civilization as a whole. It is not 'human values versus non-human values' that I am talking about, it is that <em>we are currently inside a world in which Friendly AI is impossible.</em></p>      </span>    </div>  </div></body></html>