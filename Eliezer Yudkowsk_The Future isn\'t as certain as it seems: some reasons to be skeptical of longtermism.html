<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        The Future isn\'t as certain as it seems: some reasons to be skeptical of longtermism      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Eliezer Yudkowsk</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p><strong>Previously in series</strong>:  The Hanson-Yudkowsky AI Foom Debate</p><br><p>From a recent exchange:</p><br><blockquote>
<p>Yvain:  "That's what I was saying all along. I'm not sure I understand this response that you gave about me not really being on board with 'intelligence explosion,' but it feels like you're just dodging the issue, and I feel like you're not actually addressing what the issue is. I don't get that reaction with Robin Hanson; he seems to be giving you exactly the kind of argument you might make if you've ever read any of my blog posts."</p>
</blockquote><br><p>Me:  I feel like I've seen that kind of argument before, and it's usually offered towards people who are talking about very long timescales of years and decades. This is kind of long-termism's big red-flag, so maybe I should address that.</p><br><p>In general, I am skeptical when people propose plans (like, eg, AI being developed by rich countries) that would take eons to play out. Eg, you wouldn't be reading this post right now, because there was no one alive a few billion years ago who _would _have started writing blog posts on the Internet. And I'm probably being oversimplified in that post, but my general impression is that people who propose things that will take millions of years or eons tend to be pretty full of themselves.</p><br><p>Yvain:  Then why are you a longtermist?</p><br><p>Me: Because it will matter in the far future! There will be lots of things that matter in the future\xe2\x80\x94for example, if you want a better future, then trying to have a positive impact now matters, while trying to have a good impact later doesn't matter much. For example, I care about whether I live in a world without suffering and without wars. I don't care <em>that much</em> about whether I live to see the Singularity if war is still around back then, or whether Earth ever becomes uninhabitable. I care a lot more about the impact of my actions, and my impact is in the future.</p><br><p>The Future isn't as Certain as it Seems:</p><br><p>I think the main lesson is this:  When an argument is made, it is <em>crucial</em> to keep track of how far the argument is from being a <em>total nonsequitur</em>.</p><br><p>It's easy to think that something is very far away (like, "in the future"), but this is rarely the right conclusion. The general rule seems to be\xe2\x80\x94if your conclusion doesn't follow <em>by any sort of known chain of argument</em>, you can try to reword (or shorten) that chain of argument to produce a conclusion that <em>does</em> follow. That's why a lot of the time the best way to refute a claim of "X is certain," is not to refute the claim at all\xe2\x80\x94"X is 99.9999% certain!"\xe2\x80\x94but to ask about what sort of <em>known chain of argument</em> would <em>show</em> that X is certain. Or better yet, ask how we know it's possible for such arguments to exist in the first place.</p><br><p>And I'm reminded of another old standard:  Once in a while, you run across someone who's so <em>doomed</em> to believe an idea that it's actually <em>helping</em> them to do a little critical thinking to find <em>some</em> way it can be true.</p><br><p>In my experience, this is rare\xe2\x80\x94and when it does occur, it usually gets resolved, by showing that there's a chain of argument, or a chain of counterexamples, somewhere in their future lightcone where X is <em>not</em> true.  (Or, perhaps it's a chain of counterestimulations, someplace where it is not actually being doomed.)</p><br><p>But that doesn't mean such cases aren't worth thinking about in advance.  "If we can save everyone in the world alive today, why wait to be born yourself?"\xe2\x80\x94well, what's your best future-oriented argument as to how (that sort of) claim might actually be <em>true</em>? I can't actually remember ever running into a case where a _doomed _longtermist argued that it would be a _good _idea to start a nuclear war in the next twenty years.</p><br><p>Some longtermists say, "Well, the future <em>will</em> be uncertain, but at least we can plan our <em>actions</em> so as to bring about the future that <em>is</em> most good, rather than the one that <em>might</em> have been if we behaved differently."</p><br><p>But if you can't say how you know that it's not possible for the future to be worse, then you can't tell what impact your action is likely to have!</p><br><p>Which is to say:  If we want to <em>know what we're getting ourselves into</em>, we need to know <em>how it's possible</em> for these outcomes to take place as a result of our actions.</p><br><p>So it's not just good to think about the future and reason about it with our best effort; it's also important to think about whether the future is actually possible to get.</p><br><p>(As a rule of thumb, I try to have the following counterargument in the back of my mind as I think about any claim of "X will happen!"\xe2\x80\x94whether it's plausible that "X will happen" seems to be <em>an actual argument</em> I could be expected to refute, or whether the proposition is a <em>possible world</em> that I can't <em>currently</em> rule out. That is, am I saying "X will be impossible to avoid even in the case where X is good, Y and Z not being the case?"  Or am I just trying to claim that X is good, because if it were bad, I would be able to see immediately how it would be impossible to avoid, regardless of how bad it is? Or, alternatively\xe2\x80\x94am I just making a blanket statement about X, that does not follow by any sort of known argument?)</p><br><p>I'm not sure exactly how much it's appropriate to make explicit arguments with those counterarguments in mind; I've never worked it out systematically, and it seems to depend on where I am in the intellectual traffic and whether I'm in any actual danger of <em>doom</em>. But I suspect it's more important to think about the reasoning steps that could go wrong.</p><br><p>If you're arguing with someone who is so very doomed to believe in X that they can argue about it even though it's <em>impossible</em>, you <em>must be doubly careful</em> to think about all the ways you could be wrong, because there's no point of wasting your time talking to an idiot.</p><br><p>If the future seems <em>so unlikely</em> that you can't think of any argument that may make it likely, then you might be <em>actually doomed</em>. I've encountered many of these problems myself, and I'm still working on overcoming the worst one.</p><br><p>It may not be easy to see which arguments are in fact doomed at all. One argument that keeps on coming up is along the lines of "But what if it's not okay to die?"  Or alternatively, "But what if there's no one reading this?"  (Or any other question that implies that you, at least, must survive the future.)  So I would encourage you to <em>try really really hard</em> to remember and refute things that seem obviously doomed, before trying to think of things that seem _not _doomed.</p>      </span>    </div>  </div></body></html>