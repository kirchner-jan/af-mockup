<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        Fermi problem: Why the world hasn't ended      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Paul Christian</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>For the past week I've been working on an interesting problem in AI:</p><br><p>Suppose that a few years from now, you were talking with some very smart agent, an AGI or superintelligence, and it asked you how likely it was to have destroyed the world. You might try to guess a real-number probability based on whatever evidence it has about humans.</p><br><p>At this point your friend might try to figure the right policy by asking the following questions:</p><br><ul>
<li></li>
</ul><br><p>Is the agent superintelligent? If so, how? In particular, how smart?</p><br><ul>
<li></li>
</ul><br><p>What do humans hope the agent will do in the world? How do they expect a superintelligent agent to behave?</p><br><ul>
<li>If the superintelligent agent were able to read all human literature, how likely would it be to end the world?</li>
</ul><br><ul>
<li>What is the actual distribution over policies that the agent could choose from?</li>
</ul><br><ul>
<li>How smart do humans want the agent to be?</li>
</ul><br><ul>
<li>Even if the agent were superintelligent, wouldn't it still be safe?</li>
</ul><br><ul>
<li>Are there other risks we know of that would kill us all immediately even if the agent were as smart as humans wish?</li>
</ul><br><p>These questions are intended to help the superintelligence find the answer to the famous Fermi question:</p><br><p>In 1950 the physicist Enrico Fermi posed the problem of how many nukes each country would reasonably expect the U.S.S.R. to have built by 1970. Fermi didn't think it would be much of a challenge for the Soviets to build a handful of them\xe2\x80\x94so many, in fact, that he claimed the U.S.'s chances were only 2-1. In fact, he estimated that as low as one per thousand. What would happen if the problem were real, and no country was a step behind?  </p><br><p>I won't try to fully answer these questions in this article, but I'll mention some interesting things.</p><br><p>The first:</p><br><p>Given that humans want superintelligent behavior, superintelligence needs to understand human desires to be a lot smarter than Fermi thought. (More broadly, given Fermi's naive calculation, humans expect superintelligent behavior to be a lot more surprising than human behavior. Fermi thought superintelligence should be more similar to us than we are.)</p><br><p>The second:</p><br><p>An AGI could take actions that would end the world, if the world were destroyed in the way that a large fraction of the super intelligent people who came before didn't end the world, and humans thought this was a high probability.</p><br><p>In particular, the superintelligence could end the world quickly, either by hacking the whole electricity grid, or by creating nanotechnology which gets loose and which rapidly does irreversible things to the surrounding matter. Such quick failures are not as surprising as Fermi's estimate (or as other estimates), especially since nanotech would have no reason to look like weapons the humans around it could fight (or be seen being built). </p><br><p>As for the last question:</p><br><p>The superintelligence could end this world even if no country was a distance behind! We don't have a good sense of how many countries are even in the running!</p><br><p>This is related to a general problem facing superintelligence researchers: it feels like they should figure out some set of goals for an AGI, and implement those goals efficiently, and it's not clear to them that this requires AI technology that is 10 million times more powerful than that of every currently-existing human. And indeed, an AGI doesn't look like it will be able to build nanoteshrooms and self repair.</p><br><p>However, if a superintelligence is capable of understanding human desires, it may well decide to end the world directly, rather than making the world the way humans want it (via some safe or not-so-safe way).</p><br><p>This problem is not going to be solved by just writing down a set of goals. There are a series of other questions that would have to be answered: is an AGI capable of understanding this decision to end the world, even before it understands human desires? Even after understanding human desires, will it end the world the way people hoped? After understanding human desires and the decision to end the word, how smart do humans want it to be? Is a human-level AI capable of understanding its decision to end this world even after it understands what humans intended? There is no real solution to this Fermi problem, except to think about it a lot. The next section will try to outline the parts of this problem whose answer depends on our model of human values.</p>      </span>    </div>  </div></body></html>