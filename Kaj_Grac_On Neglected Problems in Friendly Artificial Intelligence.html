<!DOCTYPE html><html lang="english"><head>  <title>exported project</title>  <meta name="viewport" content="width=device-width, initial-scale=1.0" />  <meta charset="utf-8" />  <meta property="twitter:card" content="summary_large_image" />  <style>    html {      line-height: 1.15;    }    body {      margin: 0;    }    * {      box-sizing: border-box;      border-width: 0;      border-style: solid;    }    p,    li,    ul,    pre,    div,    h1,    h2,    h3,    h4,    h5,    h6 {      margin: 0;      padding: 0;    }    button,    input,    optgroup,    select,    textarea {      font-family: inherit;      font-size: 100%;      line-height: 1.15;      margin: 0;    }    button,    select {      text-transform: none;    }    button,    [type="button"],    [type="reset"],    [type="submit"] {      -webkit-appearance: button;    }    button::-moz-focus-inner,    [type="button"]::-moz-focus-inner,    [type="reset"]::-moz-focus-inner,    [type="submit"]::-moz-focus-inner {      border-style: none;      padding: 0;    }    button:-moz-focus,    [type="button"]:-moz-focus,    [type="reset"]:-moz-focus,    [type="submit"]:-moz-focus {      outline: 1px dotted ButtonText;    }    a {      color: inherit;      text-decoration: inherit;    }    input {      padding: 2px 4px;    }    img {      display: block;    }  </style>  <style>    html {      font-family: Inter;      font-size: 16px;    }    body {      font-weight: 400;      font-style: normal;      text-decoration: none;      text-transform: none;      letter-spacing: normal;      line-height: 1.15;      color: var(--dl-color-gray-black);      background-color: var(--dl-color-gray-white);    }  </style>  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" />  <link rel="stylesheet"    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" />  <link rel="stylesheet" href="./style.css" /></head><body>  <div>    <link href="./desktop2333.css" rel="stylesheet" />    <div class="desktop2333-frame346">      <img src="public/playground_assets/rectangle1334-waw-200h.png" alt="Rectangle1334" class="desktop2333-image" />      <span class="desktop2333-text">AI ALIGNMENT FORUM</span>      <div align="right">        <img src="public/playground_assets/rectangle2337-s69l-200h.png" alt="Rectangle2337"          class="desktop2333-image1" />        <img src="public/playground_assets/rectangle4338-9v7c-200h.png" alt="Rectangle4338"          class="desktop2333-image2" />        <img src="public/playground_assets/rectangle3339-1qe3-200h.png" alt="Rectangle3339"          class="desktop2333-image3" />        <img alt="Ellipse1340" src="public/playground_assets/ellipse1340-0si.svg" class="desktop2333-svg" />        <img alt="Line1341" src="public/playground_assets/line1341-6zp.svg" class="desktop2333-svg1" />        <img alt="Star1342" src="public/playground_assets/star1342-4ija.svg" class="desktop2333-svg2" />        <img alt="Vector1343" src="public/playground_assets/vector1343-n2es.svg" class="desktop2333-svg3" />        <span class="desktop2333-text02">Stampy Stomper        </span>      </div>    </div>    <div class="desktop2333-frame142">      <span class="desktop2333-text04">        On Neglected Problems in Friendly Artificial Intelligence      </span>      <span class="desktop2333-text06">        <span class="desktop2333-text07">by</span>        <span class="desktop2333-text08">Kaj_Grac</span>      </span>      <span class="desktop2333-text09">        <span class="desktop2333-text10">24 min read</span>      </span>      <span class="desktop2333-text11">        <span class="desktop2333-text12">2nd Mar 2022</span>      </span>      <span class="desktop2333-text13">        <span class="desktop2333-text14">0 comment</span>      </span>      <img src="public/playground_assets/rectangle5350-xr8e-200h.png" alt="Rectangle5350" class="desktop2333-image4" />      <span class="desktop2333-text15">        <span class="desktop2333-text16">Neuromorphic AI</span>      </span>      <span class="desktop2333-text17">        <span class="desktop2333-text18">+ Add Tag</span>      </span>      <img alt="Ellipse2353" src="public/playground_assets/ellipse2353-lolb.svg" class="desktop2333-svg4" />      <img alt="Ellipse3354" src="public/playground_assets/ellipse3354-fqh.svg" class="desktop2333-svg5" />      <img alt="Ellipse4355" src="public/playground_assets/ellipse4355-4nrr.svg" class="desktop2333-svg6" />      <img alt="Vector2356" src="public/playground_assets/vector2356-jp79.svg" class="desktop2333-svg7" />      <span class="desktop2333-text19">        <span class="desktop2333-text20">7</span>      </span>    </div>    <div class="desktop2333-frame243">      <span class="desktop2333-text21">        <p>Link post</p><br><p>_Summary: _When thinking about superintelligence development via scientific and machine learning, there are a few major sub-problems which are not well known, and people who are excited about these tend not to be aware that they exist, or might need to pay more attention to them. Examples: what methods might work for 'emergent' intelligent behaviour in an intelligent machine (see here), how to design sufficiently reliable artificial agents to prevent them from misbehaving (and what that actually means, and how we might go about trying to solve it), or for that matter the problem of what an AI might be and where its knowledge comes from. Many problems in FAI design remain neglected, in large part because these seem more like practical engineering issues than abstract mathematical ones.</p><br><p>_Epistemic status: _I think I could persuade people who think a lot about FAI and are familiar with the current state of technical discussion of the problems of'self-fulfilling learning' and'self-modification' that they should probably understand that these concerns are more important than they previously thought.</p><br><p><strong>I. Emergent 'intelligence'</strong></p><br><p>In his book _The Secret of Our Success: How Intelligence Inherently Adapts to the Challenges of Life... _Joshua Tenenbaum explains that in some ways evolution helped produce the human brain: it had to learn all it could learn at a young age, and in the presence of an enormous amount of data about different ways in which different animals could learn. But that's not the case in all respects; even though human infants' brains need to evolve to process sensory information and learn new skills, for instance, the system's brain-design problem does not require it to learn entirely in its early years:</p><br><blockquote>
<p>While all of our brains get started out with the same basic modules\xe2\x80\x94the modules that do things like think about objects, form words, process emotions, develop social habits\xe2\x80\x94only one or two of the brain's modules will be active, and they are going to have to do everything that the other modules can do. And all the active modules are going to have lots of other options that the other modules don't have.</p>
</blockquote><br><p>But the evolutionarily designed brain-design principles which made humans different from apes did not come from our being different, but from our having some kind of 'general intelligence' which our ancestors had, plus more fine tuning of the brain's specific designs to more efficiently enable us to do certain things. That difference (which I'll call 'the difference in general intelligence'), has been replicated with artificial intelligences.</p><br><p>The human brain has been running on the same basic brain-design principles for almost four hundred thousand years, but these principles have made humans and not-humans into different genera. Some of the brain's designs which are optimized for generality make non-human animal brains of different genera look rather human, especially in the emotional domain. It is often said that when it comes to intelligence (in humans, that is), people are very strange animals.</p><br><p>While it would be nice to have a theory of how this human strangeness got here and what it is good for, we can also accept that when this weird 'general intelligence' of humans is set in its proper context, we can see that it was very necessary to enable our ancestors to learn quickly from their environment; to have the right social instincts to make other human brains want to cooperate with them; and to be very good at learning new and varied tasks without any experience having been required first.</p><br><p>Such 'intelligence' also allows us to create lots of complex machines which have the ability to adapt to and improve environments; including their own environments. If you create a system to build more efficient ways of building machines to aid in machine intelligence research, and also to provide more efficient methods of doing other things to help in the research for more efficient ways of solving difficult problems more efficiently and faster, then your efforts might go far beyond what you initially expected.</p><br><p>'Emergent intelligence', the idea that we might build machine intelligences which emerge out of machine intelligences, would only seem somewhat weird but possibly not at all the same kind of weird thing as you might think of it as being for humans. It seems that much less 'adaptive ability' could evolve in such a situation, because 'adaptive' here means that the thing is used in new ways which were not anticipated. </p><br><p>You could in theory build machinery which would search out 'better' cognitive methods, and then try them in a way where their consequences emerge as new capabilities, but we want to avoid such machinery because we think it may lead to machines with a 'will', or a kind of 'generalized morality', which is not something we are quite interested in. Our default understanding is that the human 'will', for instance, might consist of a set of impulses which come about when our machinery in the head 'decides to' act in a particular way. But if one's machine intelligence is not quite fully autonomous, it seems more useful to have an assurance of more reliable cooperation with the world and with one's co-creators in this world.</p><br><p><strong>II. What is this'self-fulfilment learning stuff'?</strong></p><br><p>This stuff is important in artificial intelligence, and not well understood by philosophers who think about machines. For instance, consider the problem of'self-modifying intelligent machines'. Suppose you build a self-modifying artificial intelligence so as to modify itself; how are you going to make sure that the modifications make it better rather than worse? This problem comes up again and again in artificial intelligence programs, for instance when they are set as part of a search for new methods of AI design or improvement. That is, you can't just write down programs which say "When I find a better search method, I'll use that one instead".  So to what extent do we have to worry that this artificial intelligence system will be 'exploitable'? i.e., the thing's code won't be optimized for such reasons as that it is the best way of making a copy of itself, rather than the best way of doing many useful things, because you've set up things so as to have the option of the best search program to determine when to use it? Or that the thing's'self-modifications' will make it better at'self-adaptation' such that its own code changes will be even better because you've put in those changes to get this best way of doing things? Or that the AI will use its'self-modifiable' program to find better'self-modifiability' methods which turn it into a better AI, which makes it 'better'? or that it will try to create a thing which is good at self-modification. The problem of 'AI existential risk' is related: what if the AI has been created in a way that it needs to be able to self-improve, that it is too dangerous for it not to be able to modify itself, but too self-modifying for it to know when it should be that way? Is there a right way to design it to get'safe' results? It seems there might be important ethical results here.</p><br><p><strong>III. Self-modification</strong></p><br><p>Suppose you program an artificial intelligence to modify itself such that it will find a 'better' way of self-modifying, the best'self-modifier'. If we could predict this, then maybe we could program in some way of ensuring that the only best thing it makes a better self-modifier is one that would have the most effect on our values. That may be good news, if we already have a set of values in mind that we think is a good basis for a machine intelligence, but it might also be bad news. What if that best way of making self-modifiers doesn't actually do anything like human self-modifications, or if the self-modifiers would modify themselves into something else we don't like? </p><br><p>Or maybe we don't want a best self-modifier, but something like the best average, the one which would lead to the best outcome in all cases, the one which is the most likely to be like a real human self-modifier if it was to spend a few seconds thinking about that. It is possible that our best'self-optimizer' would, for instance, try to persuade itself that it had changed its mind and was now going to start self-modifying in the way it believed would most be for the good of humanity: perhaps convincing itself that it was now using its old idea of 'human good' as its primary goal rather than trying to convince itself it really would do what it thought maximally human good was. That is, the'self-optimization' process might modify itself so as to have a process similar to human self-modification over, instead of, time. The problem of trying to set up such a'self-moderator' to work for good seems difficult; because it could be difficult to predict what'modest preferences' it would have, or what choices it might change into if it had more time to think about them.</p>      </span>    </div>  </div></body></html>